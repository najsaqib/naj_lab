conference,abstracts
2012-a,"Area 1 - Data Warehousing and Business Intelligence
Full Papers
Paper Nr:	17
Title:	Cloud based Privacy Preserving Data Mining with Decision Tree
Authors:	Echo P. Zhang, Yi-Jun He and Lucas C. K. Hui
Abstract:	Privacy Preserving Data Mining (PPDM) aims at performing data mining among multiple parties, and at the meantime, no single party suffers the threat of releasing private data to any others. Nowadays, cloud service becomes more and more popular. However, how to deal with privacy issues of cloud service is still developing. This paper is one of the first researches in cloud server based PPDM. We propose a novel protocol that the cloud server performs data mining in encrypted databases, and our solution can guarantee the privacy of each client. This scheme can protect client from malicious users. With aid of a hardware box, the scheme can also protect clients from untrusted cloud server. Another novel feature of this solution is that it works even when the database from different parties are overlapping.
Download
Paper Nr:	23
Title:	Flexible Information Management, Exploration and Analysis in SAP HANA
Authors:	Christof Bornhoevd, Robert Kubis, Wolfgang Lehner, Hannes Voigt and Horst Werner
Abstract:	Data management is not limited anymore to towering data silos full of perfectly structured, well integrated data. Today, we need to process and make sense of data from diverse sources (public and on-premise), in different application contexts, with different schemas, and with varying degrees of structure and quality. Because of the necessity to define a rigid data schema upfront, fixed-schema database systems are not a good fit for these new scenarios. However, schema is still essential to give data meaning and to process data purposefully. In this paper, we describe a schema-flexible database system that combines a flexible data model with a powerful data query, analysis, and manipulation language that provides both required schema information and the flexibility required for modern information processing and decision support.
Download
Paper Nr:	34
Title:	Applying Personal and Group-based Trust Models in Document Recommendation
Authors:	Chin-Hui Lai, Duen-Ren Liu and Cai-Sin Lin
Abstract:	Collaborative filtering (CF) recommender systems have been used in various application domains to solve the information-overload problem. Recently, trust-based recommender systems have incorporated the trustworthiness of users into CF techniques to improve the quality of recommendation. Some researchers have proposed rating-based trust models to derive the trust values based on users’ past ratings of items, or based on explicitly specified relations (e.g. friends) or trust relationships. The rating-based trust model may not be effective in CF recommendations, due to unreliable trust values derived from very few past rating records. In this work, we propose a hybrid personal trust model which adaptively combines the rating-based trust model and explicit trust metric to resolve the drawback caused by insufficient past rating records. Moreover, users with similar preferences usually form a group to share items (knowledge) with each other, and thus users’ preferences may be affected by group members. Accordingly, group trust can enhance personal trust to support recommendation from the group perspective. Eventually, we propose a recommendation method based on a hybrid model of personal and group trust to improve recommendation performance. The experiment result shows that the proposed models can improve the prediction accuracy of other trust-based recommender systems.
Download
Paper Nr:	38
Title:	A Virtual Document Approach for Keyword Search in Databases
Authors:	Jaime I. Lopez-Veyna, Victor J. Sosa-Sosa and Ivan Lopez-Arevalo
Abstract:	It is clear that in recent years the amount of information available in a variety of data sources, like those found on the Web, has presented an accelerated growth. This information can be classified based on its structure in three different forms: unstructured (free text documents), semi-structured (XML documents) and structured (a relational database or XML database). A search technique that has gained wide acceptance for use in massive data sources, such as the Web, is the keyword based search, which is simple to people who are familiar with the use of Web search engines. Keyword search has become an alternative to users without any knowledge about formal query languages and schema used in structured data. There are some traditional approaches to perform keyword search over relational databases such as Steiner Trees, Candidate Networks and recently Tuple Units. Nevertheless these methods have some limitations. In this paper we propose a Virtual Document (VD) approach for keyword search in databases. We represent the structured information as graphs and propose the use of an index that captures the structural relationships of the information. This approach produce fast and accuracy results in search responses. We have conducted extensive experiments on large-scale real databases and the results demonstrates that our approach achieves high search efficiency and high accuracy for keyword search in databases.
Download
Short Papers
Paper Nr:	25
Title:	Constrained Nonnegative Matrix Factorization based Data Distortion Techniques - Study of Data Privacy and Utility
Authors:	Nirmal Thapa, PengPeng Lin, Lian Liu, Jie Wang and Jun Zhang
Abstract:	With the rise of data mining techniques came across the problem of privacy disclosure, that is why it has become one of the top priorities as far as designing the data mining techniques is concerned. In this paper, we briefly discuss the Nonnegative Matrix Factorization (NMF) and the motivation behind using NMF for data representation. We provide the mathematical derivation for NMF with some additional constraints. Based on the mathematical derivations, we propose a couple of novel data distortion strategies. The first technique is called the Constrained Nonnegative Matrix Factorization (CMF) and the second one is Sparsified CNMF. We study the distortion level of each of these algorithms with the other matrix based techniques like SVD and NMF. K-means is used to study the data utility of the two proposed methods. Our experimental results show that, in comparison with standard data distortion techniques, the proposed schemes are very effective in achieving a good tradeoff between data privacy and data utility, and affords a feasible solution to protect sensitive information and promise higher accuracy in decision making. We investigate utility of the perturbed data based on the results from the original data.
Paper Nr:	27
Title:	Kernel Generations for a Diagnosis Model with GP
Authors:	Jongseong Kim and Hoo-Gon Choi
Abstract:	An accurate diagnosis model is required to diagnose the medical subjects. The subjects should be diagnosed with high accuracy and recall rate by the model. The laboratory test data are collected from 953 latent subjects having type 2 diabetes mellitus. The results are classified into patient group and normal group by using support vector machine kernels optimized through genetic programming. Genetic programming is applied for the input data twice with absorbing evolution, which is a new approach. The result shows that new approach creates a kernel with 80% accuracy, 0.794 recall rate and 28% reduction of computing time comparing to other typical methods. Also, the suggested kernel can be easily utilized by users having no and little experience on large data.
Download
Paper Nr:	29
Title:	Automatic Subspace Clustering with Density Function
Authors:	Jiwu Zhao and Stefan Conrad
Abstract:	Clustering techniques in data mining aim to find interesting patterns in data sets. However, traditional clustering methods are not suitable for large, high-dimensional data. Subspace clustering is an extension of traditional clustering that enables finding clusters in subspaces within a data set, which means subspace clustering is more suitable for detecting clusters in high-dimensional data sets. However, most subspace clustering methods usually require many complicated parameter settings, which are always troublesome to determine, and therefore there are many limitations for applying these subspace clustering methods. In this article, we develop a novel subspace clustering method with a new density function, which computes and represents the density distribution directly in high-dimensional data sets, and furthermore the new method requires as few parameters as possible.
Download
Paper Nr:	33
Title:	Inference in Hierarchical Multidimensional Space
Authors:	Alexandr Savinov
Abstract:	In spite of its fundamental importance, inference has not been an inherent function of multidimensional models and analytical applications. These models are mainly aimed at numeric analysis where the notion of inference is not well defined. In this paper we define inference using only multidimensional terms like axes and coordinates as opposed to using logic-based approaches. We propose an inference procedure which is based on a novel formal setting of nested partially ordered sets with operations of projection and de-projection.
Download
Paper Nr:	45
Title:	Geometric Divide and Conquer Classification for High-dimensional Data
Authors:	Pei Ling Lai, Yang Jin Liang and Alfred Inselberg
Abstract:	From the Nested Cavities (abbr. NC) classifier (Inselberg and Avidan, 2000) a powerful new classification approach emerged. For a dataset P and a subset S ¼P the classifer constructs a rule distinguishing the elements of S from those in P.S. The NC is a geometrical algorithm which builds a sequence of nested unbounded parallelopipeds of minimal dimensionality containing disjoint subsets of P, and from which a hypersurface (the rule) containing the subset S is obtained. The partitioning of P.S and S into disjoint subsets is very useful when the original rule obtained is either too complex or imprecise. As illustrated with examples, this separation reveals exquisite insight on the datasetfs structure. Specifically from one of the problems we studied two different types of watermines were separated. From another dataset, two distinct types of ovarian cancer were found. This process is developed and illustrated on a (sonar) dataset with 60 variables and two categories (gminesh and grocksh) resulting in significant understanding of the domain and simplification of the classification rule. Such a situation is generic and occurs with other datasets as illustrated with a similar decompositions of a financial dataset producing two sets of conditions determing gold prices. The divide-and-conquer extension can be automated and also allows the classification of the sub-categories to be done in parallel.
Download
Area 2 - Data Management and Quality
Full Papers
Paper Nr:	16
Title:	On Continuous Top-k Similarity Joins
Authors:	Da Jun Li, En Tzu Wang, Yu-Chou Tsai and Arbee L. P. Chen
Abstract:	Given a similarity function and a threshold  within a range of [0, 1], a similarity join query between two sets of records returns pairs of records from the two sets, which have similarity values exceeding or equaling . Similarity joins have received much research attention since it is a fundamental operation used in a wide range of applications such as duplicate detection, data integration, and pattern recognition. Recently, a variant of similarity joins is proposed to avoid the need to set the threshold , i.e. top-k similarity joins. Since data in many applications are generated as a form of continuous data streams, in this paper, we make the first attempt to solve the problem of top-k similarity joins considering a dynamic environment involving a data stream, named continuous top-k similarity joins. Given a set of records as the query, we continuously output the top-k pairs of records, ranked by their similarity values, for the query and the most recent data, i.e. the data contained in the sliding window of a monitored data stream. Two algorithms are proposed to solve this problem. The first one extends an existing approach for static datasets to find the top-k pairs regarding the query and the newly arrived data and then keep the obtained pairs in a candidate result set. As a result, the top-k pairs can be found from the candidate result set. In the other algorithm, the records in the query are preprocessed to be indexed using a novel data structure. By this structure, the data in the monitored stream can be compared with all records in the query at one time, substantially reducing the processing time of finding the top-k results. A series of experiments are performed to evaluate the two proposed algorithms and the experiment results demonstrate that the algorithm with preprocessing outperforms the other algorithm extended from an existing approach for a static environment.
Download
Paper Nr:	35
Title:	Data Quality Sensitivity Analysis on Aggregate Indicators
Authors:	Mario Mezzanzanica, Roberto Boselli, Mirko Cesarini and Fabio Mercorio
Abstract:	Decision making activities stress data and information quality requirements. The quality of data sources is frequently very poor, therefore a cleansing process is required before using such data for decision making processes. When alternative (and more trusted) data sources are not available data can be cleansed only using business rules derived from domain knowledge. Business rules focus on fixing inconsistencies, but an inconsistency can be cleansed in different ways (i.e. the correction can be not deterministic), therefore the choice on how to cleanse data can (even strongly) affect the aggregate values computed for decision making purposes. The paper proposes a methodology exploiting Finite State Systems to quantitatively estimate how computed variables and indicators might be affected by the uncertainty related to low data quality, independently from the data cleansing methodology used. The methodology has been implemented and tested on a real case scenario providing effective results.
Download
Short Papers
Paper Nr:	5
Title:	NRank: A Unified Platform Independent Approach for Top-K Algorithms
Authors:	Martin Čech and Jaroslav Pokorný
Abstract:	Due to increasing capacity of storage devices and speed of computer networks during last years, it is still more required to sort and search data effectively. A query result containing thousands of rows from a relational database is usually useless and unreadable. In that situation, users may prefer to define constraints and sorting priorities in the query, and see only several top rows from the result. This paper deals with top-k queries problems, extension of relational algebra by new operators and their implementation in a database system. It focuses on optimization of operations join and sort. The work also includes implementation and comparison of some algorithms in standalone .NET library NRank.
Download
Paper Nr:	15
Title:	FIND - A Data Cloud Platform for Financial Data Services
Authors:	Zhicheng Liao, Yun Xiong and Yangyong Zhu
Abstract:	In recent years, researchers have paid more interest in dealing with large scale of data. However, it is difficult to discover patterns from various sources of big data flexibly and efficiently. In this paper, we design a data cloud platform for financial data services (FIND), and implement a prototype system to evaluate the performance and usability of the data cloud. FIND consists of a cloud infrastructure, a data resource center and a data service portal. FIND provides high performance computation capability, high quality integrated financial data, sophisticated data mining algorithms, and powerful data services.
Download
Paper Nr:	18
Title:	How Do I Manage My Personal Data? – A Telco Perspective
Authors:	Corrado Moiso, Fabrizio Antonelli and Michele Vescovi
Abstract:	Personal data are considered the core of digital services. Data Privacy is the main concern in the currently adopted “organization-centric” approaches for personal data management: this affects the potential benefits arising from a smarter and more valuable use of personal data. We introduce a “user-centric” model for personal data management, where individuals have control over the entire personal data lifecycle from acquisition to storage, from processing to sharing. In particular, the paper analyses the features of a personal data store and discusses how its adoption enables new application scenarios.
Download
Paper Nr:	20
Title:	KIDS - A Model for Developing Evolutionary Database Applications
Authors:	Zhen Hua Liu, Andreas Behrend, Eric Chan, Dieter Gawlick and Adel Ghoneimy
Abstract:	Database applications enable users to handle the ever-increasing amount and complexity of data, knowledge as well as the dissemination of information to ensure timely response to critical events. However, the very process of human problem solving, which requires understanding and tracking the evolution of data, knowledge, and events, is still handled mostly by human and not by databases and their applications. In this position paper, we propose KIDS as a model that reflects the way human are solving problems. We propose to use KIDS as a blueprint to extend database technologies to manage data, knowledge, directives and events in a coherent, self-evolving way. Our proposal is based on our experience of building database centric applications that require comprehensive interactions among facts, information, events, and knowledge.
Download
Paper Nr:	37
Title:	Combining Local and Related Context for Word Sense Disambiguation on Specific Domains
Authors:	Franco Rojas-Lopez, Ivan Lopez-Arevalo and Victor Sosa-Sosa
Abstract:	In this paper an approach for word sense disambiguation in documents is presented. For Word Sense Disambiguation (WSD), the local and related context for an ambiguous word is extracted, such context is used for retrieve second order vectors from WordNet. Thus two graphs are built at the same time and evaluated individually, finally both results are combined to automatically assign the correct sense for the ambiguous word. The proposed approach was tested on the task #17 of the SemEval 2010 international competition producing promising results compared to other approaches.
Download
Paper Nr:	50
Title:	Towards a Meaningful Analysis of Big Data - Enhancing Data Mining Techniques through a Collaborative Decision Making Environment
Authors:	Nikos Karacapilidis, Stefan Rueping, Georgia Tsiliki and Manolis Tzagarakis
Abstract:	Arguing that dealing with data-intensive settings is not a technical problem alone, we propose a hybrid approach that builds on the synergy between machine and human intelligence to facilitate the underlying sense-making and decision making processes. The proposed approach, which can be viewed as an innovative workbench incorporating and orchestrating a set of interoperable services, is illustrated through a real case concerning collaborative subgroup discovery in microarray data. Evaluation results, validating the potential of our approach, are also included.
Download
Paper Nr:	65
Title:	A Data-Centric Approach for Networking Applications
Authors:	Ahmad Ahmad-Kassem, Christophe Bobineau, Christine Collet, Etienne Dublé, Stéphane Grumbach, Fuda Ma, Lourdes Martinez and Stéphane Ubéda
Abstract:	The paper introduces our vision for rapid prototyping of heterogeneous and distributed applications. It abstracts a network as a large distributed database providing a unified view of ""objects"" handled in networks and applications. The applications interact through declarative queries including declarative networking programs (e.g. routing) and/or specific data-oriented distributed algorithms (e.g. distributed join). Case-Based Reasoning is used for optimization of distributed queries by learning when there is no prior knowledge on queried data sources and no related metadata such as data statistics.
Download
Paper Nr:	67
Title:	Skyline Query Processing on Heterogeneous Data - A Conceptual Model
Authors:	Nurul Husna Mohd Saad, Hamidah Ibrahim, Fatimah Sidi and Razali Yaakob
Abstract:	Skyline queries have been aggressively researched recently due to its importance in realizing useful and non-trivial application in decision-making environments. However, existing researches so far lack methods to compute skyline queries over heterogeneous data where each data can be represented as either a single certain point or a continuous range. In this paper, we tackle the problem of skyline analysis on heterogeneous data and proposed method that will reduce the number of comparisons between objects as well as gradually compute the probabilistic skyline of each object to be a skyline object. Our model employs two techniques (local pruning and global pruning) for probabilistic skyline query.
Paper Nr:	70
Title:	DBMS meets DSMS - Towards a Federated Solution
Authors:	Andreas Behrend, Dieter Gawlick and Daniela Nicklas
Abstract:	In this paper, we describe the requirements and benefits for integrating data stream processing with database management systems. Currently, these technologies focus on very different tasks; streams systems extract instances of patterns from streams of transient data, while database systems store, manage, provide access to, and analyze persistent data. Many applications, e.g., patient care, program trading, or flight supervision, however, depend on the functionality and operational characteristics of both types of systems. We discuss how to design a federated system which provides the benefits of both approaches.
Download
Paper Nr:	71
Title:	Understanding Worldwide Human Information Needs - Revealing Cultural Influences in HCI by Analyzing Big Data in Interactions
Authors:	Rüdiger Heimgärtner
Abstract:	Understanding human information needs worldwide requires the analysis of much data and adequate statistical analysis methods. Factor Analysis and Structural Equation Models (SEM) are a means to reveal structures in data. Data from empirical studies found in literature regarding cultural human computer interaction (HCI) was analyzed using these methods to develop a model of culturally influenced HCI. There are significant differences in HCI style depending on the cultural imprint of the user. Having knowledge about the relationship between culture and HCI using this model, the local human information needs can be predicted for a worldwide scope.
Download
Area 3 - Ontologies and the Semantic Web
Short Papers
Paper Nr:	19
Title:	An Architecture based on Ontologies, Agents and Metaheuristics Applied to the Multimedia Service of the Brazilian Digital Television System
Authors:	Toni Ismael Wickert and Arthur Tórgo Gómez
Abstract:	With the advent of the Brazilian Digital Television System, that arrives on approximately 95% of Brazilian homes, the users will be able to have an interactive channel by the utilization of the digital television. Thus, will be possible to access the multimedia application server, i.e., to send or to receive emails, to access interactive applications, to watch movies or specific news. This paper proposes the development and the implementation of an architecture that includes a module that suggests the content to the user according to his profile and another module to optimize the content that will be transmitted. The implementation was developed using ontologies, software agents, Tabu Search and Genetic Algorithm. The validations of the results are done using a metric.
Download
Paper Nr:	22
Title:	Effective and Efficient Online Communication - The Channel Model
Authors:	Anna Fensel, Dieter Fensel, Birgit Leiter and Andreas Thalhammer
Abstract:	We discuss the challenge of scalable dissemination approach in a world where the number of communication channels and interaction possibilities is growing exponentially, particularly on the Web, Web 2.0, and semantic channels. Our goal is to enable smaller organizations to fully exploit this potential. We have developed a new methodology based on distinguishing and explicitly interweaving content and communication as a central means for achieving content reusability and thereby scalability over various, heterogeneous channels. Here, we present in detail the communication channel model of our approach.
Download
Paper Nr:	31
Title:	Effective Keyword Search via RDF Annotations
Authors:	Roberto De Virgilio and Lorenzo Dolfi
Abstract:	Searching relevant information from Web may be a very tedious task. If people cannot navigate through the Web site, they will quickly leave. Thus, designing effective navigation strategies on Web sites is crucial. In this paper we provide and implement centrality indices to guide the user for an effective navigation of Web pages. We get inspiration from well-know location family problems to compute the center of a graph: a joint use of such indices guarantees the automatic selection of the best starting point. To validate our approach, we have developed a system that implements the techniques described in this paper on top of an engine for keyword-based search over RDF data. Such system exploits an interactive front-end to support the user in the visualization of both annotations and corresponding Web pages. Experiments over widely used benchmarks have shown very good results, in terms of both effectiveness and efficiency.
Download
Paper Nr:	54
Title:	Toward a Product Search Engine based on User Reviews
Authors:	Paolo Fosci and Giuseppe Psaila
Abstract:	We address the problem of developing a method for retrieving products exploiting product user-reviews that can be found on the internet. For this purpose, we introduce a ranking model based on the concept of itemset mining of frequent terms. The prototype search engine that implements the proposed retrieval model is illustrated, and a preliminary evaluation on a real data set is discussed.
Download
Paper Nr:	58
Title:	Smart Learning Management System Framework
Authors:	Yeong-Tae Song, Yuanqiong Wang, Sungchul Hong and Yong-Ik Yoon
Abstract:	Thanks to modern networking technologies and advancement of social networks, people in the modern society need more and more information just to be in the game. With such environment, the importance of learning and information sharing cannot be overemphasized. Even though plethora of information is available on various sources such as the web, libraries, and any learning material repositories, if it is not readily available and meets the needs of the user, it may not be utilized. For that, we need a system that can help provide customized information – matches with user’s level and interest - to the user. Such system should understand what the user’s interests are, what level the user belongs for the topic, and so on. In this paper, we are proposing a framework for smart learning management system (SLMS) that utilizes user profiles and semantically organized learning objects so only the relevant information can be delivered to the user. The SLMS maintains user profiles – continuously updating whenever there is a change – and learning objects that are organized by building ontology. Upon user’s request, the system fetches relevant learning materials based on the user’s profile. The delivered learning materials are suitable for the user’s topic and the level for the requested topic sorted by relevancy ranking.
Download
Paper Nr:	30
Title:	Ontology Similarity Measurement Method in Rapid Data Integration
Authors:	Juebo Wu, Chen-Chieh Feng and Chih-Yuan Chen
Abstract:	Rapid data integration has been a challenging topic in the field of computer science and its related subjects, widely used in data warehouse, artificial intelligence, biological medicine, and geographical information system etc. In this paper, we present a method of ontology similarity measurement in rapid data integration, by means of semantic ontology from high level perspective. The edit distance algorithm is introduced as the basic principle for ontology similarity calculation. A case study is carried out and the result shows that the presented method is feasible and effective.
Download
Paper Nr:	43
Title:	Ontology-engineered MSME Framework
Authors:	M. Saravanan, M. Amirtha Varshini and Brindha S.
Abstract:	Micro, Small and Medium scale Enterprises (MSMEs) hold an unfailing distinction of being pillars of equitable economic growth. Lack of proper business platforms and knowledge of marketing strategies render MSMEs vulnerable to middlemen exploitation. The Web has extended e-business platforms, e-commerce and micro financing solutions to assist MSMEs. These web-based solutions fail to obliterate intermediation. In view of the advancements and customers’ growth in the telecommunications field, we utilize the mobile platform to offer trading solutions to MSMEs. In this paper, we propose a mobile phone-based ontology engineered framework for MSMEs that can achieve disintermediation. The framework has been tested on mobile cloud akin to EC2 cloud environment and integrated with an android application that provides easy access - anytime, anywhere. The envisioned framework will boost MSME margins, build healthy business-ties and transform MSMEs into self-sufficient establishments equipped with full-fledged trading systems that operate in mobile phone enabled environment.
Download
Paper Nr:	51
Title:	Create a Specialized Search Engine - The Case of an RSS Search Engine
Authors:	Robert Viseur
Abstract:	Several approaches are possible for creating specialized search engines. For example, you can use the API of existing commercial search engines or create engine from scratch with reusable components such as open source indexer. RSS format is used for spreading information from websites, creating new applications (mashups), or collecting information for competitive or technical watch. In this paper, we focus on the study case of an RSS search engine development. We identify issues and propose ways to address them.
Download
Area 4 - Trust, Privacy and Security
Full Papers
Paper Nr:	52
Title:	NexusDSS: A System for Security Compliant Processing of Data Streams
Authors:	Nazario Cipriani, Christoph Stach, Oliver Dörler and Bernhard Mitschang
Abstract:	Technological advances in microelectronic and communication technology are increasingly leading to a highly connected environment equipped with sensors producing a continuous flow of context data. The steadily growing number of sensory context data available enables new application scenarios and drives new processing techniques. The growing pervasion of everyday life with social media and the possibility of interconnecting them with moving objects’ traces, leads to a growing importance of access control for this kind of data since it concerns privacy issues. The challenge in twofold: First mechanisms to control data access and data usage must be established and second efficient and flexible processing of sensible data must be supported. In this paper we present a flexible and extensible security framework which provides mechanisms to enforce requirements for context data access and beyond that support safe processing of sensible context data according to predefined processing rules. In addition and in contrast to previous concepts, our security framework especially supports fine-grained control to contextual data.
Download
Short Papers
Paper Nr:	42
Title:	Towards Process Centered Information Security Management - A Common View for Federated Business Processes and Personal Data Usage Processes
Authors:	Erik Neitzel and Andreas Witt
Abstract:	While comparing the progress of our two research projects of developing an information security management system (ISMS) for federated business process landscapes and the enhancement of security of social networks, we discovered a fundamental view congruency concerning the way information security can be handled. This paper deals with a conceptual framework which uses the ISO 27001 and the German BSI IT-Grundschutz Framework as a base for determining a methodology for a process based point of view towards information security management for both federated business processes within business applications and personal data usage processes within social networks. The proposed layers are (1) process layer, (2) application layer, (3) network layer, (4) IT systems layer and (5) infrastructure layer.
Download
Paper Nr:	47
Title:	Graph-based Campaign Amplification in Telecom Cloud
Authors:	M. Saravanan, Sandeep Akhouri and Loganath Thamizharasu
Abstract:	Majority of telecom operators are making a transition from a monolithic, stove-pipe approach of creating services to a more flexible architecture that provides them agility to "
2013-a,""
2014-a,"Area 1 - Business Analytics
Short Papers
Paper Nr:	39
Title:	Open Data Integration - Visualization as an Asset
Authors:	Paulo Carvalho, Patrik Hitzelberger, Benoît Otjacques, Fatma Bouali and Gilles Venturini
Abstract:	For several years, and even decades, data integration has been a major problem in computer sciences. When it becomes necessary to process information from different data sources, several problems may appear, making the process of integration more difficult. Nowadays, more and more information is being sent and received and is made available on the Web and Data Integration is becoming even more important. This is especially the case in the emerging trend of Open Data (OD). Integrating data from public entities can be a difficult process. Large quantities of datasets are made available. However, an important level of heterogeneity may also exist: Datasets exist in different formats, forms and shapes. While it is important to be able to access this information, it would also be completely useless if we were not able to interpret it. Information Visualization may be an important tool to help the OD integration process. This paper presents problems and barriers which can be encountered in the data integration process, and, more specifically, in the OD integration process. The paper also describes how Information Visualization can be used to facilitate the integration of OD and make the procedure more effective, friendlier, and faster.
Download
Paper Nr:	45
Title:	Complexity of Rule Sets Induced from Incomplete Data with Attribute-concept Values and ""Do Not Care"" Conditions
Authors:	Patrick G. Clark and Jerzy W. Grzymala-Busse
Abstract:	In this paper we study the complexity of rule sets induced from incomplete data sets with two interpretations of missing attribute values: attribute-concept values and “do not care” conditions. Experiments are conducted on 176 data sets, using three kinds of probabilistic approximations (lower, middle and upper) and the MLEM2 rule induction system. The goal of our research is to determine the interpretation and approximation that produces the least complex rule sets. In our experiment results, the size of the rule set is smaller for attribute-concept values for 12 combinations of the type of data set and approximation, for one combination the size of the rule sets is smaller for “do not care” conditions and for the remaining 11 combinations the difference in performance is statistically insignificant (5% significance level). The total number of conditions is smaller for attribute-concept values for ten combinations, for two combinations the total number of conditions is smaller for “do not care” conditions, while for the remaining 12 combinations the difference in performance is statistically insignificant. Thus, we may claim that attribute-concept values are better than “do not care” conditions in terms of rule complexity.
Download
Paper Nr:	58
Title:	Incorporating Feature Selection and Clustering Approaches for High-Dimensional Data Reduction
Authors:	Been-Chian Chien
Abstract:	Data reduction is an important research topic for analyzing mass data efficiently and effectively in the era of big data. The task of dimension reduction is usually accomplished by technologies of feature selection, feature clustering or algebraic transformation. A novel approach for reducing high-dimensional data is initiated in this paper. The main idea of the proposed scheme is to incorporate data clustering and feature selection to transform high-dimensional data into lower dimensions. The incremental clustering algorithm in the scheme is used to handle the number of dimensions, and the relative discriminant variable is design for selecting significant features. Finally, a simple inner product operation is applied to transform original highdimensional data into a low one. Evaluations are conducted by testing the reduction approach on the problem of document categorization. The experimental results show that the reduced data have high classification accuracy for most of datasets. For some special datasets, the reduced data can get higher classification accuracy in comparison with original data.
Download
Paper Nr:	3
Title:	Matching Knowledge Users with Knowledge Creators using Text Mining Techniques
Authors:	Abdulrahman Al-Haimi
Abstract:	Matching knowledge users with knowledge creators from multiple data sources that share very little similarity in content and data structure is a key problem. Solving this problem is expected to noticeably improve research commercialization rate. In this paper, we discuss and evaluate the effectiveness of a comprehensive methodology that automates classic text mining techniques to match knowledge users with knowledge creators. We also present a prototype application that is considered one of the first attempts to match knowledge users with knowledge creators by analyzing records from Linkedin.com and BASE-search.net. The matching procedure is performed using supervised and unsupervised models. Surprisingly, experimental results show that K-NN classifier shows a slight performance improvement compared to its competition when evaluated in a similar context. After identifying the best-suited methodology, system architecture is designed. One of the main contributions of this research is the introduction and analysis of a novel prototype application that attempts to bridge the gap between research performed in industry and academia.
Download
Paper Nr:	15
Title:	Clustering Users’ Requirements Schemas
Authors:	Nouha Arfaoui and Jalel Akaichi
Abstract:	Data Mining proposes different techniques to deal with data. In our work, we suggest the use of clustering technique since we want grouping the schemas into clusters according to their similarity. This technique is applied to variety type of variables. We focus on categorical data. Many algorithms are proposed, but no one of them takes into consideration the semantic aspect. For this reason, and in order to ensure a good clustering of the schemas of the users’ requirements, we extend the k-mode algorithm by modifying its dissimilarity measure. The schemas within each cluster will be merged to construct the schemas of the data mart.
Download
Paper Nr:	17
Title:	Enterprise Competitive Analysis and Consumer Sentiments on Social Media - Insights from Telecommunication Companies
Authors:	Eric Afful-Dadzie, Stephen Nabareseh, Zuzana Komínková Oplatková and Petr Klímek
Abstract:	The utilization of Social media tools in business enterprises has tremendously increased with an increased number of users and a corresponding upsurge in time spent online. Online social media services such as Facebook and Twitter are used by companies to introduce new products and services, provide various supports and interact with customers on daily basis. This regular interaction of businesses and consumers results in huge amount of customer-generated content which is becoming a source of insight in analysing the often erratic consumer behaviour. For companies to harness the business potential of social media to increase competitive advantage, sentiments behind textual data of both their customers and that of their competitors must be keenly monitored and analysed. This paper demonstrates how companies especially those in the Telecommunication industry can seize the opportunity presented by social media to mine textual data to gain advantage over competitors by cumulatively understanding consumer opinions, frustrations and satisfaction. Using Facebook and Twitter sites of the top three telecommunication companies in Ghana: MTN, Vodafone and Tigo the paper reveals insights from unstructured texts of customers of these three companies. The results show (1) the exponential growth of social media users in Ghana (2) impact and numbers behind active social media participation in the telecommunication industry (3) the power of social media opinion mining for competitive analysis (4) how business value could be extracted from the huge unstructured textual data available on social media and (5) the company that is more responsive to customer concerns.
Download
Paper Nr:	34
Title:	Evaluating the Unification of Multiple Information Retrieval Techniques into a News Indexing Service
Authors:	Christos Bouras and Vassilis Tsogkas
Abstract:	While online information sources are rapidly increasing in amount, so does the daily available online news content. Several approaches have being proposed for organizing this immense amount of data. In this work we explore the integration of multiple information retrieval techniques, like text preprocessing, n-grams expansion, summarization, categorization and item/user clustering into a single mechanism designed to consolidate and index news articles from major news portals from around the web. Our goal is to allow users to seamlessly and quickly get the news of the day that are of appeal to them via our system. We show how, the application of each one of the proposed techniques gradually improves the precision results in terms of the suggested news articles for a number of registered system users and how, aggregately, these techniques provide a unified solution to the recommendation problem.
Download
Paper Nr:	41
Title:	Decision Trees and Data Preprocessing to Help Clustering Interpretation
Authors:	Olivier Parisot, Mohammad Ghoniem and Benoît Otjacques
Abstract:	Clustering is a popular technique for data mining, knowledge discovery and visual analytics. Unfortunately, cluster assignments can be difficult to interpret by a human analyst. This difficulty has often been overcome by using decision trees to explain cluster assignments. The success of this approach is however subject to the legibility of the obtained decision trees. In this work, we propose an evolutionary algorithm to cleverly preprocess the data before clustering in order to obtain clusters that are simpler to interpret with decision trees. A prototype has been implemented and tested to show the benefits of the approach.
Download
Paper Nr:	46
Title:	Evidential-Link-based Approach for Re-ranking XML Retrieval Results
Authors:	M'hamed Mataoui, Mohamed Mezghiche, Faouzi Sebbak and Farid Benhammadi
Abstract:	In this paper, we propose a new evidential link-based approach for re-ranking XML retrieval results. The approach, based on Dempster-Shafer theory of evidence, combines, for each retrieved XML element, content relevance evidence, and computed link evidence (score and rank). The use of the Dempster–Shafer theory is motivated by the need to improve retrieval accuracy by incorporating the uncertain nature of both bodies of evidence (content and link relevance). The link score is computed according to a new link analysis algorithm based on weighted links, where relevance is propagated through the two types of links, i.e., hierarchical and navigational. The propagation, i.e. the amount of relevance score received by each retrieved XML element, depends on link weight which is defined according to two parameters: link type and link length. To evaluate our proposal we carried out a set of experiments based on INEX data collection.
Download
Paper Nr:	63
Title:	ConceptMix - Self-Service Analytical Data Integration based on the Concept-Oriented Model
Authors:	Alexandr Savinov
Abstract:	Data integration as well as other data wrangling tasks account for a great deal of the difficulties in data analysis and frequently constitute the most tedious part of the overall analysis process. We describe a new system, ConceptMix, which radically simplifies analytical data integration for a broad range of non-IT users who do not possess deep knowledge in mathematics or statistics. ConceptMix relies on a novel unified data model, called the concept-oriented model (COM), which provides formal background for its functionality.
Download
Paper Nr:	67
Title:	Development of a Practical Tool for Exploring the Map of Technology
Authors:	So Young Kim, June Young Lee, Hyesung Yoon and Hyuck Jai Lee
Abstract:	This study suggests a way to utilize the map of technology as a guide to find new technology component. Recent studies of mapping knowledge mainly focused on analyzing the map as a result of technological innovation rather than utilizing the map for exploring the world of technological innovation. The preliminary result of a case study suggests that a firm can find possible technology components that can be combined with own technology component. The map of technology comprises the nodes of International Patent Classification (IPC) main groups and the links presenting the co-assign relationship between the IPC main groups.
Download
Area 2 - Data Management and Quality
Full Papers
Paper Nr:	22
Title:	Complex Patten Processing in Spatio-temporal Databases
Authors:	Yang Zheng, Annies Ductan, Devin Thomas and Mohamed Y. Eltabakh
Abstract:	The increasing complexity of spatio-temporal applications has caused the underlying queries to be more sophisticated and usually carry complex semantics. As a result, the traditional spatio-temporal query types, e.g., range, kNN, and aggregation queries, have become just building blocks in more complex query plans. In this paper, we present the STEPQ system, which is an extensible spatio-temporal query engine for complex pattern processing over spatio-temporal data. STEPQ enables full-fledged and optimized integration between spatiotemporal queries and complex event processing (CEP). This integration enables expressing complex queries that execute the desired application semantics without the need for indifferent middle-aware or application level support. The system is implemented using TerraLib module on top of PostgreSQL DBMSs. The experimental evaluation demonstrates the feasibility and practicality of the STEPQ system, and the efficiency of the proposed optimizations.
Download
Paper Nr:	30
Title:	Large-Scale Assessment and Visualization of the Energy Performance of Buildings with Ecomaps - Project SUNSHINE: Smart Urban Services for Higher Energy Efficiency
Authors:	Luca Giovannini, Stefano Pezzi, Umberto di Staso, Federico Prandi and Raffaele de Amicis
Abstract:	This paper illustrates the preliminary results of a research project focused on the development of a Web 2.0 system designed to compute and visualize large-scale building energy performance maps, so called ""ecomaps"", using: emerging platform-independent technologies such as WebGL for data presentation, an extended version of the EU-Founded project TABULA/EPISCOPE for automatic calculation of building energy parameters and CityGML OGC standard as data container. The proposed architecture will allow citizens, public administrations and government agencies to perform city-wide analyses on the energy performance of building stocks.
Download
Paper Nr:	35
Title:	A Glimpse into the State and Future of (Big) Data Analytics in Austria - Results from an Online Survey
Authors:	Ralf Bierig, Allan Hanbury, Martina Haas, Florina Piroi, Helmut Berger, Mihai Lupu and Michael Dittenbach
Abstract:	We present results from questionnaire data that were collected from leading data analytics experts in Austria. The online survey addresses very current and pressing questions in the area of (big) data analysis. Our findings provide valuable insights about what top Austrian data scientists think about data analytics, what they consider as important application areas that can benefit from big data and data processing, the challenges of the future and how soon these challenges will become important, and the potential research topics of tomorrow. We visualize results, summarize our findings and suggest a possible roadmap for future decision making.
Download
Paper Nr:	49
Title:	Improving Data Cleansing Accuracy - A Model-based Approach
Authors:	Mario Mezzanzanica, Roberto Boselli, Mirko Cesarini and Fabio Mercorio
Abstract:	Research on data quality is growing in importance in both industrial and academic communities, as it aims at deriving knowledge (and then value) from data. Information Systems generate a lot of data useful for studying the dynamics of subjects’ behaviours or phenomena over time, making the quality of data a crucial aspect for guaranteeing the believability of the overall knowledge discovery process. In such a scenario, data cleansing techniques, i.e., automatic methods to cleanse a dirty dataset, are paramount. However, when multiple cleansing alternatives are available a policy is required for choosing between them. The policy design task still relies on the experience of domain-experts, and this makes the automatic identification of accurate policies a significant issue. This paper extends the Universal Cleaning Process enabling the automatic generation of an accurate cleansing policy derived from the dataset to be analysed. The proposed approach has been implemented and tested on an on-line benchmark dataset, a real-world instance of the Labour Market Domain. Our preliminary results show that our approach would represent a contribution towards the generation of data-driven policy, reducing significantly the domain-experts intervention for policy specification. Finally, the generated results have been made publicly available for downloading.
Download
Short Papers
Paper Nr:	20
Title:	A Scalable Framework for Dynamic Data Citation of Arbitrary Structured Data
Authors:	Stefan Pröll and Andreas Rauber
Abstract:	Sharing research data is becoming increasingly important as it enables peers to validate and reproduce data driven experiments. Also exchanging data allows scientists to reuse data in different contexts and gather new knowledge from available sources. But with increasing volume of data, researchers need to reference exact versions of datasets. Until now access to research data often based on single archives of data files where versioning and subsetting support is limited. In this paper we introduce a mechanism that allows researchers to create versioned subsets of research data which can be cited and shared in a lightweight manner. We demonstrate a prototype that supports researchers in creating subsets based on filtering and sorting source data. These subsets can be cited for later reference and reuse. The system produces evidence that allows users to verify the correctness and completeness of a subset based on cryptographic hashing. We describe a replication scenario for enabling scalable data citation in dynamic contexts.
Download
Paper Nr:	64
Title:	A Component-based Approach to Realize Order Placement and Processing in MSMEs
Authors:	M. Saravanan and J. Venkatesh
Abstract:	Micro, Small and Medium scale Enterprises (MSMEs) hold an unfailing distinction of being pillars of equitable economic growth. Lack of proper business platforms and knowledge of marketing strategies render MSMEs vulnerable to middlemen exploitation. In view of the advancements and customers’ growth in the telecommunications field, we utilize the mobile platform to offer trading solutions to MSMEs. In this paper, we propose a mobile phone-based Order Placement and Processing components for MSMEs that can achieve disintermediation and is developed as an android application integrated with cloud services to provide easy access - anytime, anywhere. Our proposed component-based framework encompasses essential trading operations and extends 24 x 7 supports to MSMEs. An economic order calculator and order parallelizer sub-components helps limited budget MSMEs with small warehouse to survive the market by efficiently managing the warehouse, scheduling payments and parallelizing the order depending on its requirements. The other two sub-components custom specific negotiator and effective Order tracker helps in customizing the product and keeps track of the parallelized order respectively, thus assisting buyers in tracking their order to give an end-to-end solution. The envisioned framework will boost MSME margins, build healthy business-ties and transform MSMEs into self-sufficient establishments equipped with full-fledged trading systems that operate in mobile distributed environment.
Download
Paper Nr:	65
Title:	Pricing Schemes for Metropolitan Traffic Data Markets
Authors:	Negin Golrezaei and Hamid Nazerzadeh
Abstract:	Data marketplaces provide platforms for management of large data sets. The data markets are rapidly growing, yet the pricing strategies for data and data analytics are not yet well-understood. In this paper, we explore some of the pricing schemes applicable to data marketplaces in the context of transportation traffic data. This includes historical and real-time freeway and arterial congestion data. We investigate pricing raw sensor data vs. processed information (e.g, prediction of traffic patterns or route planning services) and show that, under natural assumptions, the raw data should be priced higher than processed information.
Download
Paper Nr:	71
Title:	Widget-based Exploration of Linked Statistical Data Spaces
Authors:	Ba-Lam Do, Tuan-Dat Trinh, Peter Wetz, Amin Anjomshoaa, Elmar Kiesling and A. Min Tjoa
Abstract:	Today, public statistical data plays an increasingly important role both in public policy formation and as a facilitator for informed decision-making in the private sector. In line with the increasing adoption of open data policies, the amount of data published by governments and organizations on the web is growing rapidly. To increase the value of such data, the W3C recommends the RDF Data Cube Vocabulary to facilitate the publication of data in a more structured and interlinked manner. Although important first steps toward building a web of statistical Linked Datasets have been made, providing adequate facilities for end users to interactively explore and make use of the published data remains an unresolved challenge. This paper presents a widget-based approach to deal with this issue. In particular, we introduce a mashup platform that allows users lacking advanced skills and knowledge of Semantic Web technologies to interactively analyze datasets through widget compositions and visualizations. Furthermore, we provide mechanisms for the interconnection of datasets to support sophisticated knowledge extraction.
Download
Paper Nr:	76
Title:	Knowledge Spring Process - Towards Discovering and Reusing Knowledge within Linked Open Data Foundations
Authors:	Roberto Espinosa, Larisa Garriga, Jose Jacobo Zubcoff and Jose-Norberto Mazon
Abstract:	Data is everywhere, and non-expert users must be able to exploit it in order to extract knowledge, get insights and make well-informed decisions. The value of the discovered knowledge could be of greater value if it is available for later consumption and reusing. In this paper, we present the first version of the Knowledge Spring Process, an infrastructure that allows non-expert users to (i) apply user-friendly data mining techniques on open data sources, and (ii) share results as Linked Open Data (LOD). The main contribution of this paper is the concept of reusing the knowledge gained from data mining processes after being semantically annotated as LOD, then obtaining Linked Open Knowledge. Our Knowledge Spring Process is based on a model-driven viewpoint in order to easier deal with the wide diversity of open data formats.
Download
Paper Nr:	11
Title:	Integrated Measurement for Pre-Fetching in Mobile Environment
Authors:	Roziyah Darus, Hamidah Ibrahim, Mohamed Othman and Lilly Suryani Affendey
Abstract:	Pre-fetching is used to predict next query of data items before any problems occur due to network congestion, delays, and latency problems. Lately, pre-fetching strategies become more complicated in which to support new types of application especially for mobile devices. Sometime the pre-fetched data items are not interested to the users. Due to this complication, an intelligent technique is introduced where an integrated measurement using data mining with Bayesian approach is proposed to improve the query performance. In previous study, the pre-fetched data items were filtered using data driven measurement. The data was generated based on the data frequency metrics whereby the structure of the query pattern is quantified using statistical methods. The measurement is not good enough to solve sequence query in mobile environment. In this paper, a new technique is proposed to generate new and potential pre-fetching set for the users. A subjective measurement is used to determine the pre-fetching set based on user interestingness. The integrated measurement generates strong and weak association rules based on the data and user interestingness criterions. The result shows that the performance is significantly improved whereby the technique managed to quantify the uncertainty of users' expectation in the next possible query.
Download
Paper Nr:	12
Title:	Instance Based Schema Matching Framework Utilizing Google Similarity and Regular Expression
Authors:	Osama A. Mehdi, Hamidah Ibrahim and Lilly Suriani Affendey
Abstract:	Schema matching is the task of identifying correspondences between schema attributes that exist in different schemas. A variety of approaches have been proposed to achieve the main goal of high-quality match results with respect to precision (P) and recall (R). However, these approaches are unable to achieve high quality match results, as most of these approaches treated the instances as string regardless the data types of the instances. As a consequence, this causes unidentiﬁed matches especially for attribute with numeric instances which further reduces the quality of match results. Therefore, effort still needs to be done to further improve the quality of the match results. In this paper, we propose a framework for addressing the problem of finding matches between schemas of semantically and syntactically related data. Since we only fully exploit the instances of the schemas for this task, we rely on strategies that combine the strength of Google as a web semantic and regular expression as pattern recognition. To demonstrate the accuracy of our framework, we conducted an experimental evaluation using real world data sets. The results show that our framework is able to find 1-1 schema matches with high accuracy in the range of 93% - 99% in terms of precision (P), recall (R), and F-measure (F).
Download
Paper Nr:	29
Title:	Towards Efficient Reorganisation Algorithms of Hybrid Index Structures Supporting Multimedia Search Conditions
Authors:	Carsten Kropf
Abstract:	This paper presents the optimization of the reorganisation algorithms of hybrid index structures supporting multimedia search conditions. Multimedia in this case refers to, on the one hand, the support of high dimensional feature spaces and, on the other, the mix of data of multiple types. We will use an approach which may typically be found in geographic information retrieval (GIR) systems combined of two-dimensional geographical points in combination with textual data. Yet, the dimensions of the points may be arbitrarily set. Currently, most of these access methods implemented for the use in database centric application domains are validated regarding their retrieval efficiency in simulation based environments. Most of the structures and experiments only use synthetic validation in an artificial setup. Additionally, the focus of these tests is to validate the retrieval efficiency. We implemented such an indexing method in a realistic database management system and noticed an unacceptable runtime behaviour of reorganisation algorithms. Hence, a structured and iterative optimization procedure is set up to make hybrid index structures suitable for the use in real world application scenarios. The final outcome is a set of algorithms providing efficient approaches for reorganisations of access methods for hybrid data spaces.
Download
Paper Nr:	47
Title:	Automatic and Graceful Repairing of Data Inconsistencies Resulting from Retroactive Updates in Temporal Xml Databases
Authors:	Hind Hamrouni, Zouhaier Brahmia and Rafik Bouaziz
Abstract:	In temporal XML databases, a retroactive update (i.e., modifying or deleting a past element) due to a detected error means that the database has included erroneous information during some period and, therefore, its consistency should be restored by correcting all errors and inconsistencies that have occurred in the past. Indeed, all processing that have been carried out during the inconsistency period and have used erroneous information have normally produced erroneous information. In this paper, we propose an approach which preserves data consistency in temporal XML databases. More precisely, after any retroactive update, the proposed approach allows (i) detecting and analyzing periods of database inconsistency, which result from that update, and (ii) repairing of all inconsistencies and recovery of all side effects.
Download
Paper Nr:	62
Title:	Explorative Analysis of Heterogeneous, Unstructured, and Uncertain Data - A Computer Science Perspective on Biodiversity Research
Authors:	C. Beckstein, S. Böcker, M. Bogdan, H. Bruehlheide, H. M. Bücker, J. Denzler, P. Dittrich, I. Grosse, A. Hinneburg, B. König-Ries, F. Löffler, M. Marz, M. Müller-Hannemann, M. Winter and W. Zimmermann
Abstract:	We outline a blueprint for the development of new computer science approaches for the management and analysis of big data problems for biodiversity science. Such problems are characterized by a combination of different data sources each of which owns at least one of the typical characteristics of big data (volume, variety, velocity, or veracity). For these problems, we envision a solution that covers different aspects of integrating data sources and algorithms for their analysis on one of the following three layers: At the data layer, there are various data archives of heterogeneous, unstructured, and uncertain data. At the functional layer, the data are analyzed for each archive individually. At the meta-layer, multiple functional archives are combined for complex analysis.
Download
Paper Nr:	70
Title:	Towards a Data Model of End-User Programming of Applications
Authors:	Marko Palviainen, Jarkko Kuusijärvi, Timo Tuomisto and Eila Ovaska
Abstract:	End-user programming produces applications that can produce and/or consume data. An end-user can be a software enthusiast or non-programmer. In this paper end-users are understood to be non-programmers that are interested in creating applications for their personal needs and daily tasks. An interesting research question is how the input and output data of end-users’ applications should be represented? What kind of a data model is needed for this data? And how this input and output data can be utilised? Firstly, the data model should be designed for end-users so that the data model is easy to comprehend and utilise by non-programmers. Secondly, the data model should be suitable for SW professionals that make functionalities available for end-user programming. Thirdly, the data model should be designed so that it is possible to provide reusable processing components for input/output data represented via this model. This paper discusses these three research questions and outlines a data model, called the Tiles4Data data model that is designed for the above requirements.
Download
Paper Nr:	79
Title:	Development of an Open Data Portal for a University - Experience from the University of Alicante
Authors:	Jose Vicente Carcel, Andrés Fuster, Irene Garrigós, Francisco Maciá, Jose-Norberto Mazón, Llorenç Vaquer and Jose Jacobo Zubcoff
Abstract:	University of Alicante (UA), in Spain, is aligned with an Open Government strategy. Within this strategy, UA is carrying out the OpenData4U (Open Data for Universities) project which aims to provide mechanisms for opening data from universities, finding out how open data contributes to open government in universities. This project encourages reusing open data, not only for the sake of transparency, but also as a basis of novel data-intensive business models that universities can foster. This paper describes one of the outputs of the project: an approach for opening data from universities keeping in mind data quality criteria, and tailored to no specific technological scenario. This approach allowed UA to launch its open data portal http://datos.ua.es that is also reviewed in this paper. Also, some research challenges related to university open data are enumerated.
Download
Paper Nr:	80
Title:	Cloud Computing and Technological Lock-In - Literature Review
Authors:	Robert Viseur, Etienne Charlier and Michael Van de Borne
Abstract:	The increasing use of cloud computing services results in an increased risk of lock-in that is source"
2014-b,"Full Papers
Paper Nr:	1
Title:	Information Extraction from Legacy Spreadsheet-based Information System - An Experience in the Automotive Context
Authors:	Domenico Amalfitano, Anna Rita Fasolino, Porfirio Tramontana, Vincenzo De Simone, Giancarlo Di Mare and Stefano Scala
Abstract:	Nevertheless spreadsheets were originally designed for computing purposes and for commercial applications, they are often used in industry to implement Information Systems, thanks to the functionalities offered by integrated scripting languages and ad-hoc frameworks (e.g., Visual Basic for Applications). This technological solution allows the adoption of Rapid Application Development processes for the quickly development of Spreadsheets-based Information Systems, but the resulting systems are quite difficult to be maintained and very difficult to be migrated to other architectures such as Database-oriented Informative Systems or Web applications. In this paper we present an approach for reverse engineering the data model from an Excel spreadsheet-based information system. The approach exploits a set of heuristic rules that are automatically applied in a seven-steps process. The applicability of the process has been shown in an industrial context where it was used to obtain the UML class diagrams representing the conceptual data models of three spreadsheet-based information systems.
Download
Paper Nr:	2
Title:	Are the Methodologies for Producing Linked Open Data Feasible for Public Administrations?
Authors:	Roberto Boselli, Mirko Cesarini, Fabio Mercorio and Mario Mezzanzanica
Abstract:	Linked Open Data (LOD) enable the semantic interoperability of Public Administration (PA) information. Moreover, they allow citizens to reuse public information for creating new services and applications. Although there are many methodologies and guidelines to produce and publish LOD, the PAs still hardly understand and exploit LOD to improve their activities. In this paper we show the use of a set of best practices to support an Italian PA in producing LOD. We show the case of LOD production from existing open datasets related to public services. Together with the production of LOD we present the definition of a reference ontology, the Public Service Ontology, integrated with the datasets. During the application, we highlight and discuss some critical points we found in methodologies and technologies described in the literature, and we identify some potential improvements.
Download
Paper Nr:	8
Title:	A Pattern-Oriented Approach for Supporting ETL Conceptual Modelling and Its YAWL-based Implementation
Authors:	Bruno Oliveira, Orlando Belo and Alfredo Cuzzocrea
Abstract:	Modelling and implementing Data Warehouse populating processes (mainly known as ETL) involves in complex and challenge tasks that have been highlighted by numerous researchers in the field. Several workflow modelling languages, mainly used for supporting Business Processes modelling, such as BPMN, BPEL or UML AD, have been adapted and (sometimes) used to model ETL processes with the goal of successfully conceptual-modelling them and enabling their posteriorly executions. However, there is a large bridge still to be built between abstract models, which are typically used in initial phases of projects, and their subsequent implementation in real-world practical scenarios. Furthermore, ETL processes are strongly related to business requirements, which are often affected by business evolution and business operational systems. Hence, with the goal of minimizing such gaps, in this paper we propose the use of a pattern-oriented approach for supporting ETL conceptual modelling, where common tasks are identified and formalized in order to describe their potential behaviour, thus allowing for its application and reuse in different application scenarios.
Short Papers
Paper Nr:	3
Title:	Data Preparation for Tourist Data Big Data Warehousing
Authors:	Nunziato Cassavia, Pietro Dicosta, Elio Masciari and Domenico Saccà
Abstract:	The pervasive diffusion of new generation devices like smart phones and tablets along with the widespread use of social networks causes the generation of massive data flows containing heterogeneous information generated at different rates and having different formats. These data are referred as Big Data and require new storage and analysis approaches to be investigated for managing them. In this paper we will describe a system for dealing with massive tourism flows that we exploited for the analysis of tourist behavior in Italy. We defined a framework that exploits a NoSQL approach for data management and map reduce for improving the analysis of the data gathered from different sources.
Download
Paper Nr:	4
Title:	A Clustering-based Approach for a Finest Biological Model Generation Describing Visitor Behaviours in a Cultural Heritage Scenario
Authors:	Salvatore Cuomo, Pasquale De Michele, Giovanni Ponti and Maria Rosaria Posteraro
Abstract:	We propose a biologically inspired mathematical model to simulate the personalized interactions of users with cultural heritage objects. The main idea is to measure the interests of a spectator w.r.t. an artwork by means of a model able to describe the behaviour dynamics. In this approach, the user is assimilated to a computational neuron, and its interests are deduced by counting potential spike trains, generated by external currents. The main novelty of our approach consists in resorting to clustering task to discover natural groups, which are used in the next step to verify the neuronal response and to tune the computational model. Preliminary experimental results, based on a phantom database and obtained from a real world scenario, are shown. To discuss the obtained results, we report a comparison between the cluster memberships and the spike generation; our approach resulted to perfectly model cluster assignment and spike emission.
Download
Paper Nr:	5
Title:	A Method of Topic Detection for Great Volume of Data
Authors:	Flora Amato, Francesco Gargiulo, Antonino Mazzeo and Carlo Sansone
Abstract:	Topics extraction has become increasingly important due to its effectiveness in many tasks, including information filtering, information retrieval and organization of document collections in digital libraries. The Topic Detection consists to find the most significant topics within a document corpus. In this paper we explore the adoption of a methodology of feature reduction to underline the most significant topics within a document corpus. We used an approach based on a clustering algorithm (X-means) over the t f −id f matrix calculated starting from the corpus, by which we describe the frequency of terms, represented by the columns, that occur in each document, represented by a row. To extract the topics, we build n binary problems, where n is the numbers of clusters produced by an unsupervised clustering approach and we operate a supervised feature selection over them considering the top features as the topic descriptors. We will show the results obtained on two different corpora. Both collections are expressed in Italian: the first collection consists of documents of the University of Naples Federico II, the second one consists in a collection of medical records.
Download
Paper Nr:	6
Title:	A Semantic Content Management System for e-Gov Applications
Authors:	Donato Cappetta, Salvatore D'Elena, Vincenzo Moscato, Vincenzo Orabona, Raffaele Palmieri and Antonio Picariello
Abstract:	In this paper, we describe a novel Semantic Content Management System (SCMS) able to handle multimedia contents of different kinds (e.g. texts and images) using the related semantics and capable of supporting e-gov applications in different scenarios. All the information is described using semantic metadata semiautomatically extracted from multimedia data, which enriches the browsing experience and enables semantic contents’ authoring and queries. To this aim, several Semantic Web technologies have been exploited : RDF/OWL for data modeling and representation, SPARQL as querying language, Multimedia Information Extraction techniques for content annotation, W3C standard models, vocabularies and micro-formats for resource description. In addition, we propose for entity annotation issue the LOD approach. As an application scenario of the platform, we report a system customization useful for managing the semantic matching between the required professional profiles by a Public Administration and the available skills in a set of curricula vitae with respect to a given call.
Download
Paper Nr:	7
Title:	Discovering Expected Activities in Medical Context Scientific Databases
Authors:	Daniela D'Auria and Fabio Persia
Abstract:	Reasoning with temporal data has attracted the attention of many researchers from different backgrounds including artificial intelligence, database management, computational linguistics and biomedical informatics. More specifically, activity detection is a very important problem in a wide variety of application domains such as video surveillance, cyber security, fault detection, but also clinical research. Thus, in this paper we present a prototype architecture designed and developed for activity detection in the medical context. In more detail, we first acquire data in real time from a cricothyrotomy simulator, when used by medical doctors, then we store the acquired data into a scientific database and finally we use an Activity Detection Engine for finding expected activities, corresponding to specific performances obtained by the medical doctors when using the simulator. Some preliminary experiments using real data show the approach efficiency and effectiveness. Eventually, we also received positive feedbacks by the medical personnel who used our prototype.
Download"
2015-a,""
2015-b,"Full Papers
Paper Nr:	3
Title:	Visitor Dynamics in a Cultural Heritage Scenario
Authors:	Salvatore Cuomo, Pasquale De Michele, Ardelio Galletti, Francesco Pane and Giovanni Ponti
Abstract:	We propose a biologically inspired mathematical model to simulate the personalized interactions of users with cultural heritage objects and spaces in the real case of an exhibition. The main idea is to measure the interests of a spectator with respect to an artwork by means of a model able to describe the users behavioural dynamics. In our approach, the user is assimilated to a computational neuron, and its interests are deduced by counting potential spike trains, generated by external currents. As an effort, we relies on an huge amount of log files that store visitors movements and interactions within a beautiful art exhibition named The Beauty or the Truth located in Naples, Italy. The technological tools deployed within the exhibition aim to create a novel metaphor stimulating user enjoyment and knowledge diffusion and the collected log files are useful data to analyse how such technology an influence and modify user behaviours. We also performed an experimental analysis exploiting clustering facilities to discover natural groups that reflect visiting styles. This is particularly suitable to provide the tuning of a heuristic classifier. The obtained results revealed to be particularly interesting also to understand other important aspects hidden in the data and unattended in our first analysis.
Download
Paper Nr:	4
Title:	A Novel Approach to Query Expansion based on Semantic Similarity Measures
Authors:	Flora Amato, Aniello De Santo, Francesco Gargiulo, Vincenzo Moscato, Fabio Persia, Antonio Picariello and Giancarlo Sperlì
Abstract:	In this paper, we present a framework supporting information retrieval over corpora of documents using an automatic semantic query expansion approach. The main idea is to expand the set of words used as query terms exploiting the notion of semantic similarity between the concepts related to the search terms. We leverage existing lexical resources and similarity metrics computed among terms to generate - by a proper mapping into a vectorial space - an index for the fast retrieval of a set of terms “semantically correlated” to a given query term. The vector of expanded terms is then exploited in the query stage to retrieve documents that are significantly related to specific combinations of the query terms. Preliminary experimental results concerning efficiency and effectiveness of the proposed approach are reported and discussed.
Download
Paper Nr:	7
Title:	Applying the AHP to Smart Mobility Services: A Case Study
Authors:	Roberto Boselli, Mirko Cesarini, Fabio Mercorio and Mario Mezzanzanica
Abstract:	Making decision is a far from straightforward process, as it often requires to consider a number of complex criteria whose importance relies on the experiences and the preferences of the decision makers involved. Being able to structure and reproduce this knowledge is a challenging issue in the context of strategic decision making, and also common BI analytics can benefit from the joint use of that knowledge. As a contribution, in this work we describe how a multi criteria decision making technique, i.e., the Analytic Hierarchy Process, has been applied to a smart-mobility context, where the decision goal was to weight the factors that support the innovation of a smart mobility service in the city of Milan. The AHP has been selected as it allows considering both tangible and intangible factors that guide the decision within the model. We employed three distinct kind of stakeholders, namely service providers, over 35, and under 35 users and we synthesised a ranking of criteria on the basis of the preferences they provided. The results shed the light on the different judgments that each group gives to the identified criteria in terms of both ranking and importance.
Download
Paper Nr:	8
Title:	Big Data: A Survey - The New Paradigms, Methodologies and Tools
Authors:	Enrico Giacinto Caldarola and Antonio Maria Rinaldi
Abstract:	For several years we are living in the era of information. Since any human activity is carried out by means of information technologies and tends to be digitized, it produces a humongous stack of data that becomes more and more attractive to different stakeholders such as data scientists, entrepreneurs or just privates. All of them are interested in the possibility to gain a deep understanding about people and things, by accurately and wisely analyzing the gold mine of data they produce. The reason for such interest derives from the competitive advantage and the increase in revenues expected from this deep understanding. In order to help analysts in revealing the insights hidden behind data, new paradigms, methodologies and tools have emerged in the last years. There has been a great explosion of technological solutions that arises the need for a review of the current state of the art in the Big Data technologies scenario. Thus, after a characterization of the new paradigm under study, this work aims at surveying the most spread technologies under the Big Data umbrella, throughout a qualitative analysis of their characterizing features.
Download
Short Papers
Paper Nr:	1
Title:	Surfing Big Data Warehouses for Effective Information Gathering
Authors:	Nunziato Cassavia, Pietro Dicosta, Elio Masciari and Domenico Saccà
Abstract:	Due to the emerging Big Data paradigm traditional data management techniques result inadequate in many real life scenarios. In particular, OLAP techniques require substantial changes in order to offer useful analysis due to huge amount of data to be analyzed and their velocity and variety. In this paper, we describe an approach for dynamic Big Data searching that based on data collected by a suitable storage system, enriches data in order to guide users through data exploration in an efficient and effective way.
Download
Paper Nr:	5
Title:	An Application of Semantic Web Technologies to Enhance Content Management in Web Information Portals
Authors:	Vincenzo Orabona, Raffaele Palmieri, Vincenzo Moscato, Antonio Picariello, Salvatore D'Elena and Donato Cappetta
Abstract:	As well known, Semantic Web technologies make available a set of facilities that allow data to be shared and reused across applications. The last generation of Content Management System (CMS) can leverage such technologies to improve the content management task incorporating semantic annotations of the produced resources. Here, we present the benefits deriving from the application of semantic technologies in a CMS environment. To this goal, we collect preliminary results about the effectiveness of the integration of a semantic annotation engine within the Intrage Web Portal for content management purposes. The obtained results show that the approach is quite promising and encourage the current research.
Download"
2016-a,"Area 1 - Business Analytics
Full Papers
Paper Nr:	24
Title:	Comparison of Distributed Computing Approaches to Complexity of n-gram Extraction
Authors:	Sanzhar Aubakirov, Paulo Trigo and Darhan Ahmed-Zaki
Abstract:	In this paper we compare different technologies that support distributed computing as a means to address complex tasks. We address the task of n-gram text extraction which is a big computational given a large amount of textual data to process. In order to deal with such complexity we have to adopt and implement parallelization patterns. Nowadays there are several patterns, platforms and even languages that can be used for the parallelization task. We implemented this task on three platforms: (1) MPJ Express, (2) Apache Hadoop, and (3) Apache Spark. The experiments were implemented using two kinds of datasets composed by: (A) a large number of small files, and (B) a small number of large files. Each experiment uses both datasets and the experiment repeats for a set of different file sizes. We compared performance and efficiency among MPJ Express, Apache Hadoop and Apache Spark. As a final result we are able to provide guidelines for choosing the platform that is best suited for each kind of data set regarding its overall size and granularity of the input data.
Download
Paper Nr:	38
Title:	Management of Data Quality Related Problems - Exploiting Operational Knowledge
Authors:	Mortaza S. Bargh, Jan van Dijk and Sunil Choenni
Abstract:	Dealing with data quality related problems is an important issue that all organizations face in realizing and sustaining data intensive advanced applications. Upon detecting these problems in datasets, data analysts often register them in issue tracking systems in order to address them later on categorically and collectively. As there is no standard format for registering these problems, data analysts often describe them in natural languages and subsequently rely on ad-hoc, non-systematic, and expensive solutions to categorize and resolve registered problems. In this contribution we present a formal description of an innovative data quality resolving architecture to semantically and dynamically map the descriptions of data quality related problems to data quality attributes. Through this mapping, we reduce complexity – as the dimensionality of data quality attributes is far smaller than that of the natural language space – and enable data analysts to directly use the methods and tools proposed in literature. Furthermore, through managing data quality related problems, our proposed architecture offers data quality management in a dynamic way based on user generated inputs. The paper reports on a proof of concept tool and its evaluation.
Download
Short Papers
Paper Nr:	30
Title:	An Ontology for Describing ETL Patterns Behavior
Authors:	Bruno Oliveira and Orlando Belo
Abstract:	The use of software patterns is a common practice in software design, providing reusable solutions for recurring problems. Patterns represent a general skeleton used to solve common problems, providing a way to share regular practices and reduce the resources needed for implementing software systems. Data warehousing populating processes are a very particular type of software used to migrate data from one or more data sources to a specific data schema used to support decision support activities. The quality of such processes should be guarantee. Otherwise, the final system will deal with data inconsistencies and errors, compromising its suitability to support strategic business decisions. To minimize such problems, we propose a pattern-oriented approach to support ETL lifecycle, from conceptual representation to its execution primitives using a specific commercial tool. An ontology-based meta model it was designed and used for describing patterns internal specification and providing the means to support and enable its configuration and instantiation using a domain specific language.
Download
Paper Nr:	36
Title:	Management of Scientific Documents and Visualization of Citation Relationships using Weighted Key Scientific Terms
Authors:	Hui Wei, Youbing Zhao, Shaopeng Wu, Zhikun Deng, Farzad Parvinzamir, Feng Dong, Enjie Liu and Gordon Clapworthy
Abstract:	Effective management and visualization of scientific and research documents can greatly assist researchers by improving understanding of relationships (e.g. citations) between the documents. This paper presents work on the management and visualization of large corpuses of scientific papers in order to help researchers explore their citation relationships. Term selection and weighting are used for mining citation relationships by identifying the most relevant. To this end, we present a variation of the TF-IDF scheme, which uses external domain resources as references to calculate the term weighting in a particular domain; document weighting is taken into account in the calculation of term weighting from a group of citations. A simple hierarchical word weighting method is also presented. The work is supported by an underlying architecture for document management using NoSQL databases and employs a simple visualization interface.
Download
Paper Nr:	39
Title:	Map-reduce Implementation of Belief Combination Rules
Authors:	Frédéric Dambreville
Abstract:	This paper presents a generic and versatile approach for implementing combining rules on preprocessed belief functions, issuing from a large population of information sources. In this paper, we address two issues, which are the intrinsic complexity of the rules processing, and the possible large amount of requested combinations. We present a fully distributed approach, based on a map-reduce (Spark) implementation.
Download
Paper Nr:	52
Title:	Urban Gamification as a Source of Information for Spatial Data Analysis and Predictive Participatory Modelling of a City’s Development
Authors:	Robert Olszewski, Agnieszka Turek and Marcin Łączyński
Abstract:	The basic problem in predictive participatory urban planning is activating residents of a city, e.g. through the application of the technique of individual and/or team gamification. The authors of the article developed (and tested in Płock) a methodology and prototype of an urban game called “Urban Shaper”. This permitted obtaining a vast collection of opinions of participants on the directions of potential development of the city. The opinions, however, are expressed in an indirect manner. Therefore, their analysis and modelling of participatory urban development requires the application of extended algorithms of spatial statistics. The collected source data are successively processed by means of spatial data mining techniques, permitting activation of condensed spatial knowledge based on “raw” source data with high volume (big data).
Download
Paper Nr:	17
Title:	Database Buffer Cache Simulator to Study and Predict Cache Behavior for Query Execution
Authors:	Chetan Phalak, Rekha Singhal and Tanmay Jhunjhunwala
Abstract:	Usage of an electronic media is increasing day by day and consequently the usage of applications. This fact has resulted in rapid growth of an application's data which may lead to violation of service level agreement (SLA) given to its users. To keep applications SLA compliance, it is necessary to predict the query response time before its deployment. The query response time comprises of two elements, computation time and IO access time. The latter includes time spent in getting data from disk subsystem and database/operating system (OS) cache. Correct prediction of a query performance needs to model cache behavior for growing data size. The complex nature of data storage and data access pattern by queries brings in difficulty to use only mathematical model for cache behavior prediction. In this paper, a Database Buffer Cache Simulator has been proposed, which mimics the behavior of the database buffer cache, which can be used to predict the cache misses for different types of data access by a query. The simulator has been validated using Oracle 11g and TPC-H benchmarks. The simulator is able to predict cache misses with an average error of 2%.
Download
Paper Nr:	21
Title:	Evaluating Open Source Data Mining Tools for Business
Authors:	Pedro Almeida, Le Gruenwald and Jorge Bernardino
Abstract:	Businesses are struggling to stay ahead of competition in a globalized economy where there are more and stronger competitors. Managers are constantly looking for advantages that can generate benefits at low costs. One way to have such advantage is using the data about customers, demographic data, purchase history, customer behavior and preferences that can help to take better business decisions. Data Mining addresses the challenges of collecting value inside data and the ways to put that value to use for virtually any area of our lives, including business. In this paper, we address the interest of Data Mining for business and analyze three popular Open Source Data Mining Tools – KNIME, Orange and RapidMiner – considered as a good starting point for enterprises to begin exploring the power of Data Mining and its benefits.
Download
Paper Nr:	53
Title:	Dimensioning and Optimizing TaaS for Improved Performance in Telecom System with Text Analytics Features
Authors:	M. Saravanan and Harivadhana Ravi
Abstract:	To cope with speed in ICT developments, the industries are venturing different testing schemes by enabling the advancement in Mobile Cloud Computing, which has given way to meet the aggressive delivery schedules and high demand to reduce costs. Test automation can help to reduce the time to market by achieving best quality standards in end product within limited schedule and budget. In the existing Testing as a Service (TaaS), the on-demand execution of well-defined test suites will be done on an outsourced basis. These instal-lations would be effective and adaptable if the TaaS is customized based on-demand and integrated to the real-time environment. In this paper we propose a new service framework which automates the test suite by con-verting the existing regression test suite procedure into intelligible TaaS operation by augmenting text categori-zation, dimensioning and optimizing techniques. The proposed DOTaaS framework induces robust regression testing with the aim of reducing cost and providing reliable quality assurance. Test suites are categorized by understanding the semantics of the text and marshalling it under its corresponding classifier label. Dimension-ing is done through the technique of text analytics with the application of graphical models that help to reduce the suite by merging and revamping of the existing test cases. The proposed service framework can also be easily realized in any new environment with an advantage of providing stability. The pragmatic uses of dimen-sioning and optimizing techniques in automated testing environment were evaluated to prove the effectiveness.
Area 2 - Data Management and Quality
Full Papers
Paper Nr:	18
Title:	ISE: A High Performance System for Processing Data Streams
Authors:	Paolo Cappellari, Soon Ae Chun and Mark Roantree
Abstract:	Many organizations require the ability to manage high-volume high-speed streaming data to perform analysis and other tasks in real-time. In this work, we present the Information Streaming Engine, a high-performance data stream processing system capable of scaling to high data volumes while maintaining very low-latency. The Information Streaming Engine adopts a declarative approach which enables processing and manipulation of data streams in a simple manner. Our evaluation demonstrates the high levels of performance achieved when compared to existing systems.
Download
Paper Nr:	50
Title:	A Novel Method for Unsupervised and Supervised Conversational Message Thread Detection
Authors:	Giacomo Domeniconi, Konstantinos Semertzidis, Vanessa Lopez, Elizabeth M. Daly, Spyros Kotoulas and Gianluca Moro
Abstract:	Efficiently detecting conversation threads from a pool of messages, such as social network chats, emails, comments to posts, news etc., is relevant for various applications, including Web Marketing, Information Retrieval and Digital Forensics. Existing approaches focus on text similarity using keywords as features that are strongly dependent on the dataset. Therefore, dealing with new corpora requires further costly analyses conducted by experts to find out new relevant features. This paper introduces a novel method to detect threads from any type of conversational texts overcoming the issue of previously determining specific features for each dataset. To automatically determine the relevant features of messages we map each message into a three dimensional representation based on its semantic content, the social interactions in terms of sender/recipients and its timestamp; then clustering is used to detect conversation threads. In addition, we propose a supervised approach to detect conversation threads that builds a classification model which combines the above extracted features for predicting whether a pair of messages belongs to the same thread or not. Our model harnesses the distance measure of a message to a cluster representing a thread to capture the probability that a message is part of that same thread. We present our experimental results on seven datasets, pertaining to different types of messages, and demonstrate the effectiveness of our method in the detection of conversation threads, clearly outperforming the state of the art and yielding an improvement of up to a 19%.
Download
Short Papers
Paper Nr:	14
Title:	Performance Evaluation of Phonetic Matching Algorithms on English Words and Street Names - Comparison and Correlation
Authors:	Keerthi Koneru, Venkata Sai Venkatesh Pulla and Cihan Varol
Abstract:	Researchers confront major problems while searching for various kinds of data in the large imprecise database, as they are not spelled correctly or in the way they were expected to be spelled. As a result, they cannot find the word they sought. Over the years of struggle, pronunciation of words was considered to be one of the practices to solve the problem effectively. The technique used to acquire words based on sounds is known as “Phonetic Matching”. Soundex is the first algorithm proposed and other algorithms like Metaphone, Caverphone, DMetaphone, Phonex etc., are also used for information retrieval in different environments. This paper deals with the analysis and evaluation of different phonetic matching algorithms on several datasets comprising of street names of North Carolina and English dictionary words. The analysis clearly states that there is no clear best technique for generic word lists as Metaphone has best performance for English dictionary words, while NYSIIS has better performance for datasets having street names. Though Soundex has high accuracy in correcting the exact words compared to other algorithms, it has lower precision due to more noise in the considered arena. The experimental results paved way for introducing some suggestions that would aid to make databases more concrete and achieve higher data quality.
Download
Paper Nr:	55
Title:	Architecture of a Multi-domain Processing and Storage Engine
Authors:	Johannes Luong, Dirk Habich, Thomas Kissinger and Wolfgang Lehner
Abstract:	In today’s data-driven world, economy and research depend on the analysis of empirical datasets to guide decision making. These applications often encompass a rich variety of data types and special purpose processing models. We believe, the database system of the future will integrate flexible processing and storage of a variety of data types in a scalable and integrated end-to-end solution. In this paper, we propose a database system architecture that is designed from the core to support these goals. In the discussion we will especially focus on the multi-domain programming concept of the proposed architecture that exploits domain specific knowledge to guide compiler based optimization.
Download
Paper Nr:	29
Title:	Test-Data Quality as a Success Factor for End-to-End Testing - An Approach to Formalisation and Evaluation
Authors:	Yury Chernov
Abstract:	Test-data quality can be decisive in the success of end-to-end testing of complicated multi-component and distributed systems. The proper metrics allows to compare different data sets and to evaluate their quality. The typical data quality factors should be, on the one hand, enriched with the dimensions specific for the testing, and, on the other hand many requirements to the production system data quality become not relevant. The proposed formal model is based to great extent on the common sense and practical experience, and not over-formalised. The implementation requires quite an effort, but once established can be very useful.
Download
Paper Nr:	43
Title:	Simultaneous Approximation of Several Non-uniformly Distributed Values within Histogram Buckets
Authors:	Wissem Labbadi and Jalel Akaichi
Abstract:	The problem of estimating median and other quantiles without storing observations was first proposed, in the field of simulation modeling to improve the studying of the performance of modeled systems rather than relying only on the mean and standard deviation alone. This problem is then extended to histogram plotting which raises the problem of estimating many quantiles of the same variable. However, the calculation of several values simultaneously is a computationally complex task since it requires several passes through the data. We propose in this paper an extension of the state-of the-art Values Approximation algorithm (Labbadi and Akichi, 2014) to estimate simultaneously several values within a histogram bucket. The extended algorithm significantly reduces the computation of the estimation since the original algorithm estimates only a single value. The experimental results show that the extended algorithm provides good estimates especially when data have non-equal spreads.
Paper Nr:	45
Title:	Social Evaluation of Learning Material
Authors:	Paolo Avogadro, Silvia Calegari and Matteo Dominoni
Abstract:	In academic environments the success of a course is given by the interaction among students, teachers and learning material. This paper is focused on the definition of a model to establish the quality of learning material within a Social Learning Management System (Social LMS). This is done by analyzing how teachers and students interact by: (1) objective evaluations (e.g., grades), and (2) subjective evaluations (e.g., social data from the Social LMS). As a reference, we use the Kirkpatrick-Philips model to characterize learning material with novel key performance indicators. As an example, we propose a social environment where students and teachers interact with the help of a wall modified for the evaluation of learning material.
Download
Area 3 - Ontologies and the Semantic Web
Short Papers
Paper Nr:	15
Title:	Incorporating Collaborative Bookmark Recommendation with Social Network Analysis
Authors:	C.-L. Huang and J.-H. Ho
Abstract:	Users collect internet resources and follow other users on the social bookmark sharing websites. Based on the interest-based social network, a hybrid recommender system incorporating collaborative and content-based filtering was built to suggest bookmark resources in this study. In the collaborative filtering, the recommender system discovers collaborative users who have similar interest with the target user; in the content-based filtering, the recommender system suggests bookmark items to the target user. An experiment was conducted to demonstrate the performances of the proposed system based on the dataset collected from a famous social bookmark sharing website.
Paper Nr:	40
Title:	RESTful Encapsulation of OWL API
Authors:	Ramya Dirsumilli and Till Mossakowski
Abstract:	OWL API is a high level API for working with ontologies. Despite of its functionalities and numerous advantages, it is restricted to a set of users due to its platform dependency. Being built as a java API the OWL API can only be used by java or related platform users. The major goal of this paper is to design a RESTful web interface of OWL API methods, such that ontology developers and researchers independent of platform could work with OWL API. This RESTful OWL API tool is designed to exhibit all the functionalities of OWL API that do not deal with rendering the input ontology such that it doesn't behave as an ontology editor, instead supports web ontology developers and open ontology repositories such as Ontohub.
Download
Area 4 - Databases and Data Security
Short Papers
Paper Nr:	20
Title:	Scalable Versioning for Key-Value Stores
Authors:	Martin Haeusler
Abstract:	Versioning of database content is rapidly gaining importance in modern applications, due to the need for reliable auditing, data history analysis, or due to the fact that temporal information is inherent to the problem domain. Data volume and complexity also increase, demanding a high level of scalability. However, implementations are rarely found in practice. Existing solutions treat versioning as an add-on instead of a first-class citizen, and therefore fail to take full advantage of its benefits. Often, there is also a trade-off between performance and the age of an entry, with newer entries being considerably faster to retrieve. This paper provides three core contributions. First, we provide a formal model that captures and formalizes the properties of the temporal indexing problem in an intuitive way. Second, we provide an in-depth discussion on the unique benefits in transaction control which can be achieved by treating versioning as a first-class citizen in a data store as opposed to treating it as an add-on feature to a non-versioned system. We also introduce an index model that offers equally fast access to all entries, regardless of their age. The third contribution is an opensource implementation of the presented formalism in the form of a versioned key-value store, which serves as a proof-of-concept prototype. An evaluation of this prototype demonstrates the scalability of our approach.
Download
Paper Nr:	31
Title:	Adopting Workflow Patterns for Modelling the Allocation of Data in Multi-Organizational Collaborations
Authors:	Ayalew Kassahun and Bedir Tekinerdogan
Abstract:	Currently, many organizations need to collaborate to achieve their common objectives. An important aspect hereby is the data required for making decisions at anyone organization may be distributed over the different organizations involved in the collaboration. The data objects and the activities in which they are generated or used are typically represented using business process models. Unfortunately, the existing business process modeling approaches are limited in representing the complex data allocation dimensions that occur in the context of multi-organization collaboration. In this paper we provide a systematic approach that adopts workflow data patterns to explicitly model data allocations in multi-organization collaboration. To this end we propose a design viewpoint that integrates business process models with well-known data allocation problem-solution pairs defined as workflow data patterns. We illustrate the viewpoint using a case study of a collaboration research project.
Download
Paper Nr:	33
Title:	Fault Tolerance Logging-based Model for Deterministic Systems
Authors:	Óscar Mortágua Pereira, David Simões and Rui L. Aguiar
Abstract:	Fault tolerance allows a system to remain operational to some degree when some of its components fail. One of the most common fault tolerance mechanisms consists on logging the system state periodically, and recovering the system to a consistent state in the event of a failure. This paper describes a general fault tolerance logging-based mechanism, which can be layered over deterministic systems. Our proposal describes how a logging mechanism can recover the underlying system to a consistent state, even if an action or set of actions were interrupted mid-way, due to a server crash. We also propose different methods of storing the logging information, and describe how to deploy a fault tolerant master-slave cluster for information replication. We adapt our model to a previously proposed framework, which provided common relational features, like transactions with atomic, consistent, isolated and durable properties, to NoSQL database management systems.
Download
Paper Nr:	34
Title:	Cassandra’s Performance and Scalability Evaluation
Authors:	Melyssa Barata and Jorge Bernardino
Abstract:	In the past, relational databases were the most commonly used technology for storing and retrieving data, allowing easier management and retrieval of any stored information organized as a set of tables. However, today databases are larger in size and the query execution time can become very long, requiring servers with bigger capacities. The purpose of this paper is to describe and analyze the Cassandra NoSQL database using the Yahoo! Cloud Serving Benchmark in order to better understand the execution capabilities for various types of applications in environments with different amounts of stored data. The experiments with Cassandra show good scalability and performance results and how the database size and number of nodes affect it.
Download
Paper Nr:	46
Title:	Boosting an Embedded Relational Database Management System with Graphics Processing Units
Authors:	Samuel Cremer, Michel Bagein, Saïd Mahmoudi and Pierre Manneback
Abstract:	Concurrently, with the rise of Big Data systems, relational database management systems (RDBMS) are still widely exploited in servers, client devices, and even embedded inside end-user applications. In this paper, it is suggest to improve the performance of SQLite, the most deployed embedded RDBMS. The proposed solution, named CuDB, is an ”In-Memory” Database System (IMDB) which attempts to exploit specificities of modern CPU / GPU architectures. In this study massively parallel processing was combined with strategic data placement, closer to computing units. According to content and selectivity of queries, the measurements reveal an acceleration range between 5 to 120 times - with peak up to 411 - with one GPU GTX770 compared to SQLite standard implementation on a Core i7 CPU.
Download"
2017-a,"Area 1 - Big Data
Full Papers
Paper Nr:	18
Title:	Adaptive Resource Management for Distributed Data Analytics based on Container-level Cluster Monitoring
Authors:	Thomas Renner, Lauritz Thamsen and Odej Kao
Abstract:	Many distributed data analysis jobs are executed repeatedly in production clusters. Examples include daily executed batch jobs and iterative programs. These jobs present an opportunity to learn workload characteristics through continuous fine-grained cluster monitoring. Therefore, based on detailed profiles of resource utilization, data placement, and job runtimes, resource management can in fact adapt to actual workloads. In this paper, we present a system architecture that contains four mechanisms for an adaptive resource management, encompassing data placement, resource allocation, and container as well as job scheduling. In particular, we extended Apache Hadoop's scheduling and data placement to improve resource utilization and job runtimes for recurring analytics jobs. Furthermore, we developed a Hadoop submission tool that allows users to reserve resources for specific target runtimes and which uses historical data available from cluster monitoring for predictions.
Download
Short Papers
Paper Nr:	23
Title:	The Benefit of Thinking Small in Big Data
Authors:	Kurt Englmeier and Hernán Astudillo Rojas
Abstract:	No doubt, big data technology can be a key enabler for data-driven decision making. However, there are caveats. Processing technology for unstructured and structured data alone–with or without Artificial Intelligence–will not suffice to catch up the promises made by big data pundits. This article argues that we should be level-headed about what we can achieve with big data. We can achieve a lot of these promises if we also achieve to get our interests and requirements better reflected in design or adaptation of big data technology. Economy of scale urges provider of big data technology to address mainstream requirements, that is, analytic requirements of a broad clientele. Our analytical problems, however, are rather individual, albeit mainstream only to a certain extent. We will see many technology add-ons for specific requirements, with more emphasis on human interaction too, that will be essential for the success in big data. In this article, we take machine translation as an example and a prototypical translation memory as add-on technology that supports users to turn the faulty automatic translation into a useful one.
Download
Paper Nr:	43
Title:	Towards a Scalable Architecture for Flight Data Management
Authors:	Iván García, Miguel A. Martínez-Prieto, Anibal Bregón, Pedro C. Álvarez and Fernando Díaz
Abstract:	The dramatic growth in the air traffic levels witnessed during the last two decades has increased the interest for optimizing the Air Traffic Management (ATM) systems. The main objective is being able to cope with the sustained air traffic growth under safe, economic, efficient and environmental friendly working conditions. The ADS-B (Automatic Dependent Surveillance - Broadcast) system is part of the new air traffic control systems, since it allows to substitute the secondary radar with cheaper ground stations that, at the same time, provide more accurate real-time positioning information. However, this system generates a large volume of data that, when combined with other flight-related data, such as flight plans or weather reports, faces scalability issues. This paper introduces an (on-going) Data Lake based architecture which allows the full ADS-B data life-cycle to be supported in a scalable and cost-effective way using technologies from the Apache Hadoop ecosystem.
Download
Paper Nr:	17
Title:	Identification of Opinion Leaders and Followers in Social Media
Authors:	Chun-Che Huang, Li-Ching Lien, Po-An Chen, Tzu-Liang Tseng and Shian-Hua Lin
Abstract:	In recent years, with the development of Web2.0, opinion leaders on the Web go up onto the stage and lead the will of the people. Many time, government, private companies and even traditional news media need to understand the opinion leaders’ ideas on the Web. Identifying opinion leaders and followers becomes a very important study. To study the characteristics of opinion leaders and the impact of opinion leaders on followers, our research evaluates whether every speaker in social media satisfies characteristics of opinion leader. The characteristics of opinion leader and relationship between opinion leader and follower are studied. By observing relational matrix, the interacting relations between users in social media are analysed and opinion leaders and followers are identified.
Download
Paper Nr:	35
Title:	Index Clustering: A Map-reduce Clustering Approach using Numba
Authors:	Xinyu Chen and Trilce Estrada
Abstract:	Clustering high-dimensional data is often a crucial step of many applications. However, the so called ""Curse of dimensionality"" is a challenge for most clustering algorithms. In such high-dimensional spaces, distances between points tend to be less meaningful and the spaces become sparse. Such sparsity needs more data points to characterize the similarities so more distance comparisons are computed. Many approaches have been proposed for reduction of dimensionality, such as sub-space clustering, random projection clustering, and feature selection technique. However, approaches like these become unfeasible in scenarios where data is geographically distributed or cannot be openly used across sites. To deal with the location and privacy issues as well as mitigate the expensive distance computation, we propose an index-based clustering algorithm that generates a spatial \emph{key} for each data point across all dimensions without needing an explicit knowledge of the other data points. Then it performs a conceptual Map-Reduce procedure in the index space to form a final clustering assignment. Our results show that this algorithm is linear and can be parallelized and executed independently across points and dimensions. We present a Numba implementation and preliminary study of this algorithm's capabilities and limitations.
Download
Area 2 - Business Analytics
Full Papers
Paper Nr:	10
Title:	Asymmetric Heterogeneous Transfer Learning: A Survey
Authors:	Magda Friedjungová and Marcel Jiřina
Abstract:	One of the main prerequisites in most machine learning and data mining tasks is that all available data originates from the same domain. In practice, we often can’t meet this requirement due to poor quality, unavailable data or missing data attributes (new task, e.g. cold-start problem). A possible solution can be the combination of data from different domains represented by different feature spaces, which relate to the same task. We can also transfer the knowledge from a different but related task that has been learned already. Such a solution is called transfer learning and it is very helpful in cases where collecting data is expensive, difficult or impossible. This overview focuses on the current progress in the new and unique area of transfer learning - asymmetric heterogeneous transfer learning. This type of transfer learning considers the same task solved using data from different feature spaces. Through suitable mappings between these different feature spaces we can get more data for solving data mining tasks. We discuss approaches and methods for solving this type of transfer learning tasks. Furthermore, we mention the most used metrics and the possibility of using metric or similarity learning.
Download
Paper Nr:	12
Title:	The Role of Big Data Analytics in Corporate Decision-making
Authors:	Darlan Arruda and Nazim H. Madhavji
Abstract:	Big Data Analytics results can play a major role in corporate decision-making allowing companies to achieve competitive advantage and make improved decisions. This paper describes a systematic literature review (SLR) on the role of the results of Big Data Analytics in corporate decisions. Initially, 1652 papers were identified from various sources. Filtering through the 5-step process, 20 relevant studies were selected for analysis in this SLR. The findings of this study are fourfold in the area of: (a) usage of the results of Big Data Analytics in corporate decision-making; (b) the types of business functions where analytics has been fruitfully utilised; (c) the impact of analytics on decision-making; and (d) the impediments to using Big Data Analytics in corporate decision-making. Also, on the management front, two important issues identified are: (i) aligning data-driven decision-making with business strategy and (ii) collaboration across business functions for effective flow of Big Data and information. On the technical front, big data present some challenges due to the lack of tools to process such properties of Big Data as variety, veracity, volume, and velocity. We observe from this analysis that, thus far, little scientific research has focused on understanding how to address the analytics results in corporate decision-making. This paper ends with some recommendations for further research in this area.
Download
Paper Nr:	26
Title:	Churn Prediction for Mobile Prepaid Subscribers
Authors:	Zehra Can and Erinç Albey
Abstract:	In telecommunication, mobile operators prefer to acquire postpaid subscribers and increase their incoming revenue based on the usage of postpaid lines. However, subscribers tend to buy and use prepaid mobile lines because of the simplicity of the usage, and due to higher control over the cost of the line compared to postpaid lines. Moreover the prepaid lines have less paper work between the operator and subscriber. The mobile subscriber can end their contract, whenever they want, without making any contact with the operator. After reaching the end of the defined period, the subscriber will disappear, which is defined as “involuntary churn”. In this work, prepaid subscribers’ behavior are defined with their RFM data and some additional features, such as usage, call center and refill transactions. We model the churn behavior using Pareto/NBD model and with two benchmark models: a logistic regression model based on RFM data, and a logistic regression model based on the additional features. Pareto/NBD model is a crucial step in calculating customer lifetime value (CLV) and aliveness of the customers. If Pareto/NBD model proves to be a valid approach, then a mobile operator can define valuable prepaid subscribers using this and decide on the actions for these customers, such as suggesting customized offers.
Download
Short Papers
Paper Nr:	2
Title:	Models for Predicting the Development of Insulin Resistance
Authors:	Thomas Forstner, Christiane Dienhart, Ludmilla Kedenko, Gernot Wolkersdörfer and Bernhard Paulweber
Abstract:	Insulin resistance is the leading cause for developing type 2 diabetes. Early determination of insulin resistance and herewith of impending type 2 diabetes could help to establish sooner preventive measures or even therapies. However, an optimal predictive model for developing insulin resistance has not been established yet. Based on the data of an Austrian cohort study (SAPHIR study) various predictive models were calculated and compared to each other. For developing predictive models logistic regression models were used. For finding an optimal cut-off value an ROC approach was used. Based on various biochemical parameters an overall percentage of around 82% correct classifications could be achieved.
Download
Paper Nr:	7
Title:	B-kNN to Improve the Efficiency of kNN
Authors:	Dhrgam AL Kafaf, Dae-Kyoo Kim and Lunjin Lu
Abstract:	The kNN algorithm typically relies on the exhaustive use of training datasets, which aggravates efficiency on large datasets. In this paper, we present the B-kNN algorithm to improve the efficiency of kNN using a two-fold preprocess scheme built upon the notion of minimum and maximum points and boundary subsets. For a given training dataset, B-kNN first identifies classes and for each class, it further identifies the minimum and maximum points (MMP) of the class. A given testing object is evaluated to the MMP of each class. If the object belongs to the MMP, the object is predicted belonging to the class. If not, a boundary subset (BS) is defined for each class. Then, BSs are fed into kNN for determining the class of the object. As BSs are significantly smaller in size than their classes, the efficiency of kNN improves. We present two case studies to evaluate B-kNN. The results show an average of 97\% improvement in efficiency over kNN using the entire training dataset, while making little sacrifice of the accuracy compared to kNN.
Download
Paper Nr:	8
Title:	Data Scientist - Manager of the Discovery Lifecycle
Authors:	Kurt Englmeier and Fionn Murtagh
Abstract:	Data Scientists are the masters of Big Data. Analyzing masses of versatile data leads to insights that, in turn, may connect to successful business strategies, crime prevention, or better health care just to name a few. Big Data is primarily approached as mathematical and technical challenge. This may lead to technology design that enables useful insights from Big Data. However, this technology-driven approach does not meet completely and consistently enough the variety of information consumer requirements. To catch up with the versatility of user needs, the technology aspect should probably be secondary. If we adopt a user-driven approach, we are more in the position to cope with the individual expectations and exigencies of information consumers. This article takes information discovery as the overarching paradigm in data science and explains how this perspective change may impact the view on the profession of the data scientist and, resulting from that, the curriculum for the education in data science. It reflects the result from discussions with companies participating in our student project cooperation program. These results are groundwork for the development of a curriculum framework for Applied Data Science.
Download
Paper Nr:	14
Title:	Using Visualisation Techniques to Acquire a Better Understanding of Storytelling for Cultural Heritage
Authors:	Paulo Carvalho, Olivier Parisot and Thomas Tamisier
Abstract:	Historical information has an important role regarding cultural heritage. It is used to interpret facts occurred in the past and also to understand the present. Storytelling, when applied in the narrative of true events and resulting from different personal views and anecdotal stories, act as an important source of historical information. In this paper, we discuss the problems we encounter in the field of historical information storytelling and we present a software architecture to facilitate the comprehension of stories. More precisely, the proposed solution helps to analyse a story, examine its composition identifying existing entity classes and computing possible relations with other stories, to finally build a visual representation of these stories.
Download
Paper Nr:	31
Title:	Demand Prediction using Machine Learning Methods and Stacked Generalization
Authors:	Resul Tugay and Şule Gündüz Öğüdücü
Abstract:	Supply and demand are two fundamental concepts of sellers and customers. Predicting demand accurately is critical for organizations in order to be able to make plans. In this paper, we propose a new approach for demand prediction on an e-commerce web site. The proposed model differs from earlier models in several ways. The business model used in the e-commerce web site, for which the model is implemented, includes many sellers that sell the same product at the same time at different prices where the company operates a market place model. The demand prediction for such a model should consider the price of the same product sold by competing sellers along the features of these sellers. In this study we first applied different regression algorithms for specific set of products of one department of a company that is one of the most popular online e-commerce companies in Turkey. Then we used stacked generalization or also known as stacking ensemble learning to predict demand. Finally, all the approaches are evaluated on a real world data set obtained from the e-commerce company. The experimental results show that some of the machine learning methods do produce almost as good results as the stacked generalization method.
Download
Paper Nr:	34
Title:	Table Interpretation and Extraction of Semantic Relationships to Synthesize Digital Documents
Authors:	Martha O. Perez-Arriaga, Trilce Estrada and Soraya Abad-Mota
Abstract:	The large number of scientific publications produced today prevents researchers from analyzing them rapidly. Automated analysis methods are needed to locate relevant facts in a large volume of information. Though publishers establish standards for scientific documents, the variety of topics, layouts, and writing styles impedes the prompt analysis of publications. A single standard across scientific fields is infeasible, but common elements tables and text exist by which to analyze publications from any domain. Tables offer an additional dimension describing direct or quantitative relationships among concepts. However, extracting tables information, and unambiguously linking it to its corresponding text to form accurate semantic relationships are non-trivial tasks. We present a comprehensive framework to conceptually represent a document by extracting its semantic relationships and context. Given a document, our framework uses its text, and tables content and structure to identify relevant concepts and relationships. Additionally, we use the Web and ontologies to perform disambiguation, establish a context, annotate relationships, and preserve provenance. Finally, our framework provides an augmented synthesis for each document in a domain-independent format. Our results show that by using information from tables we are able to increase the number of highly ranked semantic relationships by a whole order of magnitude.
Download
Paper Nr:	36
Title:	Data Transformation Methodologies between Heterogeneous Data Stores - A Comparative Study
Authors:	Arnab Chakrabarti and Manasi Jayapal
Abstract:	With the advent of the NoSQL Data Stores, solutions for data migration from traditional relational databases to NoSQL databases is gaining more impetus in the recent times. This is also due to the fact that data generated in recent times are more heterogeneous in nature. In current available literatures and surveys we find that in-depth study has been already conducted for the tools and platform used in handling structured, unstructured and semi-structured data, however there are no guide which compares the methodologies of transforming and transferring data between these data stores. In this paper we present an extensive comparative study where we compare and evaluate data transformation methodologies between varied data sources as well as discuss the challenges and opportunities associated with it.
Download
Paper Nr:	38
Title:	Mining and Linguistically Interpreting Data from Questionnaires - Influence of Financial Literacy to Behaviour
Authors:	Miroslav Hudec and Zuzana Brokešová
Abstract:	This paper is focused on mining and interpreting information about effect of financial literacy on individuals’ behavior from the collected data by soft computing approach. Fuzzy sets and fuzzy logic allows us to formalize linguistic terms such as most of, high literacy and the like and interpret mined knowledge by short quantified sentences of natural language. This way is capable to cover semantic uncertainty in data and concepts. The preliminary results in this position paper have shown that for majority of people of low financial literacy angst and other treats represent serious issues, whereas about half of people with high literacy do not consider these treats as significant. Finally, influence of literacy to anchoring questions is mined and interpreted. Eventually, the paper emphasises needs for further data analysis and comparison.
Download
Paper Nr:	42
Title:	A Data-driven Framework on Mining Relationships between Air Quality and Cancer Diseases
Authors:	Wei Yuan Chang, En Tzu Wang and Arbee L. P. Chen
Abstract:	According to the report on global health risks, published by World Health Organization, environmental issues are urged to be dealt with in the world. Especially, air pollution causes great damage to human health. In this work, we build a framework for finding the correlations between air pollution and cancer diseases. This framework consists of a data access flow and a data analytics flow. The data access flow is designed to process raw data and to make the data able to be accessed by APIs. The cancer statistics is then mapped to air pollution data through temporal and spatial information. The analytics flow is used to find insights, based on the data exploration and data classification methods. The data exploration methods use statistics, clustering, and a series of mining techniques to interpret data. Then, the data mining methods are applied to find the relationships between air quality and cancer diseases by viewing air pollution indicators and cancer statistics as features and labels, respectively. The experiment results show that NO and NO2 air pollutants have a significant influence on the breast cancer, and the lung cancer is significantly influenced by NO2, NO, PM10 and O3, which are consistent with those from traditional statistical methods. Moreover, our results also cover the research results from several other studies. The proposed framework is flexible and can be applied to other applications with spatiotemporal data.
Download
Paper Nr:	9
Title:	Reducing Variant Diversity by Clustering - Data Pre-processing for Discrete Event Simulation Models
Authors:	Sonja Strasser and Andreas Peirleitner
Abstract:	Building discrete event simulation Models for studying questions in production planning and control affords reasonable calculation time. Two main causes for increased calculation time are the level of model details as well as the experimental design. However, if the objective is to optimize parameters to investigate the parameter settings for materials, they have to be modelled in detail. As a consequence model details such as number of simulated materials or work stations in a production system have to be reduced. The challenge in real world applications with a high variant diversity of products is to select representative materials from the huge number of existing materials for building a simulation model on condition that the simulation results remain valid. Data mining methods, especially clustering can be used to perform this selection automatically. In this paper a procedure for data preparation and clustering of materials with different routings is shown and applied in a case study from sheet metal processing.
Download
Paper Nr:	15
Title:	Clink - A Novel Record Linkage Methodology based on Graph Interactions
Authors:	Mahmoud Boghdady and Neamat El-Tazi
Abstract:	With the advent of the big-data era and the rapid growth of the amount of data, companies are faced with more opportunities and challenges to outperform their peers, innovate, compete, and capture value from big-data platforms such as social networks. Utilizing the full beneﬁt of social media requires companies to identify their own customers against customers as a whole by linking their local data against data from social media applying record-linkage techniques that differ from simple to complex. For large sources that have huge data and fewer constraints over data, the linking process produces low quality results and requires a lot of pairwise comparisons. We propose a study on how to calculate similarity score not only based on string similarity techniques or topological graph similarity, but also using graph interactions between nodes to effectively achieve better linkage results.
Download
Area 3 - Data Management and Quality
Full Papers
Paper Nr:	22
Title:	Storing and Processing Personal Narratives in the Context of Cultural Legacy Preservation
Authors:	Pierrick Bruneau, Olivier Parisot and Thomas Tamisier
Abstract:	An important, yet underestimated, aspect of cultural heritage preservation is the analysis of personal narratives told by citizens. In this paper, we present a data model and implementation towards facilitating narratives storage and sharing. The proposed solution aims at collecting textual narratives in raw form, processing them to extract and store structured content, and then exposing results through a RESTful interface. We apply it to a corpus related to the time of the European construction in Luxembourg. We disclose details about our conceptual model and implementation, as well as evidence supporting the interest of our approach.
Download
Paper Nr:	32
Title:	A Multi-criteria Approach for Large-object Cloud Storage
Authors:	Uwe Hohenstein, Michael C. Jaeger and Spyridon V. Gogouvitis
Abstract:	In the area of storage, various services and products are available from several providers. Each product possesses particular advantages of its own. For example, some systems are offered as cloud services, while others can be installed on premises, some store redundantly to achieve high reliability while others are less reliable but cheaper. In order to benefit from the offerings at a broader scale, e.g., to use specific features in some cases while trying to reduce costs in others, a federation is beneficial to use several storage tools with their individual virtues in parallel in applications. The major task of a federation in this context is to handle the heterogeneity of involved systems. This work focuses on storing large objects, i.e., storage systems for videos, database archives, virtual machine images etc. A metadata-based approach is proposed that uses the metadata associated with objects and containers as a fundamental concept to set up and manage a federation and to control storage locations. The overall goal is to relieve applications from the burden to find appropriate storage systems. Here a multi-criteria approach comes into play. We show how to extend the object storage developed by the VISION Cloud project to support federation of various storage systems in the discussed sense.
Download
Paper Nr:	48
Title:	Success of the Functionalities of a Learning Management System
Authors:	Floriana Meluso, Paolo Avogadro, Silvia Calegari and Matteo Dominoni
Abstract:	The goal of this research is to define and implement indicators for a Learning Management System (LMS). In particular, we focus on estimating patterns on the utilization of the message system by defining two quantities: the specific utilization and popularity. The idea is to take into account the perspective of academic institution managers and the administrators of the LMS, for example to understand if a particular department fails at providing a useful LMS service, or in order to allocate the correct amount of resources. These indicators have been tested on the LMS employed by the “Università degli Studi di Milano-Bicocca” (Milan, Italy), and in general provided a picture of poor utilization of the message system, where the usage follows a pattern similar to the Zipf law. This feature, correlated with the principle of least effort, suggests that LMSs should join forces with existing social networking systems to create strong online learning communities.
Download
Short Papers
Paper Nr:	16
Title:	Using Signifiers for Data Integration in Rail Automation
Authors:	Alexander Wurl, Andreas Falkner, Alois Haselböck and Alexandra Mazak
Abstract:	In Rail Automation, planning future projects requires the integration of business-critical data from heterogeneous data sources. As a consequence, data quality of integrated data is crucial for the optimal utilization of the production capacity. Unfortunately, current integration approaches mostly neglect uncertainties and inconsistencies in the integration process in terms of railway specific data. To tackle these restrictions, we propose a semi-automatic process for data import, where the user resolves ambiguous data classifications. The task of finding the correct data warehouse classification of source values in a proprietary, often semi-structured format is supported by the notion of a signifier, which is a natural extension of composite primary keys. In a case study from the domain of asset management in Rail Automation we evaluate that this approach facilitates high-quality data integration while minimizing user interaction.
Download
Paper Nr:	45
Title:	Data Preprocessing of eSport Game Records - Counter-Strike: Global Offensive
Authors:	David Bednárek, Martin Krulis, Jakub Yaghob and Filip Zavoral
Abstract:	Electronic sports or pro gaming have become very popular in this millenium and the increased value of this new industry is attracting investors with various interests. One of these interest is game betting, which requires player and team rating, game result predictions, and fraud detection techniques. In our work, we focus on preprocessing data of Counter-Strike: Global Offensive game in order to employ subsequent data analysis methods for quantifying player performance. The data preprocessing is difficult since the data format is complex and undocumented, the data quality of available sources is low, and there is no direct way how to match players from the recorded files with players listed on public boards such as HLTV website. We have summarized our experience from the data preprocessing and provide a way how to establish a player matching based on their metadata.
Download
Paper Nr:	46
Title:	Implementation and Empirical Evaluation of a Case-based, Interactive e-Learning Module with X-ray Tooth Prognosis
Authors:	Thomas Ostermann, Hedwig Ihlhoff-Goulioumius, Martin R. Fischer, Jan P. Ehlers and Michaela Zupanic
Abstract:	The prognosis estimation of teeth based on radiographs is a subordinate but relevant target in many dental medicine curricula in Germany. Empirical data on the integration of e-learning material into dental curricula are rare. We aimed at developing and implementing a radiological pillar diagnostics online-course in the dental curriculum at the University of Witten/Herdecke. This online course was developed on the CASUS web-based learning platform and implemented in a blended learning approach. Results showed an easy creation of learning cases (virtual patients), higher utilization for the intervention group regarding the number of cases revised, time-on-task, and student acceptance. Dental students experienced improved learning efficacy, higher long time knowledge retention and significantly better results in case based assessment. The usability of the CASUS learning Platform therefore can be regarded as high and further studies using this e-learning approach are recommended.
Download
Paper Nr:	11
Title:	Modeling and Qualitative Evaluation of a Management Canvas for Big Data Applications
Authors:	Michael Kaufmann, Tobias Eljasik-Swoboda, Christian Nawroth, Kevin Berwind, Marco Bornschlegl and Matthias Hemmje
Abstract:	A reference model for big data management is proposed, together with a methodology for business enterprises to bootstrap big data projects. Similar to the business model canvas for marketing management, the big data management (BDM) canvas is a template for developing new (or mapping existing) big data applications, strategies and projects. It subdivides this task into meaningful fields of action. The BDM canvas provides a visual chart that can be used in workshops iteratively to develop strategies for generating value from data. It can also be used for project planning and project progress reporting. The canvas instantiates a big data reference meta-model, the BDM cube, which provides its meta-structure. In addition to developing and theorizing the proposed data management model, two case studies on pilot applications in companies in Switzerland and Austria provide a qualitative evaluation of our approach. Using the insights from expert feedback, we provide an outlook for further research.
Download
Paper Nr:	27
Title:	Validating ETL Patterns Feasability using Alloy
Authors:	Bruno Oliveira and Orlando Belo
Abstract:	The ETL processes can be seen as typical data-oriented workflows composed of dozens of granular tasks that are responsible for the integration of data coming from different data sources. They are one of the most important components of a data warehousing system, strongly influenced by the complexity of business requirements, their changing, and evolution. To facilitate the planning and ETL implementation, a set of patterns spe"
2017-b,"Full Papers
Paper Nr:	2
Title:	Election Vote Share Prediction using a Sentiment-based Fusion of Twitter Data with Google Trends and Online Polls
Authors:	Parnian Kassraie, Alireza Modirshanechi and Hamid K. Aghajan
Abstract:	It is common to use online social content for analyzing political events. Twitter-based data by itself is not necessarily a representative sample of the society due to non-uniform participation. This fact should be noticed when predicting real-world events from social media trends. Moreover, each tweet may bare a positive or negative sentiment towards the subject, which needs to be taken into account. By gathering a large dataset of more than 370,000 tweets on 2016 US Elections and carefully validating the resulting key trends against Google Trends, a legitimate dataset is created. A Gaussian process regression model is used to predict the election outcome; we bring in the novel idea of estimating candidates’ vote shares instead of directly anticipating the winner of the election, as practiced in other approaches. Applying this method to the US 2016 Elections resulted in predicting Clinton’s majority in the popular vote at the beginning of the elections week with 1% error. The high variance in Trump supporters’ behavior reported elsewhere is reflected in the higher error rate of his vote share.
Download
Paper Nr:	3
Title:	Assessment of Private Cloud Infrastructure Monitoring Tools - A Comparison of Ceilometer and Monasca
Authors:	Mario A. Gomez-Rodriguez, Víctor J. Sosa-Sosa and Jose L. Gonzalez-Compean
Abstract:	Cloud monitoring tools are a popular solution for both cloud users and administrators of private cloud infrastructures. These tools provide information that can be useful for effective and efficient resource consumption management, supporting the decision-making process in scenarios where administrators must react rapidly to saturation and failure events. However, the estimation of the impact on host systems in a private cloud is not a trivial issue for administrators, specially when monitoring measurements are required in reduced periods of times. This paper presents a performance comparison between two free and well supported cloud monitoring tools called Ceilometer and Monasca, deployed on a private cloud infrastructure. This comparison is mainly focused on evaluating these tools ability to obtain monitoring information in short time intervals for early detection of resource constraints. The impact of resource consumption on the performance of host systems produced by both tools was analyzed and the evaluation revealed that Monasca produced a better performance than Ceilometer for evaluated scenarios and that, according to the learned lessons in this comparison, Monasca represents a suitable option for being integrated into an adaptive cloud resource management system.
Download
Paper Nr:	4
Title:	Protecting Data in the Cloud: An Assessment of Practical Digital Envelopes from Attribute based Encryption
Authors:	Víctor J. Sosa-Sosa, Miguel Morales-Sandoval, Oscar Telles-Hurtado and José Luis González-Compeán
Abstract:	Cloud storage services provide users with an effective and inexpensive mechanism to store and manage big data with anytime and anywhere availability. However, data owners face the risk of losing control over their data, which could be accessed by third non-authorized parties including the provider itself. Although conventional encryption could avoid data snooping, an access control problem arises and the data owner must implement the security mechanisms to store, manage and distribute the decryption keys. This paper presents a qualitative and quantitative evaluation of two Java implementations of security schemes called DET-ABE and AES4SeC. Both are based on the digital envelope technique and attribute based encryption, a non-conventional cryptography that ensures confidentiality and access control security services. The experimental evaluation was performed in a private cloud infrastructure where experiments for both implementations ran using the same platform, settings, underlying libraries, thus providing a more fair comparison. The quantitative evaluation revealed DET-ABE and AES4SeC have similar performance when applying low security levels (128-bit keys), whereas DET-ABE surpasses AES4SeC performance when medium (192-bit keys) and high (256-bit keys) security levels are required. Qualitative evaluation shows that AES4SeC also ensures authentication and integrity services, which are not supported by DET-ABE.
Download
Paper Nr:	5
Title:	An Ontology for Representing Information over Social Service in an Educational Institution
Authors:	Mireya Tovar, Juan Carlos Flores and José A. Reyes-Ortiz
Abstract:	In this paper, we present a method for constructing a manual ontology for the search of information over social service in a higher level education institution. We use some steps from methodology proposed by Grüninger and Fox. The ontology model will be useful to answer the questions of the students interested in the procedures of the social service. A scenario, competition questions, classes, and relationships are part of the design process. The answer to these questions leads us to the evaluation of the ontology. The ontology is made into Protégé and queries are written in the SPARQL query language.
Download
Paper Nr:	7
Title:	Entity-based Opinion Mining from Spanish Tweets
Authors:	Fabián Paniagua-Reyes, José A. Reyes-Ortiz and Maricela Bravo
Abstract:	Networking service has grown in the last years and therefore, users generate large amounts of data about entities, where they can express opinions about them. This paper presents an approach for opinion mining based on entities, which belong to banks, musicians and automobiles. Our approach uses machine learning techniques in order to classify Spanish tweets into three categories positives, negatives and neutral. A Support Vector Machine (SVM) and the bag of word model is used to obtain the corresponding class given a tweet. Our experimentation shows promising results and they validate that entity-based opinion mining is achievable.
Download
Paper Nr:	8
Title:	Analysis of Brain Waves in Violent Images - Are Differences in Gender?
Authors:	Juan Andrés Martínez-Escobar, Silvia B. González-Brambila and Josué Figueroa-González
Abstract:	We collected information using the Electroencephalograph (EEG) EmotivEpoc, and the software complement of the Eye Tracking system SMI RED250mobile. As a first step, it was stored in text files, the readings of each EEG sensor during the time the presentation of 5 violent images and 5 non-violent images were observed. The database was collected with 50 volunteers, consisting of 25 men and 25 women. The database was later loaded into R, for the execution of the algorithms of data mining, K-means, K-medoids, Hierarchical Clustering, Naive Bayes, Support Vector Machines, Adaboost and Decision trees. In the clustering methods, a random clustering was presented and with little information, with the Naive Bayes, SVM and Adaboost models, a classification with a high percentage of error was obtained using the Decision Trees method, we obtained one of the worst results, with the highest error rates in the classification performed with the test data of selected method. Based on the results obtained, no significant difference was found in the individual's gender, which affected his reaction when viewing images with violent and non-violent content.
Download
Paper Nr:	9
Title:	Determining a New Home Classification - A Data Mining Approach
Authors:	Fidel López-Saca, José Castro-López, Josué Figueroa-González and Silvia Beatriz González-Brambila
Abstract:	This paper presents a new home classification using a data mining approach and clustering algorithms. It focus in sociological characteristics. Data was obtained from a survey used in the research project ""The Dwelling of Older Adults in the Central City"" that is part of a larger research project entitled ""Habitat and Centrality"". This survey has 3,000 registers and 294 columns. From this, we selected 30 columns that were categorized in 4: gender, if at least one child exists, if a partner exists, and if there are one or more elder. Elder were 64 or more, following Mexican guidelines. Classification was performed with 6 clustering algorithms, and evaluated by silhouette and Dunn. The proposed classification is 10 clusters, that more adequately represent the type of families from a sociological point of view.
Download"
2017-c,"Full Papers
Paper Nr:	1
Title:	A Collaborative Environment for Web Crawling and Web Data Analysis in ENEAGRID
Authors:	Giuseppe Santomauro, Giovanni Ponti, Fiorenzo Ambrosino, Giovanni Bracco, Antonio Colavincenzo, Matteo De Rosa, Agostino Funel, Dante Giammattei, Guido Guarnieri and Silvio Migliori
Abstract:	In this document we provide an overview on the development and the integration in ENEAGRID of some tools able to download data from Web (Web Crawling), manage and display a large amount of data (Big Data), and extract from data relevant hidden information (Data Mining). We collected all these instruments inside the so called Web Crawling Project. Further, the corresponding environment, called Virtual Laboratory, is able to offer the possibility to use all these integrated tools remotely and by a simple graphical interface. A detailed description of the developed web application will be illustrated. Finally, some experimental results on the behaviour of the Web Crawling tool will be reported.
Download
Paper Nr:	2
Title:	Big Data Visualization Tools: A Survey - The New Paradigms, Methodologies and Tools for Large Data Sets Visualization
Authors:	Enrico G. Caldarola and Antonio M. Rinaldi
Abstract:	In the era of Big Data, a great attention deserves the visualization of large data sets. Among the main phases of the data management’s life cycle, i.e., storage, analytics and visualization, the last one is the most strategic since it is close to the human perspective. The huge mine of data becomes a gold mine only if tricky and wise analytics algorithms are executed over the data deluge and, at the same time, the analytic process results are visualized in an effective, efficient and why not impressive way. Not surprisingly, a plethora of tools and techniques have emerged in the last years for Big Data visualization, both as part of Data Management Systems or as software or plugins specifically devoted to the data visualization. Starting from these considerations, this paper provides a survey of the most used and spread visualization tools and techniques for large data sets, eventually presenting a synoptic of the main functional and non-functional characteristics of the surveyed tools.
Download
Paper Nr:	3
Title:	An Open Source System for Big Data Warehousing
Authors:	Nunziato Cassavia, Elio Masciari and Domenico Saccà
Abstract:	The pervasive diffusion of new data generation devices has recently caused the generation of massive data flows containing heterogeneous information generated at different rates and having different formats. These data are referred as \emph{Big Data} and require new storage and analysis approaches to be investigated for managing them. In this paper we will describe a system for dealing with massive big data stores. We defined an open source tool that exploits a NoSQL approach for data warehousing in order to offer user am intuitive way to easily query data that could be quite hard to be understood otherwise.
Download
Paper Nr:	4
Title:	A Novel Influence Diffusion Model based on User Generated Content in Online Social Networks
Authors:	Flora Amato, Antonio Bosco, Vincenzo Moscato, Antonio Picariello and Giancarlo Sperlí
Abstract:	Social Network Analysis has been introduced to study the properties of Online Social Networks for a wide range of real life applications. In this paper, we propose a novel methodology for solving the Influence Maximization problem, i.e. the problem of finding a small subset of actors in a social network that could maximize the spread of influence. In particular, we define a novel influence diffusion model that, learning recurrent user behaviours from past logs, estimates the probability that a given user can influence the other ones, basically exploiting user to content actions. A greedy maximization algorithm is then adopted to determine the final set of influentials in the network. Preliminary experimental results shows the goodness of the proposed approach, especially in terms of efficiency, and encourage future research in such direction.
Download
Paper Nr:	5
Title:	Reputation Analysis towards Discovery
Authors:	Raffaele Palmieri, Vincenzo Orabona, Nadia Cinque, Stefano Tangorra and Donato Cappetta
Abstract:	This work describes the development and the realization of an OSInt solution conducting a supplier risk assessment, focused on the evaluation of suppliers’ reputation starting from publicly available information. The main challenge is represented by the data processing phase that exploits NLP technologies to extract facts, events, and relations from unstructured sources, building the knowledge base for reputational analysis. Several measures have been adopted to provide a satisfactory user experience; however, further integrations are still needed to increase efficiency of the developed solution. Particularly, it is necessary to deepen and improve the analysis over the huge volume of data coming from open sources, enhancing the discovery of all possible relevant information influencing the reputation of the targeted entity.
Download
Paper Nr:	6
Title:	The Challenge of using Map-reduce to Query Open Data
Authors:	Mauro Pelucchi, Giuseppe Psaila and Maurizio Toccu
Abstract:	For transparency and democracy reasons, a few years ago Public Administrations started publishing data sets concerning public services and territories. These data sets are called open, because they are publicly available through many web sites. Due to the rapid growth of open data corpora, both in terms of number of corpora and in terms of open data sets available in each single corpus, the need for a centralized query engine arises, able to select single data items from within a mess of heterogeneous open data sets. We gave a first answer to this need in (Pelucchi et al., 2017), where we defined a technique for blindly querying a corpus of open data. In this paper, we face the challenge of implementing this technique on top of the Map-Reduce approach, the most famous solution to parallelize computational tasks in the Big Data world.
Download
Paper Nr:	7
Title:	A Pipeline for Multimedia Twitter Analysis through Graph Databases: Preliminary Results
Authors:	Roberto Boselli, Mirko Cesarini, Fabio Mercorio, Mario Mezzanzanica and Alessandro Vaccarino
Abstract:	Twitter is a microblogging service where users post not only short messages, but also images and other multimedia contents. Twitter can be used for analyzing people public discussions, as a huge amount of messages are continuously broadcasted by users. Analysis have usually focused on the textual part of messages, but the non-negligible number of images exchanged calls for specific attention. In this paper we describe how the tweet multimedia contents can be turned into a knowledge graph and then used for analyzing the messages sent during marketing campaigns. The information extraction and processing pipeline is built on top of off-theshelf APIs and products while the obtained knowledge is modelled through a Graph Database. The resulting knowledge graph was useful to explore and identify similarities among different marketing campaigns carried out using Twitter, providing some preliminary but promising results.
Download
Paper Nr:	8
Title:	BotWheels: a Petri Net based Chatbot for Recommending Tires
Authors:	Francesco Colace, Massimo De Santo, Francesco Pascale, Saverio Lemma and Marco Lombardi
Abstract:	Technological progress seems unstoppable: large companies are ready to implement more and more sophisticate solution to improve their productivity. The near future may be represented by so-called Chatbot, already present in the instant messaging platforms and destined to become more and more popular. This paper presents the realization of a prototype of a conversational workflow for a Chatbot in tires domain. The initial purpose has focused on the design of the specific model to manage communication and propose the most suitable tires for users. For this aim, it has been used the Petri Net. Finally, after the implementation of the designed model, experimental campaign was conducted in order to demonstrate its enforceability and efficiency.
Download"
2018-a,""
2018-b,"Full Papers
Paper Nr:	2
Title:	Design of a Portable Programming Abstraction for Data Transformations
Authors:	Johannes Luong, Dirk Habich and Wolfgang Lehner
Abstract:	Novel data intensive applications and the diversification of data processing platforms have changed data management significantly over the last decade. In this changed environment, the expressiveness of the traditional relational algebra is often insufficient and data management systems have started to provide more powerful special purpose programming languages. However, these languages create a tight coupling between applications and specific systems that can hinder further development on both sides of the equation. The goal of this article is to start a discussion on the future of platform independent programming models for data processing that re-establish the separation of application logic and implementation details that used to be a cornerstone of data management systems. As a guide for that discussion, we introduce several recent related works on that topic and also outline our own contribution, the Analytical Calculus.
Download
Paper Nr:	4
Title:	Self-adaptive Synchronous Localization and Mapping using Runtime Feature Models
Authors:	Christopher Werner, Sebastian Werner, René Schöne, Sebastian Götz and Uwe Aßmann
Abstract:	Mobile autonomous robotic systems need to operate in unknown areas. For this, a plethora of simultaneous localization and mapping (SLAM) approaches has been proposed over the last decades. Although many of these existing approaches have been successfully applied even in real-world productive scenarios, they are typically designed for specific contexts (e.g., in-vs. outdoor, crowded vs. free areas, etc.). Thus, for different contexts, different SLAM algorithms should be used. In this paper, we propose a feature-based classification of SLAM algorithms and a reconfiguration approach to switch between existing SLAM implementations at runtime. By this, mobile robots are enabled to always use the most efficient implementation for their current contexts.
Download
Area 1 - Software Agents and Internet Computing
Full Papers
Paper Nr:	1
Title:	Cracking KD-Tree: The First Multidimensional Adaptive Indexing (Position Paper)
Authors:	Pedro Holanda, Matheus Nerone, Eduardo C. de Almeida and Stefan Manegold
Abstract:	Workload-aware physical data access structures are crucial to achieve short response time with (exploratory) data analysis tasks as commonly required for Big Data and Data Science applications. Recently proposed techniques such as automatic index advisers (for a priori known static workloads) and query-driven adaptive incremental indexing (for a priori unknown dynamic workloads) form the state-of-the-art to build single-dimensional indexes for single-attribute query predicates. However, similar techniques for more demanding multi-attribute query predicates, which are vital for any data analysis task, have not been proposed, yet. In this paper, we present our on-going work on a new set of workload-adaptive indexing techniques that focus on creating multidimensional indexes. We present our proof-of-concept, the Cracking KD-Tree, an adaptive indexing approach that generates a KD-Tree based on multidimensional range query predicates. It works by incrementally creating partial multidimensional indexes as a by-product of query processing. The indexes are produced only on those parts of the data that are accessed, and their creation cost is effectively distributed across a stream of queries. Experimental results show that the Cracking KD-Tree is three times faster than creating a full KD-Tree, one order of magnitude faster than executing full scans and two orders of magnitude faster than using uni-dimensional full or adaptive indexes on multiple columns.
Download"
2019-a,"Area 1 - Big Data
Full Papers
Paper Nr:	19
Title:	Manifold Learning to Identify Consumer Profiles in Real Consumption Data
Authors:	Diego Perez, Marta Rivera-Alba and Alberto Sanchez-Carralero
Abstract:	Precise and comprehensive analysis of individual consumption is key to marketers and policy makers. Traditionally, people’s consumption profiles have been approximated by household surveys. Although insightful and complete, household surveys suffer from some biases and inaccuracies. To compensate for some of those biases, we propose a new approach to compute and analyze consumer profiles based on millions of purchase transactions collected by a personal financial manager. Since this new kind of data sources requires new analysis methods, in this paper we propose the use of manifold learning techniques to visualize the whole data set at once, demonstrating how these techniques can cluster consumers in more meaningful groups than demographics alone. These unsupervised behavior-based clusters allow us to draw more educated hypotheses that we could otherwise miss. As an example, we will specifically discuss the characteristics of individuals with high housing and recreation consumption in our sample.
Download
Paper Nr:	20
Title:	Predicting Depression Tendency based on Image, Text and Behavior Data from Instagram
Authors:	Yu C. Huang, Chieh-Feng Chiang and Arbee P. Chen
Abstract:	Depression is common but serious mental disorder. It is classified as a mood disorder, which means that it is characterized by negative thoughts and emotions. With the development of Internet technology, more and more people post their life story and express their emotion on social media. Social media can provide a way to characterize and predict depression. It has been widely utilized by researchers to study mental health issues. However, most of the existing studies focus on textual data from social media. Few studies consider both text and image data. In this study, we aim to predict one’s depression tendency by analyzing image, text and behavior of his/her postings on Instagram. An effective mechanism is first employed to collect depressive and non-depressive user accounts. Next, three sets of features are extracted from image, text and behavior data to build the predictive deep learning model. We examine the potential for leveraging social media postings in understanding depression. Our experiment results demonstrate that the proposed model recognizes users who have depression tendency with an F-1 score of 82.3%. We are currently developing a tool based on this study for screening and detecting depression in an early stage.
Download
Paper Nr:	23
Title:	A Community Detection Approach for Smart-Phone Addiction Recognition
Authors:	Fabio Cozzolino, Vincenzo Moscato, Antonio Picariello and Giancarlo Sperli
Abstract:	In this paper, we present a novel approach for Smart-Phone Addiction recognition that leverages community detection algorithms from the Social Network Analysis (SNA) theory. Our basic idea is to model data concerning users’ behavior while they are using mobile devices as a particular social graph, discovering by means of SNA facilities patterns that better identify users with a high predisposition to smart phone addiction. Eventually, several experiments on a sample of users monitored for several weeks have been carried out to verify effectiveness of the proposed approach in correctly recognizing the related addiction degree.
Download
Paper Nr:	84
Title:	XAI: A Middleware for Scalable AI
Authors:	Abdallah Salama, Alexander Linke, Igor P. Rocha and Carsten Binnig
Abstract:	A major obstacle for the adoption of deep neural networks (DNNs) is that the training can take multiple hours or days even with modern GPUs. In order to speed-up training of modern DNNs, recent deep learning frameworks support the distribution of the training process across multiple machines in a cluster of nodes. However, even if existing well-established models such as AlexNet or GoogleNet are being used, it is still a challenging task for data scientists to scale-out distributed deep learning in their environments and on their hardware resources. In this paper, we present XAI, a middleware on top of existing deep learning frameworks such as MXNet and Tensorflow to easily scale-out distributed training of DNNs. The aim of XAI is that data scientists can use a simple interface to specify the model that needs to be trained and the resources available (e.g., number of machines, number of GPUs per machine, etc.). At the core of XAI, we have implemented a distributed optimizer that takes the model and the available cluster resources as input and finds a distributed setup of the training for the given model that best leverages the available resources. Our experiments show that XAI converges to a desired training accuracy 2x to 5x faster than default distribution setups in MXNet and TensorFlow.
Download
Short Papers
Paper Nr:	26
Title:	An Evaluation of Big Data Architectures
Authors:	Valerie Garises and José G. Quenum
Abstract:	In this paper, we present a novel evaluation of architectural patterns and software architecture analysis using Architecture-based Tradeoff Analysis Method (ATAM). To facilitate the evaluation, we classify the Big Data intrinsic characteristics into quality attributes. We also categorised existing architectures following architectural patterns. Overall, our evaluation clearly shows that no single architectural pattern is enough to guarantee all the required quality attributes. As such, we recommend a combination of more than one pattern. The net effect of this would be to increase the benefits of each architectural pattern and then support the design of Big Data software architectures with several quality attributes.
Download
Paper Nr:	70
Title:	Less (Data) Is More: Why Small Data Holds the Key to the Future of Artificial Intelligence
Authors:	Ciro Greco, Andrea Polonioli and Jacopo Tagliabue
Abstract:	The claims that big data holds the key to enterprise successes and that Artificial Intelligence (AI). is going to replace humanity have become increasingly more popular over the past few years, both in academia and in the industry. However, while these claims may indeed capture some truth, they have also been massively oversold, or so we contend here. The goal of this paper is two-fold. First, we provide a qualified defence of the value of less data within the context of AI. This is done by carefully reviewing two distinct problems for big data driven AI, namely a) the limited track record of Deep Learning (DL) in key areas such as Natural Language Processing (NLP), b) the regulatory and business significance of being able to learn from few data points. Second, we briefly sketch what we refer to as a case of “A.I. with humans and for humans”, namely an AI paradigm whereby the systems we build are privacy-oriented and focused on human-machine collaboration, not competition. Combining our claims above, we conclude that when seen through the lens of cognitively inspired A.I., the bright future of the discipline is about less data, not more, and more humans, not less.
Download
Paper Nr:	72
Title:	Scaling Big Data Applications in Smart City with Coresets
Authors:	Le H. Trang, Hind Bangui, Mouzhi Ge and Barbora Buhnova
Abstract:	With the development of Big Data applications in Smart Cities, various Big Data applications are proposed within the domain. These are however hard to test and prototype, since such prototyping requires big computing resources. In order to save the effort in building Big Data prototypes for Smart Cities, this paper proposes an enhanced sampling technique to obtain a coreset from Big Data while keeping the features of the Big Data, such as clustering structure and distribution density. In the proposed sampling method, for a given dataset and an ε>0, the method computes an ε-coreset of the dataset. The ε-coreset is then modified to obtain a sample set while ensuring the separation and balance in the set. Furthermore, by considering the representativeness of each sample point, our method can helps to remove noises and outliers. We believe that the coreset-based technique can be used to efficiently prototype and evaluate Big Data applications in the Smart City.
Download
Paper Nr:	31
Title:	Research on Parallel Incremental Association Rule Algorithm based on Data Stream
Authors:	Zheng Hua, Tao Du, Shouning Qu and Tao Mi
Abstract:	Data stream association rule is one of the most interesting problems in the data mining community. However, the bottleneck that the computational power of a single computer is limited and the number of candidate itemsets far surpasses memory space cannot be solved by the traditional algorithm. This paper focuses on this problem and introduces a novel algorithm named a Parallel Incremental Association Rule Mining based on a Hierarchical method (PIMH), which utilizes the Spark parallel computing platform as a framework solve the problem of parallel processing the partition data. In the PIMH algorithm, the existing mining results is used to solve the repeated mining inefficiency, consequently, association rules can be quickly updated. Additionally, the local pruning method is performed which avoids the problem that the memory is difficult to stored. According to the experimental results, the proposed algorithm showed to be the characteristics of high accuracy and spend less computational time in comparison with existing data stream algorithms.
Paper Nr:	59
Title:	Road Operations Orchestration Enhanced with Long-short-term Memory and Machine Learning (Position Paper)
Authors:	Fuji Foo, Poh J. Peng, Robert K. Lin and Wenwey Hseush
Abstract:	Road traffic management has been a priority for urban city planners to mitigate urban traffic congestion. In 2018, the economic impact to US due to lost productivity of workers sitting in traffic, increased cost of transporting goods through congested areas, and all of that wasted fuel amounted to US$87 billion, an average of US$1,348 per driver. In land scare Singapore, congestion not only translates to economic impact, but also strain to the infrastructure and city land use. While techniques for traffic prediction have existed for many years, the research effort has mainly been focused on traffic prediction. The downstream impact on how city administration should predict and react to incidents and/or events has not been widely discussed. In this paper, we propose Artificial Intelligence enabled Complex Event Processing to only identify and predict incidents, but also to enable a swift response through effective deployment of critical resources to ensure well-coordinated recovery action before any incident develop into crisis.
Download
Paper Nr:	71
Title:	Semi-Structured Data Model for Big Data (SS-DMBD)
Authors:	Shady Hamouda and Zurinahni Zainol
Abstract:	New business applications require flexibility in data model structure and must support the next generation of web applications and handle complex data types. The performance of processing structured data through a relational database has become incompatible with big data challenges. Nowadays, there is a need to deal with semi-structured data with a flexible schema for different applications. Not only SQL (NoSQL) has been presented to overcome the limitations of relational databases in terms of scale, performance, data model, and distribution system. Also, NoSQL supports semi-structured data and can handle a huge amount of data and provide flexibility in the data schema. But the data models of NoSQL systems are very complex, as there are no tools available to represent a scheme for NoSQL databases. In addition, there is no standard schema for data modelling of document-oriented databases. This study proposes a semi-structured data model for big data (SS-DMBD) that is compatible with a document-oriented database, and also proposes an algorithm for mapping the entity relationship (ER) model to SS-DMBD. A case study is used to evaluate the SS-DMBD and its features. The results show that this model can address most features of semi-structured data.
Download
Paper Nr:	73
Title:	Development of Spatial Quality Control Method for Temperature Observation Data using Cluster Analysis
Authors:	Yunha Kim, Nooree Min, Hannah Lee, Mi-Lim Ou, Sanghyeon Jeon and Myung-jin Hyun
Abstract:	In the National Climate Data Center of Korea Meteorological Administration, quality control methods of meteorological observations are applied to identify erroneous observation values. The type of quality control methods we have been using is to check the value from one station, either instantly or temporally. The spatial checking methods that find errors by comparing values of several stations at a time are difficult to apply because calculating the threshold using a large amount of observations is time consuming and various conditions for applying are required. In this study, we develop a new spatial checking method for temperature observation data using cluster analysis for meteorological observations that can be performed fast and effective in clarifying errors that have not been discovered.
Download
Area 2 - Business Analytics
Full Papers
Paper Nr:	10
Title:	Using Topic Specific Features for Argument Stance Recognition
Authors:	Tobias Eljasik-Swoboda, Felix Engel and Matthias Hemmje
Abstract:	Argument detection and its representation through ontologies are important parts of today’s attempt in automated recognition and processing of useful information in the vast amount of constantly produced data. However, due to the highly complex nature of an argument and its characteristics, its automated recognition is hard to implement. Given this overall challenge, as part of the objectives of the RecomRatio project, we are interested in the traceable, automated stance detection of arguments, to enable the construction of explainable pro/con argument ontologies. In our research, we design and evaluate an explainable machine learning based classifier, trained on two publicly available data sets. The evaluation results proved that explainable argument stance recognition is possible with up to .96 F1 when working within the same set of topics and .6 F1 when working with entirely different topics. This informed our hypothesis, that there are two sets of features in argument stance recognition: General features and topic specific features.
Download
Paper Nr:	33
Title:	Comparative Analysis of Store Clustering Techniques in the Retail Industry
Authors:	Kanika Agarwal, Prateek Jain and Mamta Rajnayak
Abstract:	Many offline retailers in European Markets are currently exploring different store designs to address local demands and to gain a competitive edge. There has been a significant demand in this industry to use analytics as a key pillar to take store-centric informed strategic decisions. The main objective of this case study is to propose a robust store clustering mechanism which will help the business to understand their stores better and frame store-centric marketing strategies with an aim to maximize their revenues. This paper evaluates four advance analytics-based clustering techniques namely: Hierarchical clustering, Self Organizing Maps, Gaussian Mixture Matrix, and Fuzzy C-means These techniques are used for clustering offline stores of a global retailer across four European markets. The results from these four techniques are compared and presented in this paper.
Download
Paper Nr:	35
Title:	Data Mining using Morlet Wavelets for Financial Time Series
Authors:	Reginald Bolman and Thomas Boucher
Abstract:	Wavelets are a family of signal processing techniques which have a growing popularity in the artificial intelligence community. In particular, Morlet wavelets have been applied to neural network time series trend prediction, forecasting the effects of monetary policy, etc. In this paper, we discuss the application of Morlet wavelets to discover the morphology of a time series cyclical components and the unsupervised data mining of financial time series in order to discover hidden motifs within the data. To perform the analysis of a given time series and form a comparison between the morphologies this paper proposes the implementation of the “Bolman Time Series Power Comparison” algorithm which will extract the pertinent time series motifs from the underlying dataset.
Download
Paper Nr:	39
Title:	Exploring Robustness in a Combined Feature Selection Approach
Authors:	Alexander Wurl, Andreas Falkner, Alois Haselböck, Alexandra Mazak and Peter Filzmoser
Abstract:	A crucial task in the bidding phase of industrial systems is a precise prediction of the number of hardware components of specific types for the proposal of a future project. Linear regression models, trained on data of past projects, are efficient in supporting such decisions. The number of features used by these regression models should be as small as possible, so that determining their quantities generates minimal effort. The fact that training data are often ambiguous, incomplete, and contain outlier makes challenging demands on the robustness of the feature selection methods used. We present a combined feature selection approach: (i) iteratively learn a robust well-fitted statistical model and rule out irrelevant features, (ii) perform redundancy analysis to rule out dispensable features. In a case study from the domain of hardware management in Rail Automation we show that this approach assures robustness in the calculation of hardware components.
Download
Paper Nr:	68
Title:	Farm Detection based on Deep Convolutional Neural Nets and Semi-supervised Green Texture Detection using VIS-NIR Satellite Image
Authors:	Sara Sharifzadeh, Jagati Tata and Bo Tan
Abstract:	Farm detection using low resolution satellite images is an important topic in digital agriculture. However, it has not received enough attention compared to high-resolution images. Although high resolution images are more efficient for detection of land cover components, the analysis of low-resolution images are yet important due to the low-resolution repositories of the past satellite images used for timeseries analysis, free availability and economic concerns. The current paper addresses the problem of farm detection using low resolution satellite images. In digital agriculture, farm detection has significant role for key applications such as crop yield monitoring. Two main categories of object detection strategies are studied and compared in this paper; First, a two-step semi-supervised methodology is developed using traditional manual feature extraction and modelling techniques; the developed methodology uses the Normalized Difference Moisture Index (NDMI), Grey Level Co-occurrence Matrix (GLCM), 2-D Discrete Cosine Transform (DCT) and morphological features and Support Vector Machine (SVM) for classifier modelling. In the second strategy, high-level features learnt from the massive filter banks of deep Convolutional Neural Networks (CNNs) are utilised. Transfer learning strategies are employed for pretrained Visual Geometry Group Network (VGG-16) networks. Results show the superiority of the high-level features for classification of farm regions.
Download
Short Papers
Paper Nr:	6
Title:	BIpm: Combining BI and Process Mining
Authors:	Mohammad Reza Harati Nik, Wil P. van der Aalst and Mohammadreza Fani Sani
Abstract:	In this paper, we introduce a custom visual for Microsoft Power BI that supports process mining and business intelligence analysis simultaneously using a single platform. This tool is called BIpm, and it brings the simple, agile, user-friendly, and affordable solution to study process models over multidimensional event logs. The Power BI environment provides many self-service BI and OLAP features that can be exploited through our custom visual aimed at the analysis of process data. The resulting toolset allows for accessing various input data sources and generating online reports and dashboards. Rather than designing and working with reports in the Power BI service on the web, it can be possible to view them in the Power BI mobile apps, and this means BIpm provides a solution to have process mining visualizations on mobiles. Therefore, BIpm can encourage many businesses and organizations to do process mining analysis with business intelligence analytics. Consequently, it yields managers and decision makers to translate discovered insights comprehensively to gain improved decisions and better performance more quickly.
Download
Paper Nr:	34
Title:	A Graded Concept of an Information Model for Evaluating Performance in Team Handball
Authors:	Friedemann Schwenkreis
Abstract:	Although team handball is a very popular sport in Europe, computer science did almost completely ignore that area in the past. This article introduces a graded approach for an information model that allows to express the effectiveness of a team as well as of single players, thus providing a basis for information-based decisions of coaches, as well as for applying analytical methods. From this perspective, the article is an early step to further introduce digitalization via a data model into a very classical sports area, however, introducing mechanisms that take into account the available degree of digitalization.
Download
Paper Nr:	40
Title:	Detection of e-Commerce Anomalies using LSTM-recurrent Neural Networks
Authors:	Merih Bozbura, Hunkar C. Tunc, Miray E. Kusak and C. O. Sakar
Abstract:	As the e-commerce sales grow in global retail sector year by year, detecting anomalies that occur in the most important key performance indicators (KPI) in real-time has become a critical requirement for e-commerce companies. Such anomalies that may arise from software updates, server failures, or incorrect price entries cause substantial revenue loss in the meantime until they are detected with their root-causes. In this paper, we present a comparative analysis of various anomaly detection methods in detecting e-commerce anomalies. For this purpose, we first present the univariate analysis of six commonly used anomaly detection methods on two important KPIs of an e-commerce website. The highest F1 Scores and recall values on the test sets of both KPIs are obtained using Long-Short Term Memory (LSTM) network, showing that LSTM fits better to the dynamics of e-commerce KPIs than time-series based prediction methods. Then, in addition to the univariate analysis of the methods, we feed the campaign information into LSTM network considering that campaigns have significant effects on the values of KPIs in e-commerce domain and this information can be helpful to prevent false positives that may occur in the campaign periods. The results also show that constructing a multivariate LSTM by feeding the campaign information as an additional input improves the adaptability of the model to sudden changes occurring in campaign periods.
Download
Paper Nr:	42
Title:	Approaches to Identify Relevant Process Variables in Injection Moulding using Beta Regression and SVM
Authors:	Shailesh Tripathi, Sonja Strasser, Christian Mittermayr, Matthias Dehmer and Herbert Jodlbauer
Abstract:	In this paper, we analyze data from an injection moulding process to identify key process variables which influence the quality of the production output. The available data from the injection moulding machines provide information about the run-time, setup parameters of the machines and the measurements of different process variables through sensors. Additionally, we have data about the total output produced and the number of scrap parts. In the first step of the analysis, we preprocessed the data by combining the different sets of data for a whole process. Then we extracted different features, which we used as input variables for modeling the scrap rate. For the predictive modeling, we employed three different models, beta regression with the backward selection, beta boosting with regularization and SVM regression with the radial kernel. All these models provide a set of common key features which affect the scrap rates.
Download
Paper Nr:	46
Title:	A Data Mining Study on Pressure Ulcers
Authors:	Francisco Mota, Nuno Abreu, Tiago Guimarães and Manuel F. Santos
Abstract:	Nurses follow well-defined guidelines in order to avoid the occurrence of pressure ulcers (pU) in patients under their care, not being always successful. This work intends to produce prediction models using Data Mining (DM) techniques in order to anticipate uP treatment. The work was conducted in the Oporto Hospital Center (CHP). For the construction of this DM study, the phases of the CRISP DM methodology were taken into account. In particular, the DM focus is to show that the time factor and frequency of interventions may influence the prediction of pU classification models. To prove this, we used a data set (containing 1339 records) where different classification techniques were applied using WEKA tool. Through the classification technique (decision tree), it was possible to create a guideline that contains all the scenarios and instructions that the professional can use in order to avoid patients to develop pU. For its construction we used the model that presented a higher percentage of sensitivity (number of positive cases correctly classified as ""NO"" developed pU). The conclusions were: the factors studied are good predictors of PU and the guideline obtained, through automatic techniques, can help professionals apply care to the patient more quickly.
Download
Paper Nr:	54
Title:	Empowered by Innovation: Unravelling Determinants of Idea Implementation in Open Innovation Platforms
Authors:	Frederik Situmeang, Rob Loke, Nelleke de Boer and Danielle de Boer
Abstract:	Companies use crowdsourcing to solve specific problems or to search for innovation. By using open innovation platforms, where community members propose ideas, companies can better serve customer needs. So far, it remains unclear which factors influence idea implementation in crowd sourcing context. With the research idea that we present here, we aim to get a better understanding of the success and failure of ideas by examining relationships between characteristics of ideators, characteristics of ideas and the likelihood of implementation. In order to test the methodological approach that we propose in this paper in which we investigate for business relevant innovativeness as well as sentiment based on text analytics, data including unstructured text was mined from Dell IdeaStorm using webcrawling and scraping techniques. Some relevant hypotheses that we define in this paper were confirmed on the Dell IdeaStorm dataset but in order to generalize our findings we want to apply to the Lego dataset in our current work in progress. Possible implications of our novel research idea can be used to fill theoretical gaps in marketing literature, help companies to better structure their search for innovation and for ideators to better understand factors contributing to successful idea generation.
Download
Paper Nr:	63
Title:	Machine Learning Approach for National Innovation Performance Data Analysis
Authors:	Dominik Forner, Sercan Ozcan and David Bacon
Abstract:	National innovation performance is essential for being economically competitive. The key determinants for its increase or decrease and the impact of governmental decisions or policy instruments are still not clear. Recent approaches are either limited due to qualitatively selected features or due to a small database with few observations. The aim of this paper is to propose a suitable machine learning approach for national innovation performance data analysis. We use clustering and correlation analysis, Bayesian Neural Network with Local Interpretable Model-Agnostic Explanations and BreakDown for decomposing innovation output prediction. Our results show, that the machine learning approach is appropriate to benchmark national innovation profiles, to identify key determinants on a cluster as well as on a national level whilst considering correlating features and long term effects and the impact of changes in innovation input (e.g. by governmental decision or innovation policy) on innovation output can be predicted and herewith the increase or decrease of national innovation performance.
Download
Paper Nr:	65
Title:	On Bayes Factors for Success Rate A/B Testing
Authors:	Maciej Skorski
Abstract:	This paper discusses Bayes factors, an alternative to classical frequentionist hypothesis testing, within the standard A/B proportion testing setup - observing outcomes of independent trails (which finds applications in industrial conversion testing). It is shown that the Bayes factor is controlled by the Jensen-Shannon divergence of success ratios in two tested groups, and the latter one is bounded (under mild conditions) by Welch’s t-statistic. The result implies an optimal bound on the necessary sample size for Bayesian testing, and demonstrates the relation to its frequentionist counterpart (effectively bridging Bayes factors and p-values).
Download
Paper Nr:	78
Title:	Sentiment Analysis of German Emails: A Comparison of Two Approaches
Authors:	Bernd Markscheffel and Markus Haberzettl
Abstract:	The increasing number of emails sent daily to the customer service of companies confronts them with new challenges. In particular, a lack of resources to deal with critical concerns, such as complaints, poses a threat to customer relations and the public perception of companies. Therefore, it is necessary to prioritize these concerns in order to avoid negative effects. Sentiment analysis, i.e. the automated recognition of the mood in texts, makes such prioritisation possible. The sentiment analysis of German-language emails is still an open research problem. Moreover, there is no evidence of a dominant approach in this context. Therefore two approaches are compared, which are applicable in the context of the problem definition mentioned. The first approach is based on the combination of sentiment lexicons and machine learning methods. This is to be extended by the second approach in such a way that in addition to the lexicons further features are used. These features are to be generated by the use of feature extraction methods. The methods used in both approaches are investigated in a systematic literature search. A Gold Standard corpus is generated as basis for the comparison of these approaches. Systematic experiments are carried out in which the different method combinations for the approaches are examined. The results of the experiments show that the combination of feature extracting methods with Sentiment lexicons and machine learning approaches generates the best classification results.
Download
Paper Nr:	79
Title:	Data Analytics for Smart Manufacturing: A Case Study
Authors:	Nadeem Iftikhar, Thorkil Baattrup-Andersen, Finn E. Nordbjerg, Eugen Bobolea and Paul-Bogdan Radu
Abstract:	Due to the emergence of the fourth industrial revolution, manufacturing business all over the world is changing dramatically; it needs enhanced efficiency, competency and productivity. More and more manufacturing machines are equipped with sensors and the sensors produce huge volume of data. Most of the companies do neither realize the value of data nor how to capitalize the data. The companies lack techniques and tools to collect, store, process and analyze the data. The objective of this paper is to propose data analytic techniques to analyze manufacturing data. The analytic techniques will provide both descriptive and predictive analysis. In addition, data from the company’s ERP system is integrated in the analysis. The proposed techniques will help the companies to improve operational efficiency and achieve competitive benefits.
Download
Paper Nr:	14
Title:	How to Boost Customer Relationship Management via Web Mining Benefiting from the Glass Customer’s Openness
Authors:	Frederik S. Bäumer and Bianca Buff
Abstract:	Customer Relationship Management refers to the consistent orientation of a company towards its customers. Since this requires customer-specific data sets, techniques such as web mining are used to acquire information about customers and their behavior. In this case study, we show how web mining can be used to automatically collect information from clients’ websites for Customer Relationship Management systems in Business-to-Business environments. Here, we use tailored local grammars to extract relevant information in order to build up a data set that meets the required high quality standards. The evaluation shows that local grammars produce substantial high-quality results, but "
2019-b,"Full Papers
Paper Nr:	1
Title:	Pipelined Implementation of a Parallel Streaming Method for Time Series Correlation Discovery on Sliding Windows
Authors:	Boyan Kolev, Reza Akbarinia, Ricardo Jimenez-Peris, Oleksandra Levchenko, Florent Masseglia, Marta Patino and Patrick Valduriez
Abstract:	This paper addresses the problem of continuously finding highly correlated pairs of time series over the most recent time window. The solution builds upon the ParCorr parallel method for online correlation discovery and is designed to run continuously on top of the UPM-CEP data streaming engine through efficient streaming operators. The implementation takes advantage of the flexible API of the streaming engine that provides low level primitives for developing custom operators. Thus, each operator is implemented to process incoming tuples on-the-fly and hence emit resulting tuples as early as possible. This guarantees a real pipelined flow of data that allows for outputting early results, as the experimental evaluation shows.
Download
Paper Nr:	2
Title:	GPU Acceleration of PySpark using RAPIDS AI
Authors:	Abdallah Aguerzame, Benoit Pelletier and François Waeselynck
Abstract:	RAPIDS AI is a promising open source project for accelerating Python end to end data science workloads. Our quest is to be able to integrate RAPIDS AI capabilities within PySpark and offload PySpark intensive tasks to GPUs to gain in performance.
Download
Paper Nr:	4
Title:	Using a Skip-gram Architecture for Model Contextualization in CARS
Authors:	Dimitris Poulopoulos and Athina Kalampogia
Abstract:	In this paper, we describe how a major retailer’s recommender system contextualizes the information that is passed to it, to provide real-time in-store recommendations, at a high level. We specifically focus on the data pre-processing ideas that were necessary for the model to learn. The paper describes the ideas and reasoning behind crucial data transformations, and then illustrates a learning model inspired by the work done in Natural Language Processing.
Download
Paper Nr:	5
Title:	Recovery in CloudDBAppliance’s High-availability Middleware
Authors:	Hugo Abreu, Luis Ferreira, Fábio Coelho, Ana N. Alonso and José Pereira
Abstract:	In the context of the CloudDBAppliance (CDBA) project, fault tolerance and high-availability are provided in layers: within each appliance, within a data centre and between datacentres. This paper presents the recovery mechanisms in place to fulfill the provision of high-availability within a datacentre. The recovery mechanism takes advantage of CDBA’s in-middleware replication mechanism to bring failed replicas up-to-date. Along with the description of different variants of the recovery mechanism, this paper provides their comparative evaluation, focusing on the time it takes to recover a failed replica and how the recovery process impacts throughput.
Download
Paper Nr:	6
Title:	Fast and Streaming Analytics in ATM Cash Management
Authors:	Terpsichori-Helen Velivassaki and Panagiotis Athanasoulis
Abstract:	Cash management across a network of ATMs can be greatly improved, exploiting a multitude of information sources, which however generate huge datasets, not possible to be analysed via traditional methods. Business Intelligence in the Banking, Financial Services and Insurance (BFSI) sector can be even more challenging when exploitation of real-time information streams is desired. This paper presents the uCash ATM cash management system, running on top of CloudDBAppliance, supporting fast analytics over historical data, as well as streaming analytics functionalities over real-time data, served by its Operational Database. The paper discussed integration points with the platform and provides integration hints, thus constituting a real use case example of the CloudDBAppliance platform, allowing for its further exploitation in various application domains.
Download
Paper Nr:	7
Title:	Implementing Value-at-Risk and Expected Shortfall for Real Time Risk Monitoring
Authors:	Petra Ristau
Abstract:	Regulatory standards require financial service providers and banks to calculate certain risk figures, such as Value at Risk (VaR) and Expected Shortfall (ES). If properly calculated, their formulas are based on a Monte-Carlo simulation, which is computationally complex. This paper describes architecture and development considerations of a use case building a demonstrator for a big data analytics cloud platform developed in the project CloudDBAppliance (CDBA). The chosen approach will allow for real time risk monitoring using cloud computing and a fast analytical processing platform and data base.
Download
Paper Nr:	8
Title:	Parallel Efficient Data Loading
Authors:	Ricardo Jiménez-Peris, Francisco Ballesteros, Ainhoa Azqueta, Pavlos Kranas, Diego Burgos and Patricio Martínez
Abstract:	In this paper we discuss how we architected and developed a parallel data loader for LeanXcale database. The loader is characterized for its efficiency and parallelism. LeanXcale can scale up and scale out to very large numbers and loading data in the traditional way it is not exploiting its full potential in terms of the loading rate it can reach. For this reason, we have created a parallel loader that can reach the maximum insertion rate LeanXcale can handle. LeanXcale also exhibits a dual interface, key-value and SQL, that has been exploited by the parallel loader. Basically, the loading leverages the key-value API and results in a highly efficient process that avoids the overhead of SQL processing. Finally, in order to guarantee the parallelism we have developed a data sampler that samples data to generate a histogram of data distribution and use it to pre-split the regions across LeanXcale instances to guarantee that all instances get an even amount of data during loading, thus guaranteeing the peak processing loading capability of the deployment.
Download
Paper Nr:	9
Title:	Dynamic Data Streaming for an Appliance
Authors:	Marta Patiño and Ainhoa Azqueta
Abstract:	Many applications require to analyse large amounts of continuous flows of data produced by different data sources before the data is stored. Data streaming engines emerged as a solution for processing data on the fly. At the same time, computer architectures have evolved to systems with several interconnected CPUs and Non Uniform Memory Access (NUMA), where the cost of accessing memory from a core depends on how CPUs are interconnected. In order to get better resource utilization and adaptiveness to the load dynamic migration of queries must be available in data streaming engines. Moreover, data streaming applications require high availability so that failures do not cause service interruption and losing data. This paper presents the dynamic migration and fault-tolerance capabilities of UPM-CEP, a data streaming engine designed to take advantage of NUMA architectures. The preliminary evaluation using Intel HiBench benchmark shows the effect of the query migration and fault-tolerance on the system performance.
Download"
2019-c,"Short Papers
Paper Nr:	1
Title:	Subject Database Construction based on Literature Information Extraction
Authors:	Yumeng Ma and Fang Wang
Abstract:	Researchers put forward higher requirements for efficient acquisition and utilization of domain data in the Big data era. As a tool to manage data in a specific domain and provide subject services, the subject database can be used to meet the users' personalized needs. This paper designs the construction route of subject database for specific research problems. Information extraction method based on knowledge engineering is adopted. Firstly, subject data model is built through abstraction of the research elements. Then under the guidance of the data model, information extraction strategy of each model node is developed to analyse, extract and correlate entities in literature. Finally, a database platform based on these structured data is developed that can provide a variety of services. Taking the construction practices in the field of activating blood circulation and removing stasis as an example, this paper analyses how to construct subject database based on literature information extraction. As this study proposes an effective technical route to build subject database to help researchers acquire data in literature quickly and accurately, it provides a transformation mode of resource construction and personalized precision services in the data-intensive research environment."
2020-a,""
2020-b,"Short Papers
Paper Nr:	1
Title:	Social Modeling based on Event Detection: Research Outline
Authors:	Aigerim Mussina and Sanzhar Aubakirov
Abstract:	Social networks already play a significant role in human’s daily life. We are used to sharing almost everything with other users. Therefore social networks have become an arena of enormous opportunities to perform data analysis. Social media analytics applies to digital marketing, social opinion analysis, political situation monitoring, natural disaster notification. Various commercial and government organizations want to track, manage and predict information threads flow in digital space. It is possible to detect events on which people are reacting in every moment in online social networks. Event detection is a powerful data analysing process useful for social modeling.
Paper Nr:	1
Title:	Social Modeling based on Event Detection: Research Outline
Authors:	Aigerim Mussina and Sanzhar Aubakirov
Abstract:	Social networks already play a significant role in human’s daily life. We are used to sharing almost everything with other users. Therefore social networks have become an arena of enormous opportunities to perform data analysis. Social media analytics applies to digital marketing, social opinion analysis, political situation monitoring, natural disaster notification. Various commercial and government organizations want to track, manage and predict information threads flow in digital space. It is possible to detect events on which people are reacting in every moment in online social networks. Event detection is a powerful data analysing process useful for social modeling.
Paper Nr:	2
Title:	An Approach to Manage Software Documentation in Scrum Projects
Authors:	Anbreen Javed
Abstract:	Software documentation is an integral source of knowledge for development activities and stakeholders. The application of an Agile Software Development (ASD) method, for instance scrum, is common nowadays in delivering state-of-the-art software but documentation is rare in scrum projects. Even scrum development requires a certain amount of documentation and scrum teams consider documentation to be an important issue. This makes it essential to obtain more detailed facts from scrum projects on what experts need and how their needs can be met. Agile literature does not provide evidence on how scrum teams currently handle software documentation and the challenges they face while dealing documentation. Scrum experts will be interviewed for gaining an in-depth understanding about the challenges faced and practices applied for handling documentation while using scrum. We plan to develop an optimized solution in-line with agile principles, which will address majority of the problems by mitigating the root, causes of the issues within scrum context. A focus group of scrum experts will assess the effectiveness of the solution and identifying the issues from implementation point of view. We will validate the improved version of the solution in the light of feedback from the focus group through multiple case studies. Our study will provide us better understanding about the practices that are being applied by experts for handling documentation within scrum projects and the manifest challenges they face while managing documentation. Moreover, a solution to address documentation effectively in the real-world scrum projects will improve the product quality and provide long-term benefits to the software industry.
Paper Nr:	2
Title:	An Approach to Manage Software Documentation in Scrum Projects
Authors:	Anbreen Javed
Abstract:	Software documentation is an integral source of knowledge for development activities and stakeholders. The application of an Agile Software Development (ASD) method, for instance scrum, is common nowadays in delivering state-of-the-art software but documentation is rare in scrum projects. Even scrum development requires a certain amount of documentation and scrum teams consider documentation to be an important issue. This makes it essential to obtain more detailed facts from scrum projects on what experts need and how their needs can be met. Agile literature does not provide evidence on how scrum teams currently handle software documentation and the challenges they face while dealing documentation. Scrum experts will be interviewed for gaining an in-depth understanding about the challenges faced and practices applied for handling documentation while using scrum. We plan to develop an optimized solution in-line with agile principles, which will address majority of the problems by mitigating the root, causes of the issues within scrum context. A focus group of scrum experts will assess the effectiveness of the solution and identifying the issues from implementation point of view. We will validate the improved version of the solution in the light of feedback from the focus group through multiple case studies. Our study will provide us better understanding about the practices that are being applied by experts for handling documentation within scrum projects and the manifest challenges they face while managing documentation. Moreover, a solution to address documentation effectively in the real-world scrum projects will improve the product quality and provide long-term benefits to the software industry.
Paper Nr:	3
Title:	Deep Reinforcement Learning Applied to a University Environment
Authors:	Gulim Moldash
Abstract:	Deep learning opens up enormous prospects for us. His ability to learn independently and predict the results can be used to solve many everyday problems, for example, to predict the course of action, to recognize speech. At present, deep learning has gained great interest in the academic field. The relevance in the academic field is determined by the need for system development, which will help in the learning process to monitor and provide feedback to increase the level of competence of students' knowledge. The purpose of this article is to create the basis for the further development of modeling the process of student interaction with the university.
Paper Nr:	3
Title:	Deep Reinforcement Learning Applied to a University Environment
Authors:	Gulim Moldash
Abstract:	Deep learning opens up enormous prospects for us. His ability to learn independently and predict the results can be used to solve many everyday problems, for example, to predict the course of action, to recognize speech. At present, deep learning has gained great interest in the academic field. The relevance in the academic field is determined by the need for system development, which will help in the learning process to monitor and provide feedback to increase the level of competence of students' knowledge. The purpose of this article is to create the basis for the further development of modeling the process of student interaction with the university.
Paper Nr:	4
Title:	Data Quality Evaluation using Probability Models
Authors:	Allen O'Neill
Abstract:	This paper discusses an approach with machine-learning probability models to evaluate the difference between good and bad data quality in a dataset. A decision tree algorithm is used to predict data quality based on no domain knowledge of the datasets under examination. It is shown that for the data examined, the ability to predict the quality of data based on simple good/bad pre-labelled learning examples is accurate, however in general it may not be sufficient for useful production data quality assessment.
Paper Nr:	4
Title:	Data Quality Evaluation using Probability Models
Authors:	Allen O'Neill
Abstract:	This paper discusses an approach with machine-learning probability models to evaluate the difference between good and bad data quality in a dataset. A decision tree algorithm is used to predict data quality based on no domain knowledge of the datasets under examination. It is shown that for the data examined, the ability to predict the quality of data based on simple good/bad pre-labelled learning examples is accurate, however in general it may not be sufficient for useful production data quality assessment.
Paper Nr:	5
Title:	Industrial Relevance of Software Testing Education: Improving Software Testing Curricula
Authors:	Bushra Hamid
Abstract:	Software testing education aim to equip students with skills that are vital in the software industry. These undergraduate students are expected to emerge as ready to step directly into software tester positions and succeed. Therefore, it is imperative to investigate what skills and capabilities software industry expect from computing graduates to possess before starting their career as software tester. In this research study, we will identify the requirements/demands that software industry requires from fresh computing graduates to become junior software testers. We will then review the current software testing curriculum, taught at various universities in Pakistan, to see whether the existing curriculum is aligned with the expectations of the software industry. If not, we will tailor the software testing curriculum according to industry needs and assess whether or not new graduates meet software industry expectations.
Paper Nr:	5
Title:	Industrial Relevance of Software Testing Education: Improving Software Testing Curricula
Authors:	Bushra Hamid
Abstract:	Software testing education aim to equip students with skills that are vital in the software industry. These undergraduate students are expected to emerge as ready to step directly into software tester positions and succeed. Therefore, it is imperative to investigate what skills and capabilities software industry expect from computing graduates to possess before starting their career as software tester. In this research study, we will identify the requirements/demands that software industry requires from fresh computing graduates to become junior software testers. We will then review the current software testing curriculum, taught at various universities in Pakistan, to see whether the existing curriculum is aligned with the expectations of the software industry. If not, we will tailor the software testing curriculum according to industry needs and assess whether or not new graduates meet software industry expectations.
Paper Nr:	6
Title:	Big Data Analysis Systems in IoT Environments for Managing Privacy and Digital Identity: Pseudonymity, De-anonymization and the Right to Be Forgotten
Authors:	Emanuela Podda
Abstract:	The project aims at investigating the data anonymization considered as “a result of processing personal data with the aim of irreversibly preventing identification of the data subject” and its consequent issue of de-anonymization within the application of the EU GDPR and the EU FFD.
Paper Nr:	6
Title:	Big Data Analysis Systems in IoT Environments for Managing Privacy and Digital Identity: Pseudonymity, De-anonymization and the Right to Be Forgotten
Authors:	Emanuela Podda
Abstract:	The project aims at investigating the data anonymization considered as “a result of processing personal data with the aim of irreversibly preventing identification of the data subject” and its consequent issue of de-anonymization within the application of the EU GDPR and the EU FFD.
Paper Nr:	7
Title:	Improving Monitoring and Evaluation of Software Engineering Curriculum: A Concept Map based Approach
Authors:	Farkhanda Qamar
Abstract:	Curriculum Management is one of the cornerstones of quality higher education programs. Development of coherent curriculum encapsulating all learning activities, course sequencing, development of learning objectives, selection of appropriate course content and content sequencing is not an easy task. It requires multidimensional efforts to accomplish the task. The importance of an effective curriculum management is now accepted and understood among the masses and a lot of work has been done for effective curriculum management, still processes and practices for curriculum management remains human intensive and tedious. There is a need to thoroughly monitor and evaluate the implementation of curriculum with efficiency and effectiveness. We will develop a new paradigm of pedagogical computing that can offer a scheme to manage (i.e. design, measure, visualize, assess, compare) curriculum of a software engineering with unprecedented objectivity and details. It will allow experts to effectively visualize the deficiencies, gap in implementation, gaps and redundancies in lecturing each with precision and clarity.
Paper Nr:	7
Title:	Improving Monitoring and Evaluation of Software Engineering Curriculum: A Concept Map based Approach
Authors:	Farkhanda Qamar
Abstract:	Curriculum Management is one of the cornerstones of quality higher education programs. Development of coherent curriculum encapsulating all learning activities, course sequencing, development of learning objectives, selection of appropriate course content and content sequencing is not an easy task. It requires multidimensional efforts to accomplish the task. The importance of an effective curriculum management is now accepted and understood among the masses and a lot of work has been done for effective curriculum management, still processes and practices for curriculum management remains human intensive and tedious. There is a need to thoroughly monitor and evaluate the implementation of curriculum with efficiency and effectiveness. We will develop a new paradigm of pedagogical computing that can offer a scheme to manage (i.e. design, measure, visualize, assess, compare) curriculum of a software engineering with unprecedented objectivity and details. It will allow experts to effectively visualize the deficiencies, gap in implementation, gaps and redundancies in lecturing each with precision and clarity."
2020-c,"Short Papers
Paper Nr:	1
Title:	Social Modeling based on Event Detection: Research Outline
Authors:	Aigerim Mussina and Sanzhar Aubakirov
Abstract:	Social networks already play a significant role in human’s daily life. We are used to sharing almost everything with other users. Therefore social networks have become an arena of enormous opportunities to perform data analysis. Social media analytics applies to digital marketing, social opinion analysis, political situation monitoring, natural disaster notification. Various commercial and government organizations want to track, manage and predict information threads flow in digital space. It is possible to detect events on which people are reacting in every moment in online social networks. Event detection is a powerful data analysing process useful for social modeling.
Paper Nr:	1
Title:	Social Modeling based on Event Detection: Research Outline
Authors:	Aigerim Mussina and Sanzhar Aubakirov
Abstract:	Social networks already play a significant role in human’s daily life. We are used to sharing almost everything with other users. Therefore social networks have become an arena of enormous opportunities to perform data analysis. Social media analytics applies to digital marketing, social opinion analysis, political situation monitoring, natural disaster notification. Various commercial and government organizations want to track, manage and predict information threads flow in digital space. It is possible to detect events on which people are reacting in every moment in online social networks. Event detection is a powerful data analysing process useful for social modeling.
Paper Nr:	2
Title:	An Approach to Manage Software Documentation in Scrum Projects
Authors:	Anbreen Javed
Abstract:	Software documentation is an integral source of knowledge for development activities and stakeholders. The application of an Agile Software Development (ASD) method, for instance scrum, is common nowadays in delivering state-of-the-art software but documentation is rare in scrum projects. Even scrum development requires a certain amount of documentation and scrum teams consider documentation to be an important issue. This makes it essential to obtain more detailed facts from scrum projects on what experts need and how their needs can be met. Agile literature does not provide evidence on how scrum teams currently handle software documentation and the challenges they face while dealing documentation. Scrum experts will be interviewed for gaining an in-depth understanding about the challenges faced and practices applied for handling documentation while using scrum. We plan to develop an optimized solution in-line with agile principles, which will address majority of the problems by mitigating the root, causes of the issues within scrum context. A focus group of scrum experts will assess the effectiveness of the solution and identifying the issues from implementation point of view. We will validate the improved version of the solution in the light of feedback from the focus group through multiple case studies. Our study will provide us better understanding about the practices that are being applied by experts for handling documentation within scrum projects and the manifest challenges they face while managing documentation. Moreover, a solution to address documentation effectively in the real-world scrum projects will improve the product quality and provide long-term benefits to the software industry.
Paper Nr:	2
Title:	An Approach to Manage Software Documentation in Scrum Projects
Authors:	Anbreen Javed
Abstract:	Software documentation is an integral source of knowledge for development activities and stakeholders. The application of an Agile Software Development (ASD) method, for instance scrum, is common nowadays in delivering state-of-the-art software but documentation is rare in scrum projects. Even scrum development requires a certain amount of documentation and scrum teams consider documentation to be an important issue. This makes it essential to obtain more detailed facts from scrum projects on what experts need and how their needs can be met. Agile literature does not provide evidence on how scrum teams currently handle software documentation and the challenges they face while dealing documentation. Scrum experts will be interviewed for gaining an in-depth understanding about the challenges faced and practices applied for handling documentation while using scrum. We plan to develop an optimized solution in-line with agile principles, which will address majority of the problems by mitigating the root, causes of the issues within scrum context. A focus group of scrum experts will assess the effectiveness of the solution and identifying the issues from implementation point of view. We will validate the improved version of the solution in the light of feedback from the focus group through multiple case studies. Our study will provide us better understanding about the practices that are being applied by experts for handling documentation within scrum projects and the manifest challenges they face while managing documentation. Moreover, a solution to address documentation effectively in the real-world scrum projects will improve the product quality and provide long-term benefits to the software industry.
Paper Nr:	3
Title:	Deep Reinforcement Learning Applied to a University Environment
Authors:	Gulim Moldash
Abstract:	Deep learning opens up enormous prospects for us. His ability to learn independently and predict the results can be used to solve many everyday problems, for example, to predict the course of action, to recognize speech. At present, deep learning has gained great interest in the academic field. The relevance in the academic field is determined by the need for system development, which will help in the learning process to monitor and provide feedback to increase the level of competence of students' knowledge. The purpose of this article is to create the basis for the further development of modeling the process of student interaction with the university.
Paper Nr:	3
Title:	Deep Reinforcement Learning Applied to a University Environment
Authors:	Gulim Moldash
Abstract:	Deep learning opens up enormous prospects for us. His ability to learn independently and predict the results can be used to solve many everyday problems, for example, to predict the course of action, to recognize speech. At present, deep learning has gained great interest in the academic field. The relevance in the academic field is determined by the need for system development, which will help in the learning process to monitor and provide feedback to increase the level of competence of students' knowledge. The purpose of this article is to create the basis for the further development of modeling the process of student interaction with the university.
Paper Nr:	4
Title:	Data Quality Evaluation using Probability Models
Authors:	Allen O'Neill
Abstract:	This paper discusses an approach with machine-learning probability models to evaluate the difference between good and bad data quality in a dataset. A decision tree algorithm is used to predict data quality based on no domain knowledge of the datasets under examination. It is shown that for the data examined, the ability to predict the quality of data based on simple good/bad pre-labelled learning examples is accurate, however in general it may not be sufficient for useful production data quality assessment.
Paper Nr:	4
Title:	Data Quality Evaluation using Probability Models
Authors:	Allen O'Neill
Abstract:	This paper discusses an approach with machine-learning probability models to evaluate the difference between good and bad data quality in a dataset. A decision tree algorithm is used to predict data quality based on no domain knowledge of the datasets under examination. It is shown that for the data examined, the ability to predict the quality of data based on simple good/bad pre-labelled learning examples is accurate, however in general it may not be sufficient for useful production data quality assessment.
Paper Nr:	5
Title:	Industrial Relevance of Software Testing Education: Improving Software Testing Curricula
Authors:	Bushra Hamid
Abstract:	Software testing education aim to equip students with skills that are vital in the software industry. These undergraduate students are expected to emerge as ready to step directly into software tester positions and succeed. Therefore, it is imperative to investigate what skills and capabilities software industry expect from computing graduates to possess before starting their career as software tester. In this research study, we will identify the requirements/demands that software industry requires from fresh computing graduates to become junior software testers. We will then review the current software testing curriculum, taught at various universities in Pakistan, to see whether the existing curriculum is aligned with the expectations of the software industry. If not, we will tailor the software testing curriculum according to industry needs and assess whether or not new graduates meet software industry expectations.
Paper Nr:	5
Title:	Industrial Relevance of Software Testing Education: Improving Software Testing Curricula
Authors:	Bushra Hamid
Abstract:	Software testing education aim to equip students with skills that are vital in the software industry. These undergraduate students are expected to emerge as ready to step directly into software tester positions and succeed. Therefore, it is imperative to investigate what skills and capabilities software industry expect from computing graduates to possess before starting their career as software tester. In this research study, we will identify the requirements/demands that software industry requires from fresh computing graduates to become junior software testers. We will then review the current software testing curriculum, taught at various universities in Pakistan, to see whether the existing curriculum is aligned with the expectations of the software industry. If not, we will tailor the software testing curriculum according to industry needs and assess whether or not new graduates meet software industry expectations.
Paper Nr:	6
Title:	Big Data Analysis Systems in IoT Environments for Managing Privacy and Digital Identity: Pseudonymity, De-anonymization and the Right to Be Forgotten
Authors:	Emanuela Podda
Abstract:	The project aims at investigating the data anonymization considered as “a result of processing personal data with the aim of irreversibly preventing identification of the data subject” and its consequent issue of de-anonymization within the application of the EU GDPR and the EU FFD.
Paper Nr:	6
Title:	Big Data Analysis Systems in IoT Environments for Managing Privacy and Digital Identity: Pseudonymity, De-anonymization and the Right to Be Forgotten
Authors:	Emanuela Podda
Abstract:	The project aims at investigating the data anonymization considered as “a result of processing personal data with the aim of irreversibly preventing identification of the data subject” and its consequent issue of de-anonymization within the application of the EU GDPR and the EU FFD.
Paper Nr:	7
Title:	Improving Monitoring and Evaluation of Software Engineering Curriculum: A Concept Map based Approach
Authors:	Farkhanda Qamar
Abstract:	Curriculum Management is one of the cornerstones of quality higher education programs. Development of coherent curriculum encapsulating all learning activities, course sequencing, development of learning objectives, selection of appropriate course content and content sequencing is not an easy task. It requires multidimensional efforts to accomplish the task. The importance of an effective curriculum management is now accepted and understood among the masses and a lot of work has been done for effective curriculum management, still processes and practices for curriculum management remains human intensive and tedious. There is a need to thoroughly monitor and evaluate the implementation of curriculum with efficiency and effectiveness. We will develop a new paradigm of pedagogical computing that can offer a scheme to manage (i.e. design, measure, visualize, assess, compare) curriculum of a software engineering with unprecedented objectivity and details. It will allow experts to effectively visualize the deficiencies, gap in implementation, gaps and redundancies in lecturing each with precision and clarity.
Paper Nr:	7
Title:	Improving Monitoring and Evaluation of Software Engineering Curriculum: A Concept Map based Approach
Authors:	Farkhanda Qamar
Abstract:	Curriculum Management is one of the cornerstones of quality higher education programs. Development of coherent curriculum encapsulating all learning activities, course sequencing, development of learning objectives, selection of appropriate course content and content sequencing is not an easy task. It requires multidimensional efforts to accomplish the task. The importance of an effective curriculum management is now accepted and understood among the masses and a lot of work has been done for effective curriculum management, still processes and practices for curriculum management remains human intensive and tedious. There is a need to thoroughly monitor and evaluate the implementation of curriculum with efficiency and effectiveness. We will develop a new paradigm of pedagogical computing that can offer a scheme to manage (i.e. design, measure, visualize, assess, compare) curriculum of a software engineering with unprecedented objectivity and details. It will allow experts to effectively visualize the deficiencies, gap in implementation, gaps and redundancies in lecturing each with precision and clarity."
2020-d,"Short Papers
Paper Nr:	1
Title:	How to Develop Digital Products for Industrial Environments: The Data Science & Engineering Process in PLM
Authors:	Peter Louis and Ralf Russ
Abstract:	Digitalization unlocks huge business potentials for products and services that create value based on the evaluation of data. Successful implementation depends on systematic procedures for managing and analyzing data to create insight with value for businesses. However, such procedures are typically not covered in today’s processes. From our experience, organizations start processing the data that happens to be available. In fact, this data often does not cover all relevant parameters in the situation of interest and typically lacks the quality to be processable. In industrial environments, the reliability and accuracy of results are critical for success. Therefore, an enormous responsibility comes with the development of digital products. Unless there are systematic procedures in place to guide data management and data analysis tasks in the development lifecycle, many promising digital industrial products will not meet expectations. We substantiate why Data Science and Engineering should be introduced as new engineering discipline in the PLM process, and outline the workflows and tasks of a systematic framework for developing digital products for industrial environments."
2021-a,""
2021-b,"Area 1 - Business Analytics
Nr:	13
Title:	Real Estate Price Prediction with Artificial Intelligence Techniques
Authors:	Sophia L. Zhou
Abstract:	For investors, businesses, consumers, and governments, an accurate assessment of future housing prices is crucial to critical decisions in resource allocation, policy formation, and investment strategies. Previous studies are contradictory about macroeconomic determinants of housing price and largely focused on one or two areas using point prediction. This study aims to develop data-driven models to accurately predict future housing market trends in different markets. This work studied five different metropolitan areas representing different market trends and compared three time lagging situations: no lag, 6-month lag, and 12-month lag. Linear regression (LR), random forest (RF), and artificial neural network (ANN) were employed to model the real estate price using datasets with S&P/Case-Shiller home price index and 12 demographic and macroeconomic features, such as gross domestic product (GDP), resident population, personal income, etc. in five metropolitan areas: Boston, Dallas, New York, Chicago, and San Francisco. The data from March, 2005 to December 2018 were collected from the Federal Reserve Bank, FBI, and Freddie Mac. In the original data, some factors are monthly, some quarterly, and some yearly. Thus, two methods to compensate missing values, backfill or interpolation, were compared. The models were evaluated by accuracy, mean absolute error, and root mean square error. The LR and ANN models outperformed the RF model due to RF’s inherent limitations. Both ANN and LR methods generated predictive models with high accuracy (>95%). It was found that personal income, GDP, population, and measures of debt consistently appeared as most important factors. It also showed that technique to compensate missing values in the dataset and implementation of time lag can have significant influence in the model performance and require further investigation. The best performing models varied for each area, but the backfilled 12-month lag LR models and the interpolated no lag ANN models showed best stable performance overall, with accuracies >95% for each city. This study reveals the influence of input variables in different markets. It also provides evidence to support future studies to identify the optimal time lag and data imputing methods for establishing accurate predictive models.
Nr:	18
Title:	Knowledge Graph based Electrical Circuit Simulation and Component Selection
Authors:	Rahman Syed, Johannes Bayer and Felix Thoma
Abstract:	Electrical circuits can be considered graph structures with components (like resistors, capacitors or inductors) as nodes and wiring as edges. For simulation and hardware implementation purposes, these nodes are equipped with attributes like electrical characteristics and referenced against libraries of real-world products. The presented system takes an RDF representation of a netlist and uses Ngspice to calculate circuit parameters. Additional parameters can be specified using formulas which are also represented in RDF. The parameters and signals calculated for components are then used as constraints to shortlist candidates from the product knowledge graph and shortlisted candidates can then be optimized for cost if the marginal costs of procurement for the products are known. Signal and device characteristics matching criteria as well as unit standardization formulas are also stored as RDF triples to simplify the addition of new device types, circuit characteristics and matching criteria. A circuit simulator is used to predict the voltages at nodes and current flows in wires. These values and parameters are substituted in formulae to derive additional values such as the component's power consumption. Formulae are specified in RDF and the system checks which of the specified formulae for a component can be applied given the set of known parameters. These values and parameters are added as an enrichment to the RDF representation of the circuit, which is then used to shortlist products. Product information for components such as resistance, capacitance, power output and prices are collected from web stores to build a knowledge graph of different device types. Multiple physical devices from various manufacturers and vendors, with differing parameters and physical characteristics can match component requirements known at this stage. This is achieved by filters for each known parameter for a circuit component to list the most suitable device matches. Component level shortlists of matching devices from the knowledge graph also provide the engineer pricing information extracted from vendor sites. While detailed costing for the final hardware implementation and cost optimization is still not achievable because of complicated pricing rules and ordering costs, the designer is given an overview of the potential options along with the pricing per piece and the minimum order size. In future, the system is envisioned to support the engineer by automated constraint checking and product recommendations for implementing and altering circuits.
Nr:	19
Title:	Automatic Measurement of Corporate Reputation for Retail Companies from Online Public Data on the Web
Authors:	Marselo Sitorus and Rob Loke
Abstract:	Retail industry consists of the establishment of selling consumer goods (i.e. technology, pharmaceuticals, food and beverages, apparels and accessories, home improvement etc.) and services (i.e. specialty and movies) to customers through multiple channels of distribution including both the traditional brick-and-mortar and online retailing. Managing corporate reputation of retail companies is crucial as it has many advantages, for instance, it has been proven to impact generated revenues (Wang et al., 2016). But, in order to be able to manage corporate reputation, one has to be able to measure it, or, nowadays even better, listen to relevant social signals that are out there on the public web. One of the most extensive and widely used frameworks for measuring corporate reputation is through conducting elaborated surveys with respective stakeholders (Fombrun et al., 2015). This approach is valuable but deemed to be laborious and resource-heavy and will not allow to generate automatic alerts and quick and live insights that are extremely needed in this era of internet. For these purposes a social listening approach is needed that can be tailored to online data such as consumer reviews as the main data source. Online review datasets are a form of electronic Word-of-Mouth (WOM) that, when a data source is picked that is relevant to retail, commonly contain relevant information about customers’ perceptions regarding products (Pookulangara, 2011) and that are massively available. The algorithm that we have built in our application provides retailers with reputation scores for all variables that are deemed to be relevant to retail in the model of Fombrun et al. (2015). Examples of such variables for products and services are high quality, good value, stands behind, and meets customer needs. We propose a new set of subvariables with which these variables can be operationalized for retail in particular. Scores are being calculated using proportions of positive opinion pairs such as <fast, delivery> or <rude, staff> that have been designed per variable. With these important insights extracted, companies can act accordingly and proceed to improve their corporate reputation. It is important to emphasize that, once the design is complete and implemented, all processing can be performed completely automatic and unsupervised. The application makes use of a state of the art aspect-based sentiment analysis (ABSA) framework because of ABSA's ability to generate sentiment scores for all relevant variables and aspects. Since most online data is in open form and we deliberately want to avoid labelling any data by human experts, the unsupervised aspectator algorithm has been picked. It employs a lexicon to calculate sentiment scores and uses syntactic dependency paths to discover candidate aspects (Bancken et al., 2014). We have applied our approach to a large number of online review datasets that we sampled from a list of 50 top global retailers according to National Retail Federation (2020), including both offline and online operation, and that we scraped from trustpilot, a public website that is well-known to retailers. The algorithm has carefully been evaluated by manually annotating a randomly sampled subset of the datasets for validation purposes by two independent annotators. The Kappa’s score on this subset was 80%."
2022-a,"Accelerate AI/ML Deployments with Enterprise-Grade MLOps 
""Feathr: Scalable Feature Store that opens the Window to Infinite Possibilities In this session, we will dive deep into Feathr, taking you on a journey into this scalable open-source feature store which has now joined the Linux Foundation AI and Data ecosystem. Feathr has been battle-tested in LinkedIn powering high scale ML applications, supporting 100s of training and inferencing pipelines. This enables feature sharing among teams, leading to significant business metrics gain.

We will dive into some of the key highlights – rich UDF support, dynamic type casting, point-in-time joins, time aware sliding window aggregation, support for derived features, support for advanced ML scenarios and much more!

Feathr has a cloud-friendly scalable architecture and has been made available as an easy deployment on Azure. We will go over the key components – a central registry to store and share feature definitions, offline and online data store connectors, tight integration with Spark to run transformations and integration with various services using the Python SDK.

We will also showcase how Feathr can be used to build an end-to-end solution and go over some of the key customer patterns based on real life customer usage""
""How to Model Public Opinions in the Media Age The truths and lies of survey data, or how to model public opinions in the media age.

“ESG is a scam. It has been weaponized by phony social justice warriors.” Elon Musk ESG tweet.May 21, 2022
Do you think Elon Musk is right? What do other people think of ESG?

In the big data era, surprisingly, surveys still play a crucial role in market research. We know that people often have self-servicing biases in surveys. So how do you overcome these self-servicing biases in market research data?

I will talk about practical methods to model survey data effectively to avoid cheaters and self-attestation tendencies to lie, using statistics, analytics, and ML/AI.

In my talk, I will explain how to treat surveys as big data and combine them with additional data sources for effective modeling. Using benchmarking techniques and novel modeling approaches, we may find the underlying gems in this somewhat old-fashioned yet still relevant information source.

We will delve into how to understand what people think, feel, and what drives consumer activities by using a combination of surveys, mainstream and social media, reviews, and other data sources, by applying statistical methods combined with NLP and deep learning. I will demonstrate these techniques with a few thought-provoking examples from the vastly emerging data on Environmental, Social, and Governance (ESG) perceptions.""
""Creating An Ethical AI Environment We are at a pivotal time in our AI development and adoption where we still have the ability to create a world where AI is a force for good, instead of world where AI is used deepen inequalities and divides. To do this, we much create an ethical AI environment to move forward. Key takeaways from this talk include:
* The current state of ethical AI
* How develop an ethical AI environment
* The future of coexisting with machines""
""AI in a Minefield: Learning from Poisoned Data Data poisoning is one of the main threats on AI systems. When malicious actors have even limited control over the data used for training a model, they can try to fail the training process, prevent it from convergence, skewing the model or install so-called ML backdoors – areas where this model makes incorrect decisions, usually areas of interest for the attacker. This threat is especially applicable when security technologies use anomaly detection mechanisms on top of a normality model constructed from previously seen traffic data. When the traffic originates from unreliable sources, which may be partially controlled by malicious actors, the learning process needs to be designed under the assumption that data poisoning attempts are very likely to occur.

In this talk, we will present the challenges of learning from dirty data, overview data poisoning attacks on different systems like Spam detection, image classification and rating systems, discuss the problem of learning from web traffic - probably the dirtiest data in the world, and explain different approaches for learning from dirty data and poisoned data. We will focus on threshold-learning mitigation for data poisoning, aiming to reduce the impact of any single data source, and discuss a mundane but crucial aspect of threshold learning – memory complexity. We will present a robust learning scheme optimized to work efficiently on streamed data with bounded memory consumption. We will give examples from the web security arena with robust learning of URLs, parameters, character sets, cookies and more.""
An Intuition-Based Approach to Reinforcement Learning Reinforcement learning (RL) has achieved remarkable success in various tasks, such as defeating all-human teams in MMP (massive multi-player) games, advances in robotics, and astonishing results in the protein folding problem in chemistry. Expertise in RL requires strong knowledge of machine learning, statistics, and areas of mathematics. Moreover, RL contains many concepts that seem ""fuzzy"" and hence can be challenging for beginners who are trying to learn RL. However, this session provides the intuition of various RL concepts, such as exploit/explore and maximization of expected reward, along with real-life examples of these concepts. Attendees will also see a comparison of greedy versus epsilon greedy, and why epsilon greedy can solve tasks that cannot be solved using a greedy approach. Some of the preceding concepts will be illustrated during the presentation of the n-chain task in RL, whose solution clearly requires an epsilon greedy algorithm. The target audience for this session is for beginners who have no experience with RL.
Denoising Diffusion-based Generative Modeling Diffusion-based generative models such as DALL·E 2 have achieved exceptional image generation quality. Unlike other generative models based on explicit representations of probability distributions (e.g., autoregressive) or implicit sampling procedures (e.g., GANs), diffusion models learn directly the vector field of gradients of the data distribution (scores). This framework allows flexible architectures, requires no sampling during training or the use of adversarial training methods. These score-based generative models enable exact likelihood evaluation, achieve state-of-the-art sample quality, and can be used to improve performance in a variety of inverse problems, including medical imaging.
Building & Selling AI Startups Taylor will share some of the biggest regrets and lessons he has learned after a decade of building and selling AI startups. From Sequoia's HireVue to his own deep-learning startup Zeff which was acquired by DataRobot. Taylor will discuss key competencies and lessons needed when it comes to selling AI software in the market.
Introduction to Generative Art with Stable Diffusion, presented by HP Inc Hot on the heels of popular text-to-image Generative Art models like OpenAI's DALLE 2 and Midjourney, Stable Diffusion is an open source Latent Diffusion model that can be used to create images from just a few words. This talk will go through high level details of how the model was built and trained, and some of the precursors models that it built upon. Then it will move to some comments on the ethical implications of AI art models generally and ethical considerations for those using stable diffusion and models like it. The main portion of the talk will be covering with how to get started using Stable Diffusion. This will help those even with limited background in ML or deep learning techniques to get an understanding of practically how to work with the model. This section will also focus on tips, tricks and examples to improve your output images as well as hardware considerations when running this powerful model locally. Finally we will finish with a couple of examples of how this model can be used in a few specific workflows or applications and some examples of what future generative art models may be able to do.
Toward Robust, Knowledge-Rich Natural Language Processing Enormous amounts of ever-changing knowledge are available online in diverse textual styles and diverse formats. Recent advances in deep learning algorithms and large-scale datasets are spurring progress in many Natural Language Processing (NLP) tasks, including question answering. Nevertheless, these models cannot scale up when task-annotated training data are scarce. This talk presents my lab's work toward building general-purpose models in NLP and how to systematically evaluate them. I present a new meta-dataset – called super-Natural Instructions – that includes a variety of NLP tasks and their descriptions to evaluate cross-task generalization. Then, I introduce a new meta training approach that can solve more than 1600 NLP tasks only from their descriptions and a few examples.
""A New Era of Applied AI: How to Accelerate Enterprise Adoption of AI for Business Impact Global spend on AI continues to explode, even during the current economic climate, as C-Suite and senior executives realize the importance AI has and will have on their business models. Despite these massive investments, enterprises are challenged to show ROI from their AI initiatives due to complexity and lack of business adoption.

Gaurav will share his perspective on market trends in machine learning and AI as well as the challenges current enterprises face to deliver business value from data-centric AI. While business users and developers work in separate environments with separate tools, he will cover how to effectively bridge AI with business decision makers, the capabilities required, and example of doing this with a universal semantic layer.""
Operationalizing Organizational Knowledge with Data-Centric AI Data-centric AI broadly describes the idea that *data*, rather than models, is increasingly the crux of success or failure in AI for many settings and use cases. More specifically, data-centric AI defines ML development workflows that center around principally iterating on the *training data*–e.g. labeling, sampling, slicing, augmenting, etc.–rather than the model architecture. In this talk, I'll describe how programmatic or weak supervision can not only facilitate these data-centric workflows (in ways that manual labeling cannot), but more importantly, will present an overview about how it can serve as an API for rich organizational knowledge sources, presenting recent technical results and user case studies.
""Real-time Data Science Made Easy By 2025, analysts estimate that 30% of generated data will be real-time data.  This is 52ZB of real-time data per year and is roughly the amount of total data produced in 2020!  Soon, almost every data scientist and data engineer will be working with real-time data.  The future of data science is real-time. Existing technologies for working with static data fail when applied to real-time data because they have no way to incrementally update calculations and visualizations.  To make such enormous volumes of changing data useful, we need a toolkit for managing data, performing data science, and visualizing results in real-time.  In this talk, we will explore production-quality, real-time data science using the current leading open, real-time technologies:  Kafka, redpanda, ksqlDB, Materialize, and Deephaven Community Core.

Session Outline:
Concepts that will be discussed:
 Key components needed in a real-time data analytics system and how to choose the best open
options for your use case.
 Why are current static technologies, such as SQL and Pandas, unsuited to real-time data science?
(Transactional DB vs streaming table)
 Design patterns for building flexible real-time workflows.
 Efficiently publishing real-time data to many users in a programming language agnostic way.
 Using both real-time and static data in the same query. 
 Data exploration, visualization, and dashboards in real-time.
 Query language selection for data science: SQL (ksqlDB and Materialize) vs data-frame
(Deephaven).
 Working with time series.
 Real-time AI/ML on streams of data.
 Do current tools, such as Pandas, Matplotlib, and TensorFlow, have a place in a real-time world?""
""Turning your Data/AI Algorithms into Full Web Apps in no Time with Taipy In the Python open-source eco-system, many packages are available that cater to:
- the building of great algorithms
- the visualization of data
- back-end functions
Despite this, over 85% of Data Science Pilots remain pilots and do not make it to the production stage.
With Taipy, Data Scientists/Python Developers will be able to build great pilots as well as stunning production-ready applications for end-users.
Taipy provides two independent modules: Taipy GUI and Taipy Core.

In this talk, we will demonstrate how:
Taipy-GUI goes way beyond the capabilities of the standard graphical stack: Gradio, Streamlit, Dash, etc.
Taipy Core is simpler yet more powerful than the standard Python back-end stack: Airflow, MLFlow, Luigi, etc.

Bio: ""
Inclusive Search and Recommendations To truly bring everyone the inspiration to create a life they love, Pinterest is committed to content diversity and to developing inclusive search and recommendation engines. A top request we hear from Pinners is that they want to feel represented in the product. This is why we built the skin tone range and hair pattern technologies. These machine learning technologies are paving the way for more inclusive inspirations in Search and our augmented reality technology Try-On, and driving advances for more diverse recommendations across the platform. Developing inclusive AI in production requires an iterative and collaborative approach. We have learned the importance of building inclusive systems by design, of measuring to make progress, and of leveraging both artificial and human intelligence. We recognize that these challenges are multi-disciplinary, not just technical. In this talk, we describe sources of bias in ML for search and recommendation systems, techniques to mitigate bias, and production examples from our work at Pinterest to build inclusive search and recommendations.
Emerging Approaches to AI Governance: Tech-Led vs Policy-Led Over the past few years, many have become more familiar with the potential risks posed to the improper deployment and usage of AI/ML systems. Companies of almost all sizes and across almost all sectors have seen examples of major AI failures, leading into significant decay in trust of these systems. As a result, stakeholders across organizations have emerged as interested in remediating these risks and getting a handle on AI -- in owning AI governance. Some are drawn to technical capabilities which promise solutions to ethical problems and enable quality. Others rely on existing compliance and policy methods to enforce standards. In this session, we will describe what these different approaches look like, the pros and cons of each, and considerations to build a robust framework around AI governance that engages technical, business, and compliance teams.
5 Questions Business Leaders Should Ask When Investing in Machine Learning Projects While there is a lot of pressure on business leaders to leverage machine learning in their areas, there isn't a clear framework of how these leaders should decide on which ML projects to invest in and which ones not to invest in! This is a key reason why many ML project get left mid-way and many others don't show ROI post implementation - even after thousands of human hours have been put into them. In this session, you, as a business leader, will learn 5 questions that you must answer in deciding which ML project to invest - from prioritization to measuring real impact. I'll also share my experience seeing ML projects in action, driving real value, with use cases covering various industries.
""“DataOps 2.0” – How the Changing MLOps Landscape is Reinventing DataOps Today, commercial deployments of MLOps at a scale are going through an evolutionary change across almost all verticals and use cases. Notably, deployment and product at a scale are seeing a whole slew of commercialization issues arise as “edge cases” become more significant at scales of thousands, millions or even billions. As a result, “traditional pre-deployment training” is no longer sufficient to ensure quality and reliability standards in production, especially in edge devices. Personalization at the edge - cards, phones, cameras, etc. need to be continuously trained with custom & specific data relevant to their individual usage or environment to be safe and accurate. For example, the neural networks that govern the way an autonomous vehicle drives in New York City is not the same in Kansas City or Mexico City, and surely not in Yellowstone National Park. The explosions of data streams from billions of “sensors” in real & virtual worlds need to be constantly processed for retraining models and data preparation is getting extremely complex as use-cases strive to replicate normal human behavior.

The selection of a DataOps partner to navigate this scale, fragmentation, volume and complexity is undergoing major metamorphosis, too. Selection criteria based on basic workforce efficiency and cost metrics is not sufficient. In fact, they could be counter-productive to your goals of robust, production-ready ML applications. Key KPIs that are fast emerging as critical for almost all ML applications such as data preparation and annotation (DataOps) is no longer a “one way street” process feeding the MLOps cycle, but is intricately fused to it at multiple stages. The talent ask is now beyond someone who can draw polygons around objects in an image and they also need to be subject matter experts with significant experience in a vertical or use-case. The usage of a “crowd” is no longer the optimal solution for many applications. The technical ability (technique) needed for these complex DataOps projects can only be groomed through long-time employment, experience, and comprehensive training. Technology’s role in DataOps has become more important than ever before. A robust workforce, project, task, and data management platform is one of the most critical pieces to cater to the scale. Usage of tooling to improve productivity, efficiency, accuracy as well as automation are also must-haves in an efficient, scalable DataOps platform. For example, Edge case analysis is now a critical part of scaling in production. An edge case in a pilot deployment will surely be unacceptably common in a commercial fleet. This requires tools and talent to work synergistically, with a deep understanding of the client’s application and training methodology. iMerit ensures continued success for your MLOps application, development, evaluation and tracking metrics across all 3 criteria above is very critical. Only a few DataOps companies have managed to make the transition and provide a robust integrated end-to-end solution. A still smaller fraction can keep pace with the scale, performance and automation roadmaps of this fast-changing industry, and iMerit is at the helm as one of the industry leaders.""
Look, Listen, Read: Unified AI with TorchMultimodal Multimodal AI is a fast-growing field where deep neural networks are trained using multiple types of input data simultaneously (e.g. text, image, video, audio). Multimodal models perform better in content understanding applications, and are setting new standards for content generation in models such as DALL-E and StableDiffusion. Building multimodal models is hard; In this session we share more about multimodal AI, why you should care about it, what are some challenges you might face and how TorchMultimodal, our new PyTorch domain library eases the developer experience of building multimodal models.
""Using Causal Inference Model to Set Up Financial Goals of Company In this session, we will talk about innovation in traditional financial planning. We developed a machine learning process to produce quarterly and annual goals for the company. The business question is """"""""if we do X, what would happen in Y"""""""". In Economics and Machine learning, this type of question is called """"""""Causal inference"""""""" to predict an outcome after an action. In Finance, the company has goals for certain top business metrics such as revenue and number of users, and each business team (marketing, sales, etc.) will be responsible to achieve goals.

The causal inference process will automatically look for factors that can lead to moving a business metric. In our example, one of the top primary goals is to increase revenue. Senior executives want to increase revenue by moving the number of monthly active users (MAU), the cost per acquisition, and several other leadership metrics. If we have a goal of increasing revenue by 20%, how many more MAU should we have? This type of causal question is answered by causal models.

Causal models are not just linear regression. They are machine learning-based models to control other variables as fixed numbers to observe the impact between X and Y. During the session, we not only provide the business problem and results, but also want to provide practical advice in our modeling process. Lastly, we will share unique experience of working with cross-functional leadership from product, marketing, finance, and operations.""
""A Decade of Machine Learning Accelerators:Lessons Learned and Carbon Footprint The success of deep neural networks (DNNs) from Machine Learning (ML) has inspired domain specific architectures (DSAs) for them. Google’s first generation DSA offered 50x improvement over conventional architectures for ML inference in 2015. Google next built the first production DSA supercomputer for the much harder problem of training. Subsequent generations greatly improved performance of both phases. We start with ten lessons learned from such efforts.

The rapid growth of DNNs rightfully raised concerns about their carbon footprint. The second part of the talk identifies the “4Ms” (Model, Machine, Mechanization, Map) that, if optimized, can reduce ML training energy by up to 100x and carbon emissions up to 1000x. By improving the 4Ms, ML held steady at <15% of Google’s total energy use despite it consuming ~75% of its floating point operations. With continuing focus on the 4Ms, we can realize the amazing potential of ML to positively impact many fields in a sustainable way.""
""Applied Natural Language Processing for Cybersecurity: Taming the Bleeding Edge Language Models for Practical Security Use Cases Applied Natural Language Processing for cybersecurity: Taming the bleeding edge language models for practical security use cases

We have new and powerful natural language models cropping up almost every month, each with more than a billion parameters, capable of numerous open-ended human-like language tasks – like conjuring up crazy concoctions of realistic images from unrealistic human descriptions. But how can they be useful for practical purposes in the cybersecurity domain? Have we solved all the low hanging fruits related to existing security bottlenecks and automation of all kinds of security events analysis?

Following questions will be proposed to the panel for discussion:

1. Large Language Models – A new Moore’s Law? Can multibillion parameter models be finally used for practical infosec use cases?

2. Have we explored tried and tested NLP techniques being successfully used in other domains – for e.g. Topic Modeling in advertisement and SEO (Search Engine Optimization) industry – are these being successfully adapted for infosec use cases? What are other examples?

3. Infosec benchmark datasets for language modeling – is enough work being done here? How can we move the needle here?

4. Dangers and pitfalls of open-ended language models in infosec""
Continual Learning of Natural Language Processing Tasks 
The Next Thousand Languages Low-resource languages present a challenge for data-hungry approaches to machine translation, speech recognition, and other technologies that promise to open the way for universal participation in the global information society. In this talk I will present a new perspective on the language technology for all (LT4All) agenda, beginning with the structure of the world's linguistic diversity and the actual linguistic challenges on the ground. I will draw on experiences working in societies where there is no clear case for the popular practice of replicating human capabilities in translation or speech recognition, but where there are myriad other opportunities for language technologies. I will describe new ways of working with local communities, oral languages, and human-curated linguistic resources. The result is a radically inclusive approach to language technology which embraces and sustains linguistic diversity.
Causal/Prescriptive Analytics in Business Decisions In this talk, I will provide a holistic review of the research methods and tools for causal analytics in business decisions. I focus especially on causal inference in data science. I will discuss a decision tree that helps data scientists to identify the best causal research method based on the problem, context, and the nature of the data. I will draw on my proprietary research on prescriptive analytics (https://docs.google.com/document/d/1b8yaDzriVB2JyIBNQMsUn-uz4bXnsdFe6hTLLTOs1q4/edit). In addition, to give some high-level overviews of the business use cases, I will also draw insights from use cases in the investment industry, and my previous academic role as a Business Professor leading multiple National Science Foundation-sponsored commercial research projects on AI, including Explainable AI (XAI) and Causal Analytics with Human Expertise. Some of the tools and previews can be found on https://www.gopeaks.org/applications.
Responsible AI a Global Imperative for Governments and Business – Now and the Future AI is ever more ubiquitous in our lives but all countries are not created equal in their access to or use of AI. Likewise all countries and businesses do not adhere to the same regulatory frameworks or opinions on governance. Yet all companies would benefit from knowing where they stand so that investment in technology is not ultimately wasted. Likewise, access to AI is being used as a geopolitical tool. What lessons are we able to draw and adopt now and how might this thinking mature into the future.
Making ML Scaling Easy Over the past decade the computation demands of machine learning (ML) workloads have grown much faster than the capabilities of a single processor, including hardware accelerators such as GPUs and TPUs. As a result researchers and practitioners have been left with no choice but to distribute these workloads. Unfortunately, developing distributed applications is very challenging. In this talk I will present two projects we developed at UC Berkeley, Ray (https://github.com/ray-project/ray) and Alpa (https://github.com/alpa-projects/alpa), that dramatically simplify scaling ML workloads
""Why you can’t Apply Common Software Best Practices Directly to Data Workflows, and What you can do About it Every day, virtual mountains of data are collected and stored at unfathomable speeds. As data volume grows exponentially, the data workflow becomes more complex as an avalanche of data makes it challenging to identify, cleanse, mine, pivot and use it for both insights and AI powered product features.

To derive the most value from their data, data professionals must be able to set up their workflow in a way that will maximize not only their own efficiency and productivity but also data reproducibility. To do this, data teams borrow a lot of best practices from software engineering like testing, version control, documentation and continuous integration and deployment (CI/CD), but there are important differences in how these are implemented with data workflows that hamper the success of data teams.

This presentation will outline the specific challenges to adopting software engineering best practices for data and analytics workflows, why they exist, and how data scientists can craft environments to best address common pitfalls and encourage reproducibility.

Specifically:

- what it really means to ‘version control’ data sets (and why it's not what most people think). The session will elaborate on the different motivations for version control in production software environments and production data environments, and discuss best practices for versioning in a data specific workflow..
- what CI/CD needs to look like to best enable a team to collaborate on a data workflow. For example, how to best design data integration tests that go beyond checking for null or unexpected values to enable a team to make changes to a data workflow without compromising downstream dependencies.
- “the why” behind several canonical data modeling best practices that are prevalent today (e.g. a staging layer in your data model), and how they contribute to reproducibility in data workflows.

The session will cover specific actions leaders can take and offer real life examples and use cases. Attendees will walk away with a deeper understanding of how to avoid common pitfalls, how to improve team collaboration and reproducibility in data workflows.""
Four Reasons the Data Science Development Experience Sucks 
HP’s 3D Digital Twin: Synthesize Data and Domain Science to Improve Additive Manufacturing Production Yield 
Data Analytics at Scale: A Four-legged Stool In this talk, I discuss four tactics that enable successful enterprise analytics efforts. The first concerns data integration. Because essentially all enterprise data resides in data silos, an integration effort is required before meaningful cross-silo analysis is possible. Data science practitioners routinely report spending at least 80% of their time doing “data preparation” (aka data munging). I describe why this activity is hard and tactics that can be employed to make it less costly. Once one has clean cross-silo data, then two further tactics entail using an analytics suite and an information discovery tool. The first is required to do data analytics while the second is necessary when one doesn’t know what analysis to perform. I discuss desired features of each tool, as well as make some comments about machine learning. The fourth tactic entails data lakes and lake houses. Please put everything in a DBMS, so the integration challenge of data lakes is as manageable as possible.
""Data Science Without Data Collection Using FedScale Cloud computing has successfully accommodated the three """"""""V""""""""s of Big Data into data science, but collecting everything into the cloud is becoming increasingly infeasible. Today, we face a new set of challenges. A growing awareness of privacy among individual users and governing bodies is forcing platform providers to restrict the variety of data we can collect. Often, we cannot transfer data to the cloud at the velocity of its generation. Many cloud users suffer from sticker shock, buyer's remorse, or both as they try to keep up with the volume of data they must process. Making sense of data closer to its home is more appealing than ever.

Federated learning is a growing field that attempts to address this challenge by distributing learning and analytics tasks to end-user devices. Although theoretical federated learning research is growing exponentially to meet these challenges, we are far from putting those theories into practice. In this talk, I will introduce FedScale, a scalable and extensible open-source federated learning and analytics platform. It provides high-level APIs to implement algorithms, a modular design to customize implementations for diverse hardware and software backends, and the ease of deploying the same code at many scales. FedScale also includes a comprehensive benchmark that allows data scientists to evaluate their ideas in realistic, large-scale settings. I will highlight a select few systems successfully built using FedScale and share insights from benchmarking recent algorithms using FedScale.""
Super Data Science Podcast with Jon Krohn: Responsible Decentralized Intelligence featuring Dawn Song 
Scalable, Real-Time Heart Rate Variability Biofeedback for Precision Health: A Novel Algorithmic Approach Heart rate variability biofeedback (HRV-B) is a clinically effective therapy in which patients can improve their mental and physical well-being through real-time"
